{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d06f0d",
   "metadata": {},
   "source": [
    "# Remove Noise Points\n",
    "What it is: Completely remove the outlier points from the dataset.\n",
    "\n",
    "When to use:\n",
    "\n",
    "Outliers are due to data errors or measurement noise\n",
    "\n",
    "You want a clean dataset for downstream tasks\n",
    "\n",
    "The outliers don't contain valuable information\n",
    "\n",
    "You're building models that are sensitive to outliers\n",
    "\n",
    "Pros:\n",
    "\n",
    " Creates a clean dataset for modeling\n",
    "\n",
    " Removes potentially harmful outliers\n",
    "\n",
    " Simple and straightforward\n",
    "\n",
    "Cons:\n",
    "\n",
    " Loses information that might be valuable\n",
    "\n",
    " May remove genuine but rare events\n",
    "\n",
    " Reduces dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise_points(X, labels):\n",
    "    \"\"\"\n",
    "    Remove points labeled as noise (label = -1 in DBSCAN)\n",
    "    \"\"\"\n",
    "    # Keep only points that belong to clusters (not noise)\n",
    "    clean_mask = labels != -1\n",
    "    X_clean = X[clean_mask]\n",
    "    labels_clean = labels[clean_mask]\n",
    "    \n",
    "    print(f\"Removed {np.sum(~clean_mask)} noise points\")\n",
    "    print(f\"Original: {len(X)} points, Clean: {len(X_clean)} points\")\n",
    "    \n",
    "    return X_clean, labels_clean\n",
    "\n",
    "# Example with DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "# Generate data with some noise\n",
    "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n",
    "# Add some noise points\n",
    "noise_points = np.random.uniform(-10, 10, (20, 2))\n",
    "X = np.vstack([X, noise_points])\n",
    "\n",
    "# Cluster with DBSCAN\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Remove noise points\n",
    "X_clean, labels_clean = remove_noise_points(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3182e1c",
   "metadata": {},
   "source": [
    "# Relabel as Anomalies\n",
    "What it is: Keep the points but explicitly label them as anomalies for special treatment.\n",
    "\n",
    "When to use:\n",
    "\n",
    "Outliers represent genuine anomalies of interest\n",
    "\n",
    "You want to analyze the anomalous points separately\n",
    "\n",
    "Building anomaly detection systems\n",
    "\n",
    "The outliers contain valuable business information\n",
    "\n",
    "\n",
    "Pros:\n",
    "\n",
    " Preserves all data points\n",
    "\n",
    " Enables separate analysis of anomalies\n",
    "\n",
    " Useful for fraud detection, rare event analysis\n",
    "\n",
    " Maintains dataset completeness\n",
    "\n",
    "Cons:\n",
    " Requires special handling in downstream tasks\n",
    "\n",
    " More complex data management\n",
    "\n",
    " May confuse some algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_as_anomalies(X, labels, anomaly_label=-1):\n",
    "    \"\"\"\n",
    "    Explicitly relabel noise points as anomalies\n",
    "    \"\"\"\n",
    "    # Create explicit anomaly labels\n",
    "    anomaly_mask = labels == -1\n",
    "    \n",
    "    # You can create a separate dataset for analysis\n",
    "    anomalies = X[anomaly_mask]\n",
    "    normal_data = X[~anomaly_mask]\n",
    "    normal_labels = labels[~anomaly_mask]\n",
    "    \n",
    "    print(f\"Found {len(anomalies)} anomalies\")\n",
    "    print(f\"Normal data: {len(normal_data)} points\")\n",
    "    \n",
    "    # For analysis, you might want to keep track of both\n",
    "    analysis_data = {\n",
    "        'normal_data': normal_data,\n",
    "        'normal_labels': normal_labels,\n",
    "        'anomalies': anomalies,\n",
    "        'all_data': X,\n",
    "        'anomaly_mask': anomaly_mask\n",
    "    }\n",
    "    \n",
    "    return analysis_data\n",
    "\n",
    "# Example usage\n",
    "analysis_result = relabel_as_anomalies(X, labels)\n",
    "\n",
    "# You can now analyze anomalies separately\n",
    "anomalies = analysis_result['anomalies']\n",
    "print(\"Anomaly characteristics:\")\n",
    "print(f\"Mean position: {np.mean(anomalies, axis=0)}\")\n",
    "print(f\"Bounds: [{np.min(anomalies, axis=0)}, {np.max(anomalies, axis=0)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6103e7",
   "metadata": {},
   "source": [
    "# Reassign to Nearest Cluster\n",
    "What it is: Assign outlier points to their nearest cluster based on distance metrics.\n",
    "\n",
    "When to use:\n",
    "\n",
    "Outliers are mild and likely just borderline cases\n",
    "\n",
    "You want to maintain dataset structure\n",
    "\n",
    "For visualization or reporting purposes\n",
    "\n",
    "When you need complete clustering without noise\n",
    "Pros:\n",
    "\n",
    " Maintains complete dataset\n",
    "\n",
    " Provides \"clean\" clustering results\n",
    "\n",
    " Useful for visualization\n",
    "\n",
    " Handles borderline cases gracefully\n",
    "\n",
    "Cons:\n",
    "\n",
    " May force inappropriate cluster assignments\n",
    "\n",
    " Can distort cluster characteristics\n",
    "\n",
    " Loss of information about data uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign_to_nearest_cluster(X, labels, method='centroid'):\n",
    "    \"\"\"\n",
    "    Reassign noise points to their nearest cluster\n",
    "    \"\"\"\n",
    "    clean_labels = labels.copy()\n",
    "    noise_mask = labels == -1\n",
    "    \n",
    "    if not np.any(noise_mask):\n",
    "        print(\"No noise points to reassign\")\n",
    "        return clean_labels\n",
    "    \n",
    "    if method == 'centroid':\n",
    "        # Calculate cluster centroids from non-noise points\n",
    "        unique_labels = np.unique(labels[labels != -1])\n",
    "        centroids = []\n",
    "        \n",
    "        for cluster_id in unique_labels:\n",
    "            cluster_points = X[labels == cluster_id]\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            centroids.append(centroid)\n",
    "        \n",
    "        centroids = np.array(centroids)\n",
    "        \n",
    "        # For each noise point, find nearest centroid\n",
    "        for i in np.where(noise_mask)[0]:\n",
    "            point = X[i]\n",
    "            distances = np.linalg.norm(centroids - point, axis=1)\n",
    "            nearest_cluster_idx = np.argmin(distances)\n",
    "            clean_labels[i] = unique_labels[nearest_cluster_idx]\n",
    "    \n",
    "    elif method == 'nearest_neighbor':\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        # Find nearest non-noise point for each noise point\n",
    "        non_noise_mask = labels != -1\n",
    "        non_noise_points = X[non_noise_mask]\n",
    "        non_noise_labels = labels[non_noise_mask]\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(non_noise_points)\n",
    "        \n",
    "        for i in np.where(noise_mask)[0]:\n",
    "            point = X[i].reshape(1, -1)\n",
    "            distances, indices = nbrs.kneighbors(point)\n",
    "            nearest_label = non_noise_labels[indices[0][0]]\n",
    "            clean_labels[i] = nearest_label\n",
    "    \n",
    "    print(f\"Reassigned {np.sum(noise_mask)} noise points using {method} method\")\n",
    "    return clean_labels\n",
    "\n",
    "# Example usage\n",
    "reassigned_labels = reassign_to_nearest_cluster(X, labels, method='centroid')\n",
    "\n",
    "# Verify no more noise points\n",
    "unique_labels_after = np.unique(reassigned_labels)\n",
    "print(f\"Labels after reassignment: {unique_labels_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc30bcf",
   "metadata": {},
   "source": [
    "# How would you handle outliers identified by clustering algorithms?\"\n",
    "\n",
    "I consider three main strategies based on the business context and data characteristics:\n",
    "\n",
    "Remove noise points when outliers are likely data errors and I need a clean dataset for modeling\n",
    "\n",
    "Relabel as anomalies when outliers represent genuine rare events that need separate investigation, like in fraud detection\n",
    "\n",
    "Reassign to nearest cluster for borderline cases or when I need to maintain complete dataset structure\n",
    "\n",
    "The choice depends on whether the outliers contain valuable information, the impact on downstream tasks, and the specific business objectives. For example, in customer segmentation, I might remove obvious data errors but investigate potential high-value outliers separately .\n",
    "\n",
    "\n",
    "\n",
    "- Always document your handling strategy\n",
    "\n",
    "- Validate that the chosen approach improves your analysis\n",
    "\n",
    "- Consider the business impact of each strategy\n",
    "\n",
    "- Test multiple approaches if uncertain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
