{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce9b744",
   "metadata": {},
   "source": [
    "# Mahalanobis Distance\n",
    "Mahalanobis Distance measures the distance between a point and a distribution, taking into account the correlations between variables and their different scales.\n",
    "\n",
    "``` Unlike Euclidean distance that treats all directions equally, Mahalanobis Distance understands that data can be correlated and have different variances.```\n",
    "\n",
    "Mathematical Definition\n",
    "For a single point x from distribution with mean μ and covariance matrix Σ:\n",
    "\n",
    "\n",
    "``` D² = (x - μ)ᵀ Σ⁻¹ (x - μ)```\n",
    "Where:\n",
    "\n",
    "x = data point (vector)\n",
    "\n",
    "μ = mean vector of the distribution\n",
    "\n",
    "Σ = covariance matrix of the distribution\n",
    "\n",
    "Σ⁻¹ = inverse of covariance matrix\n",
    "\n",
    "D² = squared Mahalanobis distance\n",
    "\n",
    "Geometric Interpretation\n",
    "Think of the data as an ellipsoid (like a stretched sphere):\n",
    "\n",
    "Euclidean distance: Measures distance as if the data were a perfect sphere\n",
    "\n",
    "Mahalanobis distance: Measures distance accounting for the actual shape and orientation of the ellipsoid\n",
    "\n",
    "### Step-by-Step Example\n",
    "Let's use height-weight data to understand the intuition:\n",
    "``` bash\n",
    "Dataset:\n",
    "\n",
    "\n",
    "Person A: [Height=170cm, Weight=65kg]\n",
    "Person B: [Height=175cm, Weight=70kg]\n",
    "Person C: [Height=180cm, Weight=75kg]\n",
    "Person D: [Height=190cm, Weight=40kg]  ← Suspicious!\n",
    "Step 1: Calculate Mean Vector (μ)\n",
    "\n",
    "\n",
    "Height mean = (170+175+180+190)/4 = 178.75\n",
    "Weight mean = (65+70+75+40)/4 = 62.5\n",
    "\n",
    "μ = [178.75, 62.5]\n",
    "Step 2: Calculate Covariance Matrix (Σ)\n",
    "\n",
    "\n",
    "Covariance measures how variables change together:\n",
    "\n",
    "Σ = [[Var(Height),    Cov(Height, Weight)],\n",
    "     [Cov(Weight, Height), Var(Weight)]]\n",
    "\n",
    "After calculation:\n",
    "Σ = [[62.92,  37.50],\n",
    "     [37.50, 193.75]]\n",
    "Step 3: Calculate Inverse Covariance Matrix (Σ⁻¹)\n",
    "\n",
    "\n",
    "Σ⁻¹ = [[ 0.0243, -0.0047],\n",
    "       [-0.0047,  0.0079]]\n",
    "Step 4: Calculate Mahalanobis Distance for Person D\n",
    "\n",
    "\n",
    "x - μ = [190-178.75, 40-62.5] = [11.25, -22.5]\n",
    "\n",
    "D² = [11.25, -22.5] × Σ⁻¹ × [11.25, -22.5]ᵀ\n",
    "   = [11.25, -22.5] × [[0.0243, -0.0047], [-0.0047, 0.0079]] × [11.25, -22.5]ᵀ\n",
    "\n",
    "Step 1: [11.25×0.0243 + (-22.5)×(-0.0047), 11.25×(-0.0047) + (-22.5)×0.0079]\n",
    "       = [0.273 + 0.106, -0.053 + (-0.178)] = [0.379, -0.231]\n",
    "\n",
    "Step 2: [0.379, -0.231] × [11.25, -22.5]ᵀ\n",
    "       = 0.379×11.25 + (-0.231)×(-22.5)\n",
    "       = 4.26 + 5.20 = 9.46\n",
    "\n",
    "D = √9.46 = 3.08\n",
    "Comparison with Euclidean Distance\n",
    "Let's compare both distances for Person D:\n",
    "\n",
    "Euclidean Distance from mean:\n",
    "\n",
    "\n",
    "√((190-178.75)² + (40-62.5)²) = √(126.56 + 506.25) = √632.81 = 25.16\n",
    "Mahalanobis Distance: 3.08\n",
    "\n",
    "Wait! Why is Mahalanobis smaller? Because it understands that:\n",
    "\n",
    "Height and weight are correlated (tall people tend to weigh more)\n",
    "\n",
    "A tall, lightweight person is unusual given this correlation\n",
    "\n",
    "But it accounts for the natural variability in the data\n",
    "\n",
    "The Real Power: Understanding Correlations\n",
    "Consider this more dramatic example:\n",
    "\n",
    "Scenario: Tech company employees\n",
    "\n",
    "\n",
    "Features: [Years_Experience, Salary]\n",
    "Normal pattern: Experience ↑ → Salary ↑\n",
    "\n",
    "Normal employees:\n",
    "[2, 60000], [5, 75000], [8, 90000], [12, 120000]\n",
    "\n",
    "Suspicious employee:\n",
    "[1, 200000]  ← 1 year experience, $200K salary\n",
    "Euclidean distance might not flag this as extreme, but Mahalanobis distance will recognize this breaks the expected correlation pattern!\n",
    "\n",
    "Statistical Properties\n",
    "Chi-Square Distribution:\n",
    "\n",
    "For multivariate normal data, squared Mahalanobis distances follow a Chi-Square distribution\n",
    "\n",
    "Degrees of freedom = number of features (p)\n",
    "\n",
    "Outlier threshold: D² > χ²(p, α) where α is significance level (e.g., 0.95, 0.99)\n",
    "\n",
    "Critical Values:\n",
    "\n",
    "For p=2 features, α=0.95: χ²(2, 0.95) = 5.991\n",
    "\n",
    "For p=2 features, α=0.99: χ²(2, 0.99) = 9.210\n",
    "```\n",
    "\n",
    "## When to Use Mahalanobis Distance\n",
    "- Excellent for:\n",
    "\n",
    "Multivariate outlier detection\n",
    "\n",
    "Correlated features\n",
    "\n",
    "Different scales and units\n",
    "\n",
    "Quality control processes\n",
    "\n",
    "Anomaly detection in complex systems\n",
    "\n",
    "- Requirements:\n",
    "\n",
    "More observations than features (n > p)\n",
    "\n",
    "No perfect multicollinearity\n",
    "\n",
    "Roughly multivariate normal distribution\n",
    "\n",
    "- Limitations:\n",
    "\n",
    "Sensitive to outliers in mean/covariance estimation\n",
    "\n",
    "Computationally expensive for high dimensions\n",
    "\n",
    "Requires matrix inversion (can be unstable)\n",
    "\n",
    "#### Robust Mahalanobis Distance\n",
    "Problem: Regular Mahalanobis is sensitive to outliers in the mean and covariance estimates.\n",
    "\n",
    "- Solution: Use robust estimators:\n",
    "\n",
    "Minimum Covariance Determinant (MCD)\n",
    "\n",
    "Use median instead of mean\n",
    "\n",
    "Use robust covariance estimators\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
