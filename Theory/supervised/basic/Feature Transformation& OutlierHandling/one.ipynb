{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b3fac0",
   "metadata": {},
   "source": [
    "# OUTLIERS:\n",
    "What are Outliers?\n",
    "Definition: Observations that deviate significantly from other observations\n",
    "\n",
    "Statistical Perspective: Data points that don't conform to the assumed distribution\n",
    "\n",
    "Types:\n",
    "\n",
    "Point Outliers: Individual data points\n",
    "\n",
    "Contextual Outliers: Abnormal in specific context\n",
    "\n",
    "Collective Outliers: Group of data points abnormal collectively\n",
    "\n",
    "Mathematical Formulations\n",
    "1.1 Z-Score Method\n",
    "``` bash\n",
    "For data point x_i in feature X:\n",
    "    z_i = (x_i - μ) / σ\n",
    " ```\n",
    "    \n",
    "where:\n",
    "    μ = mean(X)\n",
    "    σ = standard deviation(X)\n",
    "    \n",
    "Outlier if: |z_i| > threshold (typically 3)\n",
    "Theoretical Basis: Based on Normal Distribution (Gaussian) properties where:\n",
    "\n",
    "68% data within μ ± σ\n",
    "\n",
    "95% data within μ ± 2σ\n",
    "\n",
    "99.7% data within μ ± 3σ\n",
    "\n",
    "- Assumptions: Data follows normal distribution\n",
    "- Limitations: Sensitive to extreme values (mean and variance are not robust)\n",
    "\n",
    "1.2 Interquartile Range (IQR) Method\n",
    "``` bash\n",
    "Q1 = 25th percentile\n",
    "Q3 = 75th percentile\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "Lower Bound = Q1 - k × IQR\n",
    "Upper Bound = Q3 + k × IQR\n",
    "```\n",
    "\n",
    "Typically k = 1.5 (mild outliers) or k = 3 (extreme outliers)\n",
    "Theoretical Basis: Based on quartiles which are robust to outliers\n",
    "Statistical Properties:\n",
    "\n",
    "### Non-parametric method\n",
    "\n",
    "Doesn't assume normal distribution\n",
    "\n",
    "Based on data percentiles\n",
    "\n",
    "1.3 Modified Z-Score (Robust Z-Score)\n",
    "``` bash\n",
    "MAD = median(|X_i - median(X)|)\n",
    "Modified Z = 0.6745 × (X_i - median(X)) / MAD\n",
    "Uses median instead of mean → more robust\n",
    "```\n",
    "\n",
    "1.4 Mahalanobis Distance\n",
    "\n",
    "``` D² = (x - μ)ᵀ Σ⁻¹ (x - μ)  ```\n",
    "\n",
    "where:\n",
    "    x = data point\n",
    "    μ = mean vector\n",
    "    Σ = covariance matrix\n",
    "Theoretical Basis: Measures distance in multivariate space considering correlations\n",
    "\n",
    "## OUTLIER TREATMENT STRATEGIES: THEORY\n",
    "2.1 Removal\n",
    "When to use:\n",
    "\n",
    "Clear measurement errors\n",
    "\n",
    "< 5% of data are outliers\n",
    "\n",
    "Large dataset\n",
    "\n",
    "Statistical Impact:\n",
    "\n",
    "Reduces variance\n",
    "\n",
    "May introduce bias if outliers are informative\n",
    "\n",
    "Changes distribution parameters\n",
    "\n",
    "2.2 Capping/Winsorizing\n",
    "``` bash\n",
    "x_capped = \n",
    "    ⎧ lower_bound, if x < lower_bound\n",
    "    ⎨ x,          if lower_bound ≤ x ≤ upper_bound\n",
    "    ⎩ upper_bound, if x > upper_bound\n",
    "\n",
    "```\n",
    "Statistical Properties:\n",
    "\n",
    "Preserves sample size\n",
    "\n",
    "Reduces variance less than removal\n",
    "\n",
    "Creates artificial peaks at boundaries\n",
    "\n",
    "Winsorizing: Special case where extreme values are replaced with nearest non-outlier\n",
    "\n",
    "2.3 Transformation\n",
    "Theory: Apply mathematical functions to reduce impact of outliers while preserving data\n",
    "\n",
    "# LOG TRANSFORMATION\n",
    "3.1 Purpose and Effect\n",
    "``` bash\n",
    "Original: y = f(x)\n",
    "Log Transform: y' = log(y)\n",
    "```\n",
    "Theoretical Effects:\n",
    "\n",
    "Variance Stabilization: Reduces heteroscedasticity\n",
    "\n",
    "Symmetrization: Converts right-skewed to more symmetric\n",
    "\n",
    "Additive Effects: Multiplicative relationships become additive\n",
    "\n",
    "3.2 Mathematical Properties\n",
    "``` bash\n",
    "Natural Log (ln) vs Log10\n",
    "\n",
    "ln(x) = logₑ(x) = 2.3026 × log₁₀(x)\n",
    "Both have same effect on distribution shape\n",
    "\n",
    "Log(1+x) Transformation\n",
    "\n",
    "y = log(1 + x)\n",
    "Why add 1?: To handle zero values (log(0) = -∞)\n",
    "```\n",
    "3.3 Box-Cox Transformation (Generalized)\n",
    "``` bash\n",
    "y(λ) = \n",
    "    ⎧ (y^λ - 1)/λ, if λ ≠ 0\n",
    "    ⎨ log(y),      if λ = 0\n",
    "\n",
    "```\n",
    "Theoretical Basis:\n",
    "\n",
    "Finds optimal λ that maximizes log-likelihood\n",
    "\n",
    "Makes data approximately normal\n",
    "\n",
    "Assumption: y > 0\n",
    "\n",
    "Log-Likelihood Function:\n",
    "\n",
    "``` bash\n",
    "L(λ) = -(n/2) × ln(σ²(λ)) + (λ-1) × Σ ln(y_i)\n",
    "```\n",
    "where σ²(λ) is variance of transformed data\n",
    "\n",
    "3.4 Yeo-Johnson Transformation\n",
    "``` bash\n",
    "For y ≥ 0: same as Box-Cox\n",
    "For y < 0: ψ(y, λ) = \n",
    "    ⎧ [(y+1)^λ - 1]/λ, if λ ≠ 0, y ≥ 0\n",
    "    ⎨ log(y+1),        if λ = 0, y ≥ 0\n",
    "    ⎩ -[(-y+1)^(2-λ) - 1]/(2-λ), if λ ≠ 2, y < 0\n",
    "    ⎩ -log(-y+1),      if λ = 2, y < 0\n",
    "```\n",
    "Advantage: Handles negative values\n",
    "\n",
    "# FEATURE TRANSFORMATION THEORY\n",
    "4.1 Scaling Methods: Mathematical Formulation\n",
    "Standardization (Z-score Normalization)\n",
    "\n",
    "``` x_standardized = (x - μ) / σ ```\n",
    "Properties:\n",
    "\n",
    "Mean = 0, Standard Deviation = 1\n",
    "\n",
    "Preserves original distribution shape\n",
    "\n",
    "Sensitive to outliers\n",
    "\n",
    "Min-Max Scaling\n",
    "\n",
    "``` x_scaled = (x - min(x)) / (max(x) - min(x)) ```\n",
    "Properties:\n",
    "\n",
    "Range: [0, 1]\n",
    "\n",
    "Sensitive to outliers (min and max affected)\n",
    "\n",
    "Robust Scaling\n",
    "\n",
    "``` x_robust = (x - median(x)) / IQR ```\n",
    "Properties:\n",
    "\n",
    "Uses median and IQR → robust to outliers\n",
    "\n",
    "Preserves outliers (just scales them)\n",
    "\n",
    "Unit Vector Scaling (Normalization)\n",
    "\n",
    "``` x_unit = x / ||x||  ```\n",
    "where ||x|| = √(Σx_i²) (L2 norm)\n",
    "\n",
    "4.2 Power Transformations Theory\n",
    "General Power Transformation\n",
    "\n",
    "``` T(y) = y^λ  ```\n",
    "Effect on skewness:\n",
    "\n",
    "λ > 1: Increases right skew\n",
    "\n",
    "λ < 1: Reduces right skew\n",
    "\n",
    "λ = 0: Log transform (by limit)\n",
    "\n",
    "Theoretical Justification\n",
    "Power transformations aim to:\n",
    "\n",
    "Stabilize variance (Homoscedasticity)\n",
    "\n",
    "Improve normality (Gaussianity)\n",
    "\n",
    "Linearize relationships\n",
    "\n",
    "\n",
    "# DOMAIN-DRIVEN FEATURES: THEORETICAL BASIS\n",
    "5.1 Mathematical Foundation\n",
    "Interaction Features\n",
    "```bash\n",
    "z = f(x, y) where f is:\n",
    "    - Multiplicative: x × y\n",
    "    - Ratio: x / y\n",
    "    - Polynomial: x², x³, √x\n",
    "```\n",
    "Theoretical Basis:\n",
    "\n",
    "Captures non-linear relationships\n",
    "\n",
    "Based on Taylor expansion: f(x,y) ≈ f(0) + f_x x + f_y y + f_xy xy + ...\n",
    "\n",
    "Interaction terms = cross-derivatives\n",
    "\n",
    "Composite Features\n",
    "\n",
    "``` Composite Score = Σ w_i × f_i(x_i)```\n",
    "where w_i are weights based on domain knowledge\n",
    "\n",
    "5.2 Statistical Justification\n",
    "Dimensionless Quantities\n",
    "```bash\n",
    "Reynolds Number = (ρ × v × L) / μ  [Fluid Dynamics]\n",
    "Sharpe Ratio = (R_p - R_f) / σ_p    [Finance]\n",
    "```\n",
    "Advantage: Scale-invariant, comparable across contexts\n",
    "\n",
    "Information Ratio (Signal-to-Noise)\n",
    "\n",
    "```IR = μ / σ```\n",
    "Measures quality of signal relative to noise\n",
    "\n",
    "# BINNING/DISCRETIZATION THEORY\n",
    "6.1 Mathematical Formulation\n",
    "Equal Width Binning\n",
    "```bash\n",
    "Bin width = (max - min) / k\n",
    "Bin boundaries: min + i × width, i = 0..k\n",
    "```\n",
    "Equal Frequency (Quantile) Binning\n",
    "\n",
    "```Each bin contains n/k observations``\n",
    "```Boundaries at percentiles: 100 × i/k %```\n",
    "6.2 Information Theory Basis\n",
    "Entropy-Based Binning\n",
    "Maximize Information Gain:\n",
    "\n",
    "\n",
    "``` IG = H(D) - Σ (|D_i|/|D|) × H(D_i)```\n",
    "\n",
    "where:\n",
    "    H(D) = -Σ p(c) × log₂ p(c)  [Entropy]\n",
    "    D_i = data in bin i\n",
    "Minimum Description Length (MDL) Principle\n",
    "\n",
    "```Total Cost = Model Cost + Data Cost ```\n",
    "Optimal bins minimize total cost\n",
    "6.3 Optimal Binning Algorithms\n",
    "1. Fisher's Natural Breaks (Jenks)\n",
    "\n",
    "``` Minimize: GVF = (SDAM - SDCM) / SDAM```\n",
    "\n",
    "where:\n",
    "    SDAM = Sum of squared deviations from array mean\n",
    "    SDCM = Sum of squared deviations from class means\n",
    "Maximize Goodness of Variance Fit (GVF)\n",
    "\n",
    "2. Decision Tree Binning\n",
    "```bash\n",
    "Splits chosen to maximize:\n",
    "    - Information Gain\n",
    "    - Gini Impurity reduction\n",
    "    - Variance reduction (regression)\n",
    "```\n",
    "Mathematical Formulation (CART Algorithm):\n",
    "\n",
    "\n",
    "``` ΔI(s) = I(D) - (|D_left|/|D|)×I(D_left) - (|D_right|/|D|)×I(D_right) ```\n",
    "Choose split s that maximizes ΔI(s)\n",
    "\n",
    "# Why binning?\n",
    "\n",
    "Reduce noise\n",
    "\n",
    "Make non-linear patterns easier\n",
    "\n",
    "Convert continuous → categorical\n",
    "\n",
    "Good for decision tree models\n",
    "\n",
    "- Equal-Width Binning\n",
    "``` df['bins'] = pd.cut(df['column'], bins=4, labels=False)```\n",
    "\n",
    "- Equal-Frequency Binning\n",
    "``` df['bins'] = pd.qcut(df['column'], q=4, labels=False)```\n",
    "\n",
    "- Domain Binning (Best)\n",
    "Examples:\n",
    "\n",
    "Age → child, teen, adult, senior\n",
    "\n",
    "Income → low, mid, high\n",
    "\n",
    "Blood pressure → normal, high, critical\n",
    "\n",
    "When to use\n",
    "\n",
    "Tree models (RF, XGBoost)\n",
    "\n",
    "Meaningful categories required\n",
    "\n",
    "Outlier smoothing\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
