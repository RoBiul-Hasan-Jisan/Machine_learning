{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfb635a",
   "metadata": {},
   "source": [
    "# Area Under the Curve\n",
    "\n",
    "What is AUC?\n",
    "AUC stands for Area Under the ROC Curve (Receiver Operating Characteristic Curve). It's a performance metric for binary classification models that measures the model's ability to distinguish between positive and negative classes.\n",
    "\n",
    "ROC Curve Fundamentals\n",
    "Core Components:\n",
    "X-axis: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "\n",
    "Y-axis: True Positive Rate (TPR) = TP / (TP + FN)\n",
    "\n",
    "Diagonal line: Random classifier (AUC = 0.5)\n",
    "\n",
    "Perfect classifier: Top-left corner (AUC = 1.0)\n",
    "\n",
    "### Confusion Matrix Terms:\n",
    "```bash\n",
    "TP = True Positives  (correctly predicted positive)\n",
    "TN = True Negatives  (correctly predicted negative)\n",
    "FP = False Positives (incorrectly predicted positive)\n",
    "FN = False Negatives (incorrectly predicted negative)\n",
    "```\n",
    "\n",
    "### AUC Interpretation\n",
    "AUC Value\tInterpretation\n",
    "0.90-1.00\tExcellent\n",
    "0.80-0.90\tGood\n",
    "0.70-0.80\tFair\n",
    "0.60-0.70\tPoor\n",
    "0.50-0.60\tFail (worse than random)\n",
    "\n",
    "### Mathematical Foundation\n",
    "Probability Perspective:\n",
    "AUC = P(score_positive > score_negative)\n",
    "\n",
    "Where score_positive is the predicted probability for a random positive instance\n",
    "\n",
    "Where score_negative is the predicted probability for a random negative instance\n",
    "\n",
    "### Calculation Methods:\n",
    "\n",
    "Trapezoidal Rule (Most Common):\n",
    "\n",
    "```bash\n",
    "def calculate_auc(fpr, tpr):\n",
    "    # Sort by fpr\n",
    "    sorted_indices = np.argsort(fpr)\n",
    "    fpr_sorted = fpr[sorted_indices]\n",
    "    tpr_sorted = tpr[sorted_indices]\n",
    "    \n",
    "    # Calculate area using trapezoidal rule\n",
    "    auc = np.trapz(tpr_sorted, fpr_sorted)\n",
    "    return auc\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6abc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_roc_auc_analysis(y_true, y_pred_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive ROC/AUC visualization\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                 label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title(f'ROC Curve - {model_name}')\n",
    "    axes[0].legend(loc=\"lower right\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Threshold Analysis\n",
    "    youden_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[youden_idx]\n",
    "    \n",
    "    axes[1].plot(thresholds, tpr, label='True Positive Rate', lw=2)\n",
    "    axes[1].plot(thresholds, fpr, label='False Positive Rate', lw=2)\n",
    "    axes[1].axvline(x=optimal_threshold, color='red', linestyle='--', \n",
    "                    label=f'Optimal Threshold: {optimal_threshold:.3f}')\n",
    "    axes[1].set_xlabel('Threshold')\n",
    "    axes[1].set_ylabel('Rate')\n",
    "    axes[1].set_title('Threshold Analysis')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Probability Distribution\n",
    "    pos_probs = y_pred_proba[y_true == 1]\n",
    "    neg_probs = y_pred_proba[y_true == 0]\n",
    "    \n",
    "    axes[2].hist(pos_probs, bins=30, alpha=0.7, label='Positive Class', \n",
    "                 color='green', density=True)\n",
    "    axes[2].hist(neg_probs, bins=30, alpha=0.7, label='Negative Class', \n",
    "                 color='red', density=True)\n",
    "    axes[2].axvline(x=optimal_threshold, color='black', linestyle='--', \n",
    "                    label=f'Optimal Threshold')\n",
    "    axes[2].set_xlabel('Predicted Probability')\n",
    "    axes[2].set_ylabel('Density')\n",
    "    axes[2].set_title('Probability Distribution')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0d33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# One-vs-Rest (OvR) AUC\n",
    "def multiclass_auc_ovr(y_true, y_pred_proba, classes):\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    \n",
    "    auc_scores = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        auc_score = roc_auc_score(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        auc_scores[cls] = auc_score\n",
    "    \n",
    "    # Macro-average AUC\n",
    "    macro_auc = np.mean(list(auc_scores.values()))\n",
    "    \n",
    "    return auc_scores, macro_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59368a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import scipy.stats as stats\n",
    "\n",
    "def auc_with_ci(y_true, y_pred_proba, n_bootstraps=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate AUC with confidence intervals using bootstrapping\n",
    "    \"\"\"\n",
    "    auc_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Bootstrap sample\n",
    "        indices = resample(range(len(y_true)))\n",
    "        y_true_bs = y_true[indices]\n",
    "        y_pred_bs = y_pred_proba[indices]\n",
    "        \n",
    "        # Calculate AUC for bootstrap sample\n",
    "        try:\n",
    "            auc_bs = roc_auc_score(y_true_bs, y_pred_bs)\n",
    "            auc_scores.append(auc_bs)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = (1 - confidence) / 2\n",
    "    lower_bound = np.percentile(auc_scores, 100 * alpha)\n",
    "    upper_bound = np.percentile(auc_scores, 100 * (1 - alpha))\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    \n",
    "    return mean_auc, (lower_bound, upper_bound), auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6bc6d",
   "metadata": {},
   "source": [
    "### When to Use AUC:\n",
    "Imbalanced datasets (AUC is threshold-independent)\n",
    "\n",
    "Comparing multiple models\n",
    "\n",
    "When both false positives and false negatives are important\n",
    "\n",
    "Probability ranking is important\n",
    "\n",
    "### When NOT to Use AUC:\n",
    "Extremely imbalanced datasets (consider Precision-Recall AUC)\n",
    "\n",
    "Cost-sensitive learning (use cost curves)\n",
    "\n",
    "Multi-class with class imbalance (use macro/micro averaging carefully)\n",
    "\n",
    "---\n",
    "\n",
    "AUC measures ranking ability, not absolute predictions\n",
    "\n",
    "Range: 0.5 (random) to 1.0 (perfect)\n",
    "\n",
    "Threshold-independent - evaluates all possible thresholds\n",
    "\n",
    "Use AUC-PR for highly imbalanced datasets\n",
    "\n",
    "Always validate with business metrics and cost functions\n",
    "\n",
    "Monitor AUC drift in production systems\n",
    "\n",
    "Combine with calibration curves for complete assessment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
