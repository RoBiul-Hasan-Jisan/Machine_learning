{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cff431c",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Despite its name, Logistic Regression is a **supervised classification** algorithm used to predict the probability of a target variable. It is most commonly used for **binary classification** ($0$ or $1$, Yes or No).\n",
    "\n",
    "\n",
    "\n",
    "### 1. The Core Idea\n",
    "It combines a standard linear model with a \"squashing\" function to ensure the output is always a probability between $0$ and $1$.\n",
    "\n",
    "**Step 1: The Linear Part**\n",
    "Calculate the weighted sum of inputs (just like Linear Regression):\n",
    "$$z = w \\cdot x + b$$\n",
    "\n",
    "**Step 2: The Sigmoid Function (Activation)**\n",
    "Apply the **Sigmoid** function to $z$ to map it to the range $[0, 1]$:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Final Hypothesis:**\n",
    "$$P(y=1|x) = \\hat{y} = \\frac{1}{1 + e^{-(wx+b)}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Decision Boundary\n",
    "The model outputs a probability (e.g., $0.85$). To make a classification decision, we apply a threshold (usually $0.5$).\n",
    "\n",
    "* If $P(y=1|x) \\ge 0.5 \\rightarrow$ Predict **Class 1**.\n",
    "* If $P(y=1|x) < 0.5 \\rightarrow$ Predict **Class 0**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Loss Function: Binary Cross-Entropy (Log Loss)\n",
    "We **cannot** use Mean Squared Error (MSE) because the Sigmoid function makes the error surface \"wavy\" (non-convex), which confuses Gradient Descent.\n",
    "\n",
    "Instead, we use **Log Loss**, which penalizes confident wrong predictions heavily.\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]$$\n",
    "\n",
    "* If actual $y=1$: We want $\\hat{y} \\approx 1$. If $\\hat{y} \\approx 0$, loss approaches $\\infty$.\n",
    "* If actual $y=0$: We want $\\hat{y} \\approx 0$. If $\\hat{y} \\approx 1$, loss approaches $\\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why Not Linear Regression?\n",
    "1.  **Unbounded Output:** Linear Regression can predict values like $1.5$ or $-0.2$, which don't make sense as probabilities.\n",
    "2.  **Assumption Violation:** Linear Regression assumes errors are normally distributed; in classification, errors are Bernoulli distributed.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Assumptions of Logistic Regression\n",
    "1.  **Binary Outcome:** The target variable is binary (or converted to binary).\n",
    "2.  **Independence:** Observations are independent of each other.\n",
    "3.  **Linearity of Log-Odds:** The relationship between the independent variables and the **log-odds** of the dependent variable is linear.\n",
    "    * $\\ln(\\frac{p}{1-p}) = wx + b$\n",
    "4.  **No Multicollinearity:** Little to no correlation between independent variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Evaluation Metrics\n",
    "Since we are doing classification, we use:\n",
    "* **Accuracy**\n",
    "* **Precision & Recall**\n",
    "* **F1-Score** (Harmonic mean of Precision and Recall)\n",
    "* **ROC-AUC Curve**\n",
    "\n",
    "---\n",
    "\n",
    "### 7. FAQ\n",
    "\n",
    "**Q: Is Logistic Regression a linear model?**\n",
    "**A:** Yes. Even though the output curve is non-linear (S-shape), the decision boundary it creates is a straight line (or plane) in the feature space. The relationship between features and the *log-odds* is linear.\n",
    "\n",
    "**Q: Can it handle multi-class problems?**\n",
    "**A:** Yes.\n",
    "* **One-vs-Rest (OvR):** Trains one binary classifier for each class against all others.\n",
    "* **Multinomial:** Uses the **Softmax** function instead of Sigmoid to output probabilities for $K$ classes that sum to $1$.\n",
    "\n",
    "**Q: Why Log Loss instead of MSE?**\n",
    "**A:** Log Loss is **convex** for Logistic Regression, ensuring Gradient Descent finds the global minimum. MSE would result in many local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c570c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            z = self.w * X + self.b\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            dw = (1/n) * np.sum(X * (y_pred - y))\n",
    "            db = (1/n) * np.sum(y_pred - y)\n",
    "\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = self.w * X + self.b\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return [1 if i >= 0.5 else 0 for i in y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b4957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict(np.array([3, 5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b0d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[[0.64726666 0.35273334]\n",
      " [0.18436618 0.81563382]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict([[3], [5]]))\n",
    "print(model.predict_proba([[3], [5]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65bcda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0470438]]\n",
      "[-3.74817743]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db60b43",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is a **supervised** dimensionality reduction and classification technique. Unlike PCA (which looks for variance), LDA looks for \"separability.\"\n",
    "\n",
    "**Goal:** Find a linear combination of features (a new axis) that best separates two or more classes.\n",
    "\n",
    "\n",
    "\n",
    "### 1. The Core Idea: Fisher's Criterion\n",
    "LDA tries to achieve two things simultaneously:\n",
    "1.  **Maximize Between-Class Variance ($S_B$):** Push the centers (means) of different classes as far apart as possible.\n",
    "2.  **Minimize Within-Class Variance ($S_W$):** Keep the data points of the same class clustered tightly together.\n",
    "\n",
    "**The Objective Function ($J$):**\n",
    "$$J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "* We want to find the projection vector $w$ that maximizes this ratio.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Comparison: LDA vs. PCA\n",
    "\n",
    "| Feature | **LDA** | **PCA** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Type** | **Supervised** (Uses class labels $y$) | **Unsupervised** (Ignores labels, looks at $X$ only) |\n",
    "| **Goal** | Maximize **Class Separability** | Maximize **Data Variance** |\n",
    "| **Focus** | \"Which direction separates Red vs. Blue?\" | \"Which direction has the most spread?\" |\n",
    "| **Output** | Axes for Classification | Axes for Reconstruction/Compression |\n",
    "| **Use Case** | Feature Extraction & Classification | Dimensionality Reduction & Visualization |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Steps of LDA (The Algorithm)\n",
    "1.  **Compute Mean Vectors:** Calculate the mean vector $\\mu_k$ for each class.\n",
    "2.  **Compute Scatter Matrices:**\n",
    "    * **Within-Class Scatter ($S_W$):** How spread out is the data inside each class?\n",
    "    * **Between-Class Scatter ($S_B$):** How far apart are the class means?\n",
    "3.  **Solve Eigenvalue Problem:** We compute the eigenvectors of the matrix:\n",
    "    $$A = S_W^{-1} S_B$$\n",
    "4.  **Select Top Eigenvectors:** Choose the top $k$ eigenvectors (discriminants) to form the new subspace.\n",
    "5.  **Project:** Transform the original data onto these new axes.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. LDA as a Classifier\n",
    "Once projected, LDA can classify new data points simply by measuring the distance to the centroid (mean) of each class in the new space. It assigns the sample to the closest class mean.\n",
    "\n",
    "**Assumptions:**\n",
    "1.  Features are **Normally Distributed** (Gaussian).\n",
    "2.  **Homoscedasticity:** All classes share the same Covariance Matrix (spread shape).\n",
    "3.  **Linear Separability:** Classes can be separated by a straight line/plane.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| Simple, fast, and easy to interpret. | Fails if data is not normally distributed. |\n",
    "| Reduces dimensionality while preserving class info. | Fails if classes share different covariance structures (spreads). |\n",
    "| Works very well for small datasets. | Poor performance on non-linear data (curved boundaries). |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. FAQ\n",
    "\n",
    "**Q: Difference between LDA and Logistic Regression?**\n",
    "**A:**\n",
    "* **LDA** assumes data is Gaussian and uses the entire distribution to find the boundary. It is a \"Generative\" model.\n",
    "* **Logistic Regression** makes fewer assumptions (does not assume normality) and focuses only on the boundary. It is a \"Discriminative\" model.\n",
    "\n",
    "**Q: Can LDA be used for regression?**\n",
    "**A:** No. It deals with class labels. For regression, you would use standard Linear Regression or PLS (Partial Least Squares).\n",
    "\n",
    "**Q: When should I choose LDA over PCA?**\n",
    "**A:** Use **LDA** when you have labels and your specific goal is to classify the data. Use **PCA** when you don't have labels (unsupervised) or just want to compress the data for storage/visualization.\n",
    "\n",
    "> **One-Liner:**\n",
    "> * **PCA:** \"Show me the spread.\" (Max Variance)\n",
    "> * **LDA:** \"Show me the difference.\" (Max Separation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5eed43",
   "metadata": {},
   "source": [
    "# LDA: Deep Dive & Geometric Intuition\n",
    "\n",
    "### 1. Geometric Intuition\n",
    "**Goal:** Find a projection that maximizes class separability.\n",
    "\n",
    "* **Think:** \"Squeeze\" the points of the same class together, and \"Push\" the centers of different classes apart.\n",
    "* **The Result:** For $K$ classes, LDA finds at most $(K-1)$ discriminant axes (directions).\n",
    "\n",
    "\n",
    "\n",
    "**Objective Function:**\n",
    "$$J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "* **$w$**: The projection vector (what we are solving for).\n",
    "* **$S_B$**: Between-class scatter matrix (Measure of separation).\n",
    "* **$S_W$**: Within-class scatter matrix (Measure of compactness).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When to Use What? (The Master Table)\n",
    "\n",
    "| Condition | Use **LDA**? | Better Alternative |\n",
    "| :--- | :--- | :--- |\n",
    "| **Normal Features** | ✅ **Works Well** | -- |\n",
    "| **Non-Normal** | ❌ Poor Performance | **Logistic Regression** |\n",
    "| **Equal Variance** | ✅ **Optimal** | -- |\n",
    "| **Unequal Variance** | ⚠️ Suboptimal | **QDA** (Quadratic DA) |\n",
    "| **Linear Separable** | ✅ **Excellent** | -- |\n",
    "| **Non-Linear** | ❌ Fails | **SVM** (RBF Kernel) |\n",
    "| **Many Features** | ❌ Singular $S_W$ | **Regularized LDA** |\n",
    "| **Small $n$** | ⚠️ Overfits | **Naive Bayes** |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Top Interview Questions (Q&A)\n",
    "\n",
    "#### Q1: LDA vs. Logistic Regression - When to choose which?\n",
    "Both are linear classifiers, but they make different assumptions.\n",
    "* **Choose LDA when:**\n",
    "    1.  Features are approximately **Normally Distributed**.\n",
    "    2.  Classes have similar **Covariance** (spread).\n",
    "    3.  **Small sample size** (LDA is more data-efficient).\n",
    "    4.  You need probabilistic outputs with Gaussian assumptions.\n",
    "* **Choose Logistic Regression when:**\n",
    "    1.  Features are **not normal** (binary, counts, skewed).\n",
    "    2.  You want interpretable coefficients (log-odds).\n",
    "    3.  You need to handle many features (L1/L2 regularization handles this better).\n",
    "* **Key Insight:** LDA models $P(X|Y)$ (Generative), while LR models $P(Y|X)$ (Discriminative).\n",
    "\n",
    "#### Q2: Can LDA handle more than 2 classes? How?\n",
    "**Yes.** LDA handles multiple classes naturally; it does not need One-vs-Rest.\n",
    "* **Solution:** It finds $(K-1)$ discriminant axes.\n",
    "* **Classification:** It projects data to this lower-dimensional space and uses Mahalanobis distance to find the closest class mean.\n",
    "* **Example:** If you have 10 classes, LDA finds at most 9 discriminant components.\n",
    "\n",
    "#### Q3: What happens when $S_W$ is singular? How to fix?\n",
    "$S_W$ (Within-class scatter) becomes non-invertible (singular) when $N_{samples} < N_{features}$ or when features are perfectly correlated.\n",
    "* **Fix 1 (Regularization):** Add $\\lambda I$ to $S_W$ (Shrinkage LDA).\n",
    "* **Fix 2 (PCA):** Run PCA first to reduce dimensions, then run LDA.\n",
    "* **Fix 3 (Feature Selection):** Remove correlated features.\n",
    "\n",
    "#### Q4: How does LDA differ from ANOVA?\n",
    "* **ANOVA:** Univariate. It tests if group means are different for **one** feature at a time.\n",
    "* **LDA:** Multivariate. It finds a linear combination of **all** features that maximizes separation.\n",
    "* **Think:** ANOVA is 1D LDA. LDA is Multivariate ANOVA + Dimensionality Reduction.\n",
    "\n",
    "#### Q5: Can LDA be used for feature selection?\n",
    "**Yes.** The magnitude of the discriminant coefficients indicates feature importance (assuming data is scaled). You can also use Stepwise LDA to add/remove features based on their separation power.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Mathematical Summary\n",
    "\n",
    "1.  **Within-class scatter:**\n",
    "    $$S_W = \\sum \\sum (x - \\mu_c)(x - \\mu_c)^T$$\n",
    "2.  **Between-class scatter:**\n",
    "    $$S_B = \\sum n_c (\\mu_c - \\mu)(\\mu_c - \\mu)^T$$\n",
    "3.  **Optimization:**\n",
    "    $$\\max_w \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "4.  **Solution:**\n",
    "    Eigenvectors of $S_W^{-1} S_B$.\n",
    "5.  **Components:**\n",
    "    $\\min(K-1, p)$ where $K=$ classes, $p=$ features.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Final Quick Comparison\n",
    "\n",
    "* **LDA vs PCA?** $\\rightarrow$ LDA: Supervised (Max Class Separation); PCA: Unsupervised (Max Variance).\n",
    "* **LDA vs QDA?** $\\rightarrow$ LDA: Linear boundary (Equal Covariance); QDA: Quadratic boundary (Different Covariances).\n",
    "* **LDA vs Logistic Regression?** $\\rightarrow$ LDA: Generative (Assumes Normality); LR: Discriminative (Robust to non-normality).\n",
    "* **Assumption Checklist:** Normality, Equal Covariance, Linear Separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[4.0, 2.0], [2.0, 4.0], [2.0, 3.0], [3.0, 6.0], [4.0, 4.0]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "# Create LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "\n",
    "# Predict class\n",
    "print(lda.predict([[3.0, 4.0]]))\n",
    "\n",
    "# Transform data to lower dimension\n",
    "X_new = lda.transform(X)\n",
    "print(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec1e80",
   "metadata": {},
   "source": [
    "| Feature               | LDA                              | QDA                                 |\n",
    "| --------------------- | -------------------------------- | ----------------------------------- |\n",
    "| Covariance assumption | Same for all classes             | Different for each class            |\n",
    "| Decision boundary     | Linear                           | Quadratic                           |\n",
    "| Flexibility           | Less                             | More                                |\n",
    "| Number of parameters  | Fewer → less risk of overfitting | More → can overfit if small dataset |\n",
    "| When to use           | Classes have similar variance    | Classes have different variance     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57650b",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "QDA is a supervised classification algorithm similar to LDA, but it is **more flexible**.\n",
    "\n",
    "**The Main Difference:**\n",
    "* **LDA** assumes all classes share the **same** Covariance Matrix ($\\Sigma_{shared}$). It draws a **straight line**.\n",
    "* **QDA** assumes each class has its **own** Covariance Matrix ($\\Sigma_k$). It draws a **quadratic curve**.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Key Concepts\n",
    "\n",
    "* **Per-Class Covariance:** It acknowledges that \"Class A might be a tight circle, while Class B is a wide oval.\" It doesn't force them to have the same shape.\n",
    "* **Quadratic Boundary:** Because the spreads are different, the optimal separating line becomes a curve (parabola, hyperbola, or ellipse).\n",
    "\n",
    "### 2. LDA vs. QDA Comparison\n",
    "\n",
    "| Feature | **LDA** (Linear) | **QDA** (Quadratic) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Covariance Matrix** | **Shared** ($\\Sigma$) | **Specific per class** ($\\Sigma_k$) |\n",
    "| **Decision Boundary** | Linear (Straight line/plane) | Quadratic (Curved) |\n",
    "| **Flexibility** | Low (High Bias) | High (Low Bias) |\n",
    "| **Parameters** | Few (Less overfitting) | Many (Risk of overfitting) |\n",
    "| **Best Used When** | Classes have similar spread. | Classes have **different spreads**. |\n",
    "\n",
    "### 3. When to Use QDA?\n",
    "1.  **Unequal Variances:** When one class is much more spread out than the other.\n",
    "2.  **Non-Linear Separation:** When a straight line cannot separate the classes well.\n",
    "3.  **Large Sample Size:** QDA needs more data than LDA because it has to estimate a full covariance matrix for *every* class.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Steps of QDA\n",
    "1.  **Compute Means:** Calculate the mean vector $\\mu_k$ for each class.\n",
    "2.  **Compute Covariance Matrices:** Calculate a separate matrix $\\Sigma_k$ for each class.\n",
    "    * *Note: In LDA, we would average these into one. In QDA, we keep them separate.*\n",
    "3.  **Compute Posterior:** Use Bayes' Theorem with a multivariate Gaussian formula.\n",
    "4.  **Classify:** Assign the sample to the class with the highest posterior probability.\n",
    "\n",
    "**The Math (Simplified):**\n",
    "The discriminant function $\\delta_k(x)$ includes a term like:\n",
    "$$x^T \\Sigma_k^{-1} x$$\n",
    "Because $\\Sigma_k$ is different for each class, this $x^2$ term (quadratic) remains in the final equation, creating the curved boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| **Better Fit:** Can model complex, non-linear boundaries. | **Overfitting:** Requires estimating many more parameters ($p(p+1)/2$ parameters per class). |\n",
    "| **Handles Variance:** Excellent when class spreads differ drastically. | **Data Hungry:** Needs more data samples to estimate those parameters accurately. |\n",
    "| **Probabilistic:** Gives actual probabilities, not just labels. | **Assumptions:** Still assumes data is Normally Distributed. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. FAQ\n",
    "\n",
    "**Q: Difference between LDA and QDA?**\n",
    "**A:**\n",
    "* **LDA:** Shared covariance $\\rightarrow$ Linear boundary.\n",
    "* **QDA:** Per-class covariance $\\rightarrow$ Quadratic boundary.\n",
    "\n",
    "**Q: When to prefer QDA over LDA?**\n",
    "**A:** When you know (or suspect) that the variance/spread of the classes is very different. (e.g., The \"Fraud\" class is very spread out, but \"Non-Fraud\" is very tight).\n",
    "\n",
    "**Q: Can QDA handle more than 2 classes?**\n",
    "**A:** Yes, it naturally handles multi-class classification by calculating probabilities for each class and picking the winner.\n",
    "\n",
    "**Q: Does QDA require feature scaling?**\n",
    "**A:** Not strictly (because it learns the variance), but it helps numerical stability. However, normalization is usually recommended for all Gaussian-based models.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Rule of Thumb\n",
    "* **LDA:** Simple, Robust, Low Variance (Use for small data / similar spreads).\n",
    "* **QDA:** Flexible, Complex, Low Bias (Use for large data / different spreads)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e192d6",
   "metadata": {},
   "source": [
    "# QDA: Deep Dive & Nuances\n",
    "\n",
    "### 1. Geometric Intuition\n",
    "\n",
    "* **The Core Idea:** QDA fits a **separate Gaussian distribution** to each class.\n",
    "* **The Shape:** Unlike LDA, which forces all classes to have the same shape (shared covariance), QDA allows each class to have its own unique \"shape\" (mean + specific covariance matrix).\n",
    "* **The Result:** The decision boundary emerges from comparing these overlapping multivariate Gaussians. It naturally forms curves (ellipses, parabolas, hyperbolas).\n",
    "\n",
    "\n",
    "\n",
    "**The Math (Bayes' Theorem in Action):**\n",
    "For class $k$, we assume the data follows a multivariate normal distribution:\n",
    "$$P(X | Y = k) \\sim N(\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Using Bayes' Theorem:\n",
    "$$P(Y = k | X) \\propto P(X | Y = k) \\cdot P(Y = k)$$\n",
    "\n",
    "**The Discriminant Function ($\\delta_k$):**\n",
    "After taking logs and simplifying, we get the score for class $k$:\n",
    "$$\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) - \\frac{1}{2} \\ln|\\Sigma_k| + \\ln \\pi_k$$\n",
    "\n",
    "* **Note:** The term $x^T \\Sigma_k^{-1} x$ is quadratic. In LDA, $\\Sigma_k$ is the same for all classes, so the quadratic term cancels out. In QDA, it stays, creating the curve.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Bias-Variance Tradeoff (The Parameter Explosion)\n",
    "\n",
    "QDA is much more \"data hungry\" than LDA because it has to estimate many more numbers.\n",
    "\n",
    "**Parameter Counting (for $K$ classes and $d$ features):**\n",
    "\n",
    "* **LDA:** $K \\cdot d$ (means) + $d(d+1)/2$ (shared covariance) + $K$ (priors).\n",
    "* **QDA:** $K \\cdot d$ (means) + $K \\cdot d(d+1)/2$ (**separate covariances**) + $K$ (priors).\n",
    "\n",
    "**Example ($d=10$ features, $K=3$ classes):**\n",
    "* **LDA:** $30 + 55 + 3 = \\mathbf{88}$ parameters.\n",
    "* **QDA:** $30 + 165 + 3 = \\mathbf{198}$ parameters.\n",
    "* *Result:* QDA has ~2.25x more parameters to learn, meaning it has **Higher Variance** and needs more training data to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Top Interview Questions (Q&A)\n",
    "\n",
    "#### Q1: When does QDA outperform LDA?\n",
    "**A:** QDA wins when:\n",
    "1.  **Covariances Differ:** Class spreads are significantly different (e.g., Class A is tight, Class B is wide). Check this with **Bartlett's test**.\n",
    "2.  **Sufficient Data:** You have enough samples to estimate those extra parameters (Rule of thumb: $10 \\times \\text{features}$ per class).\n",
    "3.  **Non-Linearity:** The true decision boundary is curved.\n",
    "\n",
    "#### Q2: What happens when covariance matrices are singular?\n",
    "**A:** \"Singularity\" means the matrix cannot be inverted (determinant is 0). This happens if:\n",
    "* $N_{samples} < N_{features}$ (High-dimensional, low-sample data).\n",
    "* Features are perfectly correlated (collinearity).\n",
    "* **Solution:** Use **Regularization**. Add a small value to the diagonal: $\\Sigma_{new} = \\Sigma + \\lambda I$. This is often called \"Regularized QDA.\"\n",
    "\n",
    "#### Q3: Can QDA handle categorical features directly?\n",
    "**A:** **No.** QDA assumes features are **Continuous** and **Gaussian**.\n",
    "* *Workaround:* You can technically use One-Hot Encoding, but the resulting matrix is often singular or sparse, breaking the Gaussian assumption. It's better to use Naive Bayes for categorical data.\n",
    "\n",
    "#### Q4: How do you visualize QDA boundaries?\n",
    "**A:** Since boundaries are quadratic:\n",
    "* **2D:** Plot contours of the discriminant function.\n",
    "* **Higher Dimensions:** You cannot easily visualize the \"curve\" in 10D. You typically project data into 2D (using PCA or LDA) and then plot the QDA boundaries on top, though this is an approximation.\n",
    "\n",
    "#### Q5: Can QDA be used for feature selection?\n",
    "**A:** **Not really.** Unlike LDA (which gives you discriminant vectors with weights), QDA's parameters are hidden inside complex quadratic matrices. There are no simple coefficients like $w_1, w_2$ to rank features.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Summary Cheat Sheet\n",
    "\n",
    "| Condition | **QDA** Recommendation |\n",
    "| :--- | :--- |\n",
    "| **Small Sample Size** |  **Avoid** (Use LDA or Naive Bayes) |\n",
    "| **High Dimensions ($p \\gg n$)** |  **Avoid** (Covariance becomes singular) |\n",
    "| **Different Class Spreads** |  **Use** (Captures variance differences) |\n",
    "| **Non-Linear Boundary** |  **Use** (Captures curves) |\n",
    "| **Interpretability** |  **Low** (No simple coefficients) |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mathematical Summary\n",
    "\n",
    "1.  **Class-conditional density:** $$P(X|Y=k) = N(\\mu_k, \\Sigma_k)$$\n",
    "2.  **Discriminant function:** $$\\delta_k(x) = -\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k) - \\frac{1}{2}\\ln|\\Sigma_k| + \\ln\\pi_k$$\n",
    "3.  **Decision rule:** $$\\hat{y} = \\arg\\max_k \\delta_k(x)$$\n",
    "4.  **Total Parameters:** $$K \\left[ d + \\frac{d(d+1)}{2} + 1 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[4.0, 2.0], [2.0, 4.0], [2.0, 3.0], [3.0, 6.0], [4.0, 4.0]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "# Create QDA model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X, y)\n",
    "\n",
    "# Predict class\n",
    "print(qda.predict([[3.0, 4.0]]))\n",
    "\n",
    "# Posterior probabilities\n",
    "print(qda.predict_proba([[3.0, 4.0]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
