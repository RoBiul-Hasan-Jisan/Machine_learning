{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cff431c",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "Logistic Regression is a supervised classification algorithm used to predict probabilities for binary outcomes (0 or 1).\n",
    "\n",
    "Despite the name, it is used for classification, not regression.\n",
    "\n",
    "\n",
    "Core Idea\n",
    "\n",
    "Uses a linear combination of features, then applies the sigmoid function to squash the output into a probability between 0 and 1\n",
    "Uses Linear Model + Sigmoid Function\n",
    "\n",
    "Output is probability between 0 and 1\n",
    "\n",
    "\n",
    "Linear Part  z = wx + b\n",
    "\n",
    "Sigmoid Function  σ(z) = 1/ (1+e^(−z))\n",
    "\n",
    "\n",
    "Converts linear output into probability\n",
    "\n",
    "Decision Boundary\n",
    "\n",
    "if probability ≥ 0.5 → Class 1\n",
    "\n",
    "else → Class 0\n",
    "\n",
    "Loss Function\n",
    "\n",
    "Binary Cross-Entropy (Log Loss)\n",
    "\n",
    "Loss=−[ylog(p)+(1−y)log(1−p)]\n",
    "\n",
    "MSE is not used because sigmoid is non-linear\n",
    "\n",
    "How It Learns\n",
    "\n",
    "Updates parameters (β₀, β₁) to minimize log loss, typically using an optimization algorithm like Gradient Descent\n",
    "\n",
    "Uses Gradient Descent\n",
    "\n",
    "Updates weights to minimize log loss\n",
    "\n",
    "Why Not Linear Regression?\n",
    "\n",
    "Linear regression outputs values beyond [0,1]\n",
    "\n",
    "Poor for classification\n",
    "\n",
    "Logistic Regression gives probability\n",
    "\n",
    "Evaluation Metrics\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1-Score\n",
    "\n",
    "ROC-AUC\n",
    "\n",
    "Assumptions\n",
    "\n",
    "1.Outcome is binary.\n",
    "\n",
    "2.Observations are independent.\n",
    "\n",
    "3.Linear relationship between log-odds and features. \n",
    "\n",
    "4.Little multicollinearity\n",
    "\n",
    "Why sigmoid?\n",
    "\n",
    "Converts output to probability\n",
    "\n",
    "Why log loss?\n",
    "\n",
    "Penalizes wrong confident predictions more\n",
    "\n",
    "Is Logistic Regression linear?\n",
    "\n",
    "Linear in parameters, non-linear in output\n",
    "\n",
    "Can it do multi-class?\n",
    "\n",
    "Yes, using One-Vs-Rest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c570c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            z = self.w * X + self.b\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            dw = (1/n) * np.sum(X * (y_pred - y))\n",
    "            db = (1/n) * np.sum(y_pred - y)\n",
    "\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = self.w * X + self.b\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return [1 if i >= 0.5 else 0 for i in y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b4957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict(np.array([3, 5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b0d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[[0.64726666 0.35273334]\n",
      " [0.18436618 0.81563382]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict([[3], [5]]))\n",
    "print(model.predict_proba([[3], [5]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65bcda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0470438]]\n",
      "[-3.74817743]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db60b43",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "LDA is a supervised dimensionality reduction and classification technique.\n",
    "\n",
    "It finds a linear combination of features that best separates two or more classes.\n",
    "\n",
    "Mainly used for classification.\n",
    "\n",
    "Maximize between-class variance and minimize within-class variance.\n",
    "\n",
    "Key Concepts\n",
    "\n",
    "Within-class variance → how spread out data is in a class\n",
    "\n",
    "Between-class variance → how far apart class means are\n",
    "\n",
    "Projection → LDA projects data into a lower-dimensional space (like 1D or 2D) that maximizes class separability\n",
    "\n",
    "```bash\n",
    "\n",
    "\n",
    " Feature     LDA                                         PCA                                                      \n",
    " ----------  ------------------------------------------  -------------------------------------------------------- \n",
    " Supervised   uses class labels                           unsupervised                                            \n",
    " Goal        Maximize class separability                 Maximize variance of data                                \n",
    " Output      Lower-dimensional space for classification  Lower-dimensional space for reconstruction / compression \n",
    " Use case    Classification                              Dimensionality reduction                                 \n",
    "\n",
    "```\n",
    "When to Use LDA\n",
    "\n",
    "Classification tasks\n",
    "\n",
    "Reduce dimensionality before classification\n",
    "\n",
    "Small datasets (better than PCA for class separation)\n",
    "\n",
    "Steps of LDA\n",
    "\n",
    "Compute mean vectors for each class\n",
    "\n",
    "Compute within-class scatter matrix (SW)\n",
    "\n",
    "Compute between-class scatter matrix (SB)\n",
    "\n",
    "Solve eigenvalue problem for SW^−1 *  SB\n",
    "\n",
    "Select top eigenvectors → new subspace\n",
    "\n",
    "Project data → classify using  nearest centroid\n",
    "\n",
    "LDA as Classifier\n",
    "\n",
    "After projecting data, assign a sample to the class with closest mean in the LDA space\n",
    "\n",
    "Assumes features are normally distributed\n",
    "\n",
    "Advantages\n",
    "\n",
    "Simple, fast, easy to interpret\n",
    "\n",
    "Reduces dimensionality while improving class separation\n",
    "\n",
    "Works well for linearly separable classes\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Assumes normal distribution of features\n",
    "\n",
    "Assumes same covariance matrix for all classes\n",
    "\n",
    "Poor performance if classes are not linearly separable\n",
    "\n",
    "\n",
    "Difference between LDA and Logistic Regression?\n",
    "LDA assumes normality, Logistic Regression does not; LDA uses class distributions.\n",
    "\n",
    "Q: Difference between LDA and PCA?\n",
    "LDA is supervised, PCA is unsupervised.\n",
    "\n",
    "Q: Can LDA be used for regression?\n",
    "No, only for classification.\n",
    "\n",
    "Q: When to use LDA?\n",
    "Small datasets, linearly separable classes, feature reduction + classification.\n",
    "\n",
    "PCA → “max variance” (unsupervised), LDA → “max class separation” (supervised)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5eed43",
   "metadata": {},
   "source": [
    "Geometric Intuition:\n",
    "\n",
    "Goal: Find projection that maximizes class separability\n",
    "\n",
    "Think: \"Squeeze\" same classes together, \"push\" different classes apart\n",
    "\n",
    "\n",
    "For K classes, LDA finds at most (K-1) discriminant axes that maximize:\n",
    "\n",
    "Objective: J(w) = (wᵀS_B w) / (wᵀS_W w)\n",
    "where:\n",
    "\n",
    "  w = projection vector (what we're solving for)\n",
    "\n",
    "  S_B = Between-class scatter matrix (separation measure)\n",
    "\n",
    "  S_W = Within-class scatter matrix (compactness measure)\n",
    "\n",
    "```bash\n",
    "┌─────────────────┬────────────────────────┬─────────────────────────┐\n",
    "│ Condition       │ Use LDA                │ Use Alternative         │\n",
    "├─────────────────┼────────────────────────┼─────────────────────────┤\n",
    "│ Normal features │  Works well           │                         │\n",
    "│ Non-normal      │  Poor performance     │ Logistic Regression     │\n",
    "│ Equal variance  │  Optimal              │                         │\n",
    "│ Unequal var.    │  Suboptimal           │ QDA (Quadratic DA)      │\n",
    "│ Linear sep.     │  Excellent            │                         │\n",
    "│ Non-linear      │  Fails                │ SVM with RBF kernel     │\n",
    "│ Many features   │  Singular S_W         │ Regularized LDA         │\n",
    "│ Small n         │  Overfits             │ Naive Bayes             │\n",
    "└─────────────────┴────────────────────────┴─────────────────────────┘\n",
    "```\n",
    "Q1: LDA vs Logistic Regression - When to choose which?\n",
    "\n",
    "Both are linear classifiers, but different assumptions:\n",
    "\n",
    "Choose LDA when:\n",
    "1. Features are (approximately) normally distributed\n",
    "2. Classes have similar covariance structures\n",
    "3. Small sample size (LDA is more data-efficient)\n",
    "4. Need probabilistic outputs with Gaussian assumption\n",
    "\n",
    "Choose Logistic Regression when:\n",
    "1. Features are not normal (binary, counts, etc.)\n",
    "2. Want interpretable coefficients (log-odds)\n",
    "3. Need to handle many features (L1/L2 regularization)\n",
    "4. Model misspecification is a concern (LR is more robust)\n",
    "\n",
    "Key insight: LDA models P(X|Y), LR models P(Y|X)\n",
    "\n",
    "Q2: Can LDA handle more than 2 classes? How?\n",
    "\n",
    "\"Yes, LDA naturally extends to multi-class:\n",
    "\n",
    "One-vs-Rest approach: Not needed, LDA handles multiple classes directly\n",
    "\n",
    "Solution: Find (K-1) discriminant axes that maximize class separation\n",
    "\n",
    "Classification: Project to (K-1) dimensional space, use Mahalanobis distance\n",
    "\n",
    "Visualization: For K classes, you can visualize in up to (K-1) dimensions\n",
    "\n",
    "Example: 10 classes → at most 9 discriminant components\"\n",
    "\n",
    "Q3: What happens when S_W is singular? How to fix?\n",
    "\n",
    "\"S_W becomes singular when:\n",
    "\n",
    "n_samples < n_features (common in genomics, text)\n",
    "\n",
    "Features are linearly dependent\n",
    "\n",
    "Constant features or zero variance\n",
    "\n",
    "slv\n",
    "\n",
    "Regularization: Add λI to S_W (shrinkage LDA)\n",
    "\n",
    "PCA preprocessing: Reduce dimensions first\n",
    "\n",
    "Feature selection: Remove correlated/irrelevant features\n",
    "\n",
    "Use pseudoinverse: SVD-based solution\n",
    "\n",
    "Kernel LDA: Map to higher dimension where data is linearly separable\"\n",
    "\n",
    "Q4: How does LDA differ from ANOVA?\n",
    "\n",
    "\n",
    "\"Both separate group means, but:\n",
    "\n",
    "• ANOVA: Tests if group means are different (univariate, one feature)\n",
    "\n",
    "• LDA: Finds linear combination that maximizes separation (multivariate)\n",
    "\n",
    "Think: ANOVA is 1D LDA. LDA = multivariate ANOVA + dimension reduction.\"\n",
    "\n",
    "Q5: Can LDA be used for feature selection?\n",
    "\n",
    "\"Yes, two ways:\n",
    "\n",
    "Discriminant coefficients: Magnitude indicates feature importance\n",
    "\n",
    "Stepwise LDA: Add/remove features based on discriminant power\n",
    "\n",
    "But caution: LDA coefficients assume linear separability and equal variance.\"\n",
    "\n",
    "\n",
    "When to Use LDA:\n",
    "\n",
    " Small to medium datasets\n",
    "\n",
    " Normally distributed features\n",
    "\n",
    " Linear class boundaries\n",
    "\n",
    " Need dimensionality reduction + classification\n",
    "\n",
    " Interpretable feature importance needed\n",
    "\n",
    "When to Avoid LDA:\n",
    "\n",
    " Highly non-normal features\n",
    "\n",
    " Non-linear decision boundaries\n",
    "\n",
    " Very high-dimensional data (n_features >> n_samples)\n",
    "\n",
    " Classes with very different variances\n",
    "\n",
    " Need for non-linear interactions\n",
    " \n",
    "\n",
    "1. Within-class scatter: S_W = Σ Σ (x - μ_c)(x - μ_c)ᵀ\n",
    "2. Between-class scatter: S_B = Σ n_c (μ_c - μ)(μ_c - μ)ᵀ\n",
    "3. Objective: max_w (wᵀS_B w) / (wᵀS_W w)\n",
    "4. Solution: eigenvectors of S_W^{-1} S_B\n",
    "5. components: min(K-1, p) where K=classes, p=features\n",
    "\n",
    "Q: LDA vs PCA? → LDA: supervised, max class separation; PCA: unsupervised, max variance\n",
    "\n",
    "Q: LDA vs QDA? → LDA: linear, equal covariance; QDA: quadratic, different covariances\n",
    "\n",
    "Q: LDA vs Logistic Regression? → LDA: generative, models P(X|Y); LR: discriminative, models P(Y|X)\n",
    "\n",
    "Q: Assumptions? → Normality, equal covariance, linear separability\n",
    "\n",
    "Q: Singular S_W fix? → Regularization, PCA first, feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[4.0, 2.0], [2.0, 4.0], [2.0, 3.0], [3.0, 6.0], [4.0, 4.0]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "# Create LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "\n",
    "# Predict class\n",
    "print(lda.predict([[3.0, 4.0]]))\n",
    "\n",
    "# Transform data to lower dimension\n",
    "X_new = lda.transform(X)\n",
    "print(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323eb09b",
   "metadata": {},
   "source": [
    "# QDA\n",
    "\n",
    "QDA is a supervised classification algorithm like LDA.\n",
    "\n",
    "Main difference: assumes each class has its own covariance matrix, instead of sharing one (as in LDA).\n",
    "\n",
    "Decision boundary is quadratic instead of linear.\n",
    "\n",
    "Allow different spreads for each class → more flexible decision boundary\n",
    "\n",
    "\n",
    "\n",
    "Within-class covariance → each class has its own covariance matrix\n",
    "\n",
    "Decision boundary → quadratic curve separating classes\n",
    "\n",
    "Works best when class variances are different\n",
    "\n",
    "When to Use QDA\n",
    "\n",
    "Classification tasks\n",
    "\n",
    "Classes have different spreads / covariance\n",
    "\n",
    "Small or moderate-dimensional data (high-dimensional → overfitting risk)\n",
    "\n",
    "Steps of QDA\n",
    "\n",
    "Compute mean vector for each class\n",
    "\n",
    "Compute covariance matrix for each class\n",
    "\n",
    "Compute posterior probability for each class using Bayes’ theorem\n",
    "\n",
    "Assign sample to class with highest posterior probability\n",
    "\n",
    "\n",
    "Advantages\n",
    "\n",
    "Can model non-linear boundaries\n",
    "\n",
    "Better than LDA when class covariances differ\n",
    "\n",
    "Probabilistic output (posterior probabilities)\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "More parameters → risk of overfitting for small datasets\n",
    "\n",
    "Assumes normal distribution of features\n",
    "\n",
    "Less interpretable than LDA\n",
    "\n",
    "Q: Difference between LDA and QDA?\n",
    "\n",
    "LDA → shared covariance → linear boundary, QDA → per-class covariance → quadratic boundary\n",
    "\n",
    "Q: When to prefer QDA over LDA?\n",
    "\n",
    "Class variances are different\n",
    "\n",
    "Q: Can QDA handle more than 2 classes?\n",
    "\n",
    "Yes, multi-class classification\n",
    "\n",
    "Q: Does QDA require feature scaling?\n",
    "\n",
    "Not strictly, but helps sometimes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LDA → Linear boundary, shared covariance\n",
    "QDA → Quadratic boundary, per-class covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec1e80",
   "metadata": {},
   "source": [
    "| Feature               | LDA                              | QDA                                 |\n",
    "| --------------------- | -------------------------------- | ----------------------------------- |\n",
    "| Covariance assumption | Same for all classes             | Different for each class            |\n",
    "| Decision boundary     | Linear                           | Quadratic                           |\n",
    "| Flexibility           | Less                             | More                                |\n",
    "| Number of parameters  | Fewer → less risk of overfitting | More → can overfit if small dataset |\n",
    "| When to use           | Classes have similar variance    | Classes have different variance     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57650b",
   "metadata": {},
   "source": [
    "Geometric Intuition:\n",
    "\n",
    "QDA fits a separate Gaussian distribution to each class\n",
    "\n",
    "Each class gets its own \"shape\" (mean + covariance matrix)\n",
    "\n",
    "Decision boundary emerges from comparing these multivariate Gaussians\n",
    "\n",
    "For class k, assume data follows multivariate normal distribution:\n",
    "\n",
    "\n",
    "P(X | Y = k) ~ N(μ_k, Σ_k)\n",
    "\n",
    "Using Bayes' Theorem:\n",
    "\n",
    "P(Y = k | X) ∝ P(X | Y = k) * P(Y = k)\n",
    "\n",
    "After taking logs and simplifying:\n",
    "\n",
    "Discriminant function δ_k(x) = -½(x - μ_k)ᵀΣ_k⁻¹(x - μ_k) - ½log|Σ_k| + log π_k\n",
    "\n",
    "where π_k = prior probability of class k\n",
    "\n",
    "Bias-Variance Tradeoff in QDA:\n",
    "\n",
    "Number of parameters to estimate:\n",
    "\n",
    "For K classes with d features:\n",
    "\n",
    "LDA: K*d (means) + d(d+1)/2 (shared covariance) + K (priors)\n",
    "\n",
    "QDA: K*d (means) + K*d(d+1)/2 (separate covariances) + K (priors)\n",
    "\n",
    "Example: d=10, K=3\n",
    "\n",
    "LDA: 30 + 55 + 3 = 88 parameters\n",
    "\n",
    "QDA: 30 + 165 + 3 = 198 parameters\n",
    "\n",
    "QDA has 2.25x more parameters → needs more data\n",
    "\n",
    "\n",
    "Q1: When does QDA outperform LDA?\n",
    "\n",
    "\"QDA outperforms LDA when:\n",
    "\n",
    "Class covariance matrices are significantly different (check with Bartlett's test)\n",
    "\n",
    "Sample size is large enough (rule of thumb: at least 10× features per class)\n",
    "\n",
    "True decision boundary is quadratic/non-linear\n",
    "\n",
    "Classes have different spreads or orientations\n",
    "\n",
    "Example: Class A is tightly clustered, Class B is widely spread → QDA will capture this \n",
    "\n",
    "better.\"\n",
    "\n",
    "Q2: How many parameters does QDA need to estimate?\n",
    "\n",
    "\"For K classes with d features:\n",
    "\n",
    "Means: K × d parameters\n",
    "\n",
    "Covariance matrices: K × d(d+1)/2 parameters (each symmetric)\n",
    "\n",
    "Priors: K parameters\n",
    "\n",
    "Total: K[d + d(d+1)/2 + 1] parameters\n",
    "\n",
    "Example: 3 classes, 10 features → 3[10 + 55 + 1] = 198 parameters\n",
    "\n",
    "This explains why QDA needs much more data than LDA!\"\n",
    "\n",
    "Q3: What happens when covariance matrices are singular?\n",
    "\n",
    "\"Singular covariance occurs when:\n",
    "\n",
    "n_samples < n_features (common in high-dim low-sample settings)\n",
    "\n",
    "Features are linearly dependent\n",
    "\n",
    "Perfect collinearity\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Regularization: Add λI to covariance matrices (reg_param in sklearn)\n",
    "\n",
    "Feature selection/reduction: Use PCA or filter methods first\n",
    "\n",
    "Use diagonal QDA: Assumes features are independent (like Naive Bayes)\n",
    "\n",
    "Shrinkage QDA: Shrink toward pooled covariance (like LDA-QDA hybrid)\"\n",
    "\n",
    "Q4: Can QDA handle categorical features?\n",
    "\n",
    "\"Directly? No. QDA assumes continuous, normally distributed features.\n",
    "\n",
    "Workarounds:\n",
    "\n",
    "Encoding: Use one-hot encoding, but beware of curse of dimensionality\n",
    "\n",
    "Separate modeling: Model categorical features with different distributions\n",
    "\n",
    "Mixed models: Use QDA for continuous, Naive Bayes for categorical\n",
    "\n",
    "Kernel methods: Map to continuous space\n",
    "\n",
    "Better: Use models designed for mixed data types.\"\n",
    "\n",
    "Q5: How to visualize QDA decision boundaries?\n",
    "\n",
    "\"Three approaches:\n",
    "\n",
    "2D feature space: Plot contours of discriminant functions\n",
    "\n",
    "LD1-LD2 projection: Project to LDA space first, then apply QDA\n",
    "\n",
    "Pairwise plots: For multi-class, plot each pair of features\n",
    "\n",
    "Key insight: QDA boundaries can be ellipses, parabolas, or hyperbolas depending on \n",
    "covariance differences.\"\n",
    "\n",
    "When to Use QDA :\n",
    "\n",
    "Different class covariances/spreads\n",
    "\n",
    "Moderate to large sample sizes\n",
    "\n",
    "Quadratic/non-linear decision boundaries\n",
    "\n",
    "Need probabilistic outputs\n",
    "\n",
    "Visualization of class distributions\n",
    "\n",
    "When to Avoid QDA:\n",
    "\n",
    " Very small sample sizes\n",
    "\n",
    " High-dimensional data (p >> n)\n",
    "\n",
    " Features not normally distributed\n",
    "\n",
    " Computational efficiency needed\n",
    "\n",
    " Need for interpretable linear coefficients\n",
    "\n",
    "1. Class-conditional density: P(X|Y=k) = N(μ_k, Σ_k)\n",
    "\n",
    "2. Discriminant function: δ_k(x) = -½(x-μ_k)ᵀΣ_k⁻¹(x-μ_k) - ½log|Σ_k| + logπ_k\n",
    "\n",
    "3. Decision rule: ŷ = argmax_k δ_k(x)\n",
    "\n",
    "4.  parameters: K[d + d(d+1)/2 + 1]\n",
    "\n",
    "\n",
    "Q: QDA vs LDA? → QDA: separate covariances, quadratic boundaries; LDA: shared covariance,\n",
    "\n",
    "linear boundaries\n",
    "\n",
    "Q: When QDA fails? → Small samples, high dimensions, non-normal data\n",
    "\n",
    "Q: Regularization? → Add λI to covariance matrices to prevent singularity\n",
    "\n",
    "Q: Multi-class? → Direct extension works naturally\n",
    "\n",
    "Q: Feature importance? → Not directly available (unlike LDA coefficients)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[4.0, 2.0], [2.0, 4.0], [2.0, 3.0], [3.0, 6.0], [4.0, 4.0]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "# Create QDA model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X, y)\n",
    "\n",
    "# Predict class\n",
    "print(qda.predict([[3.0, 4.0]]))\n",
    "\n",
    "# Posterior probabilities\n",
    "print(qda.predict_proba([[3.0, 4.0]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
