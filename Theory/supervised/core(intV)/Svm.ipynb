{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0493f432",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "SVM is a powerful supervised learning algorithm used for **classification** (SVC) and **regression** (SVR).\n",
    "\n",
    "**Core Goal:** Find the optimal **Hyperplane** that best separates the classes with the **Maximum Margin**.\n",
    "\n",
    "\n",
    "\n",
    "### 1. The Geometric Intuition\n",
    "Imagine you have red and blue balls on a table. You want to place a stick (hyperplane) to separate them.\n",
    "* You can put the stick in many places.\n",
    "* **The Best Stick:** The one that has the widest possible gap (margin) between the red balls and the blue balls.\n",
    "* **Support Vectors:** The specific data points closest to the hyperplane that \"support\" or define the margin. If you move other points, the boundary doesn't change. If you move a support vector, the boundary moves.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Hard Margin vs. Soft Margin\n",
    "\n",
    "**1. Hard Margin (Strict):**\n",
    "* Does **not** allow any errors. All data points must be correctly classified outside the margin.\n",
    "* **Problem:** Only works if data is perfectly linearly separable. Highly sensitive to outliers (one outlier can ruin the model).\n",
    "\n",
    "**2. Soft Margin (Flexible):**\n",
    "* Allows some misclassifications (slack) to maintain a wider, more generalizable margin.\n",
    "* **Controlled by `C`:** The regularization parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The \"Kernel Trick\" (The Magic)\n",
    "What if the data is not linearly separable (e.g., a red circle inside a blue ring)? You cannot draw a straight line to separate them.\n",
    "\n",
    "**Solution:** Project the data into a **higher dimension**.\n",
    "* In 2D, they overlap.\n",
    "* If we lift the red center up (into 3D), we can slide a flat sheet (hyperplane) between the red and blue points.\n",
    "* **Kernel Trick:** A mathematical shortcut that calculates high-dimensional relationships without actually transforming the data (saving huge computational power).\n",
    "\n",
    "\n",
    "\n",
    "**Common Kernels:**\n",
    "* **Linear:** For simple, large datasets (like text classification).\n",
    "* **Polynomial:** Maps to degree `d`.\n",
    "* **RBF (Radial Basis Function):** Infinite dimensions. The default and most powerful kernel for non-linear data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Important Hyperparameters\n",
    "\n",
    "**1. `C` (Regularization Parameter)**\n",
    "* **High C:** Strict. Tries to classify *everything* correctly. Result: Small margin, risk of Overfitting.\n",
    "* **Low C:** Loose. Accepts some errors to get a wider margin. Result: Smoother boundary, better Generalization.\n",
    "\n",
    "**2. `Gamma` ($\\gamma$) (For RBF Kernel)**\n",
    "* Defines how far the influence of a single training example reaches.\n",
    "* **High Gamma:** Close reach. Points must be very close to be similar. Result: Complex, \"wiggly\" boundary (Overfitting).\n",
    "* **Low Gamma:** Far reach. Even distant points are considered similar. Result: Smooth boundary (Underfitting).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| **Accuracy:** Very effective in high-dimensional spaces. | **Slow:** computationally expensive for large datasets ($>100k$ rows). |\n",
    "| **Memory Efficient:** Uses a subset of training points (support vectors). | **Noise:** Sensitive to overlapping classes and noise. |\n",
    "| **Versatile:** Different Kernel functions for different decision boundaries. | **Black Box:** Harder to interpret probability (unlike Logistic Regression). |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. FAQ (Interview Questions)\n",
    "\n",
    "**Q: Why is it called \"Support Vector\" Machine?**\n",
    "**A:** The decision boundary relies *only* on the data points closest to the line (the Support Vectors). If you remove all other data points, the boundary remains exactly the same.\n",
    "\n",
    "**Q: How does SVM handle multi-class classification?**\n",
    "**A:** SVM is natively binary. For multi-class, it uses:\n",
    "* **One-vs-Rest (OvR):** Trains 1 classifier per class (Class A vs. All Others).\n",
    "* **One-vs-One (OvO):** Trains a classifier for every pair ($A$ vs $B$, $B$ vs $C$, etc.).\n",
    "\n",
    "**Q: When should I use a Linear Kernel vs. RBF?**\n",
    "**A:**\n",
    "* Use **Linear** if features > samples (e.g., Text Classification, DNA).\n",
    "* Use **RBF** if samples > features and the relationship is non-linear.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# SVM requires Feature Scaling!\n",
    "# C=1.0 is default. kernel='rbf' is default.\n",
    "svm_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    ")\n",
    "\n",
    "# svm_model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
