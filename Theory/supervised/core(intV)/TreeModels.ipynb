{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68448f98",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Tree (DT)\n",
    "\n",
    "A **Decision Tree** is a non-parametric supervised learning method used for both **Classification** and **Regression**. It models decisions as a tree-like structure.\n",
    "\n",
    "### 1. Structure & Terminology\n",
    "* **Root Node:** The starting point representing the entire dataset.\n",
    "* **Internal Node:** Represents a test on a specific feature (e.g., \"Is Age > 30?\").\n",
    "* **Branch:** The outcome of the test (e.g., \"Yes\" or \"No\").\n",
    "* **Leaf Node (Terminal):** The final output/prediction (e.g., \"Buy\" or \"Don't Buy\").\n",
    "\n",
    "\n",
    "\n",
    "### 2. Types of Trees\n",
    "| Type | Task | Output |\n",
    "| :--- | :--- | :--- |\n",
    "| **Decision Tree Classifier** | Classification | Class label (0, 1, 2...) |\n",
    "| **Decision Tree Regressor** | Regression | Continuous value (Real number) |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How It Works (The Algorithm)\n",
    "The tree is built using a **Recursive Partitioning** (Divide and Conquer) strategy.\n",
    "\n",
    "1.  **Select the Best Split:** The algorithm iterates through all features and finds the threshold that best separates the data (maximizes purity or minimizes error).\n",
    "2.  **Split Data:** Divide the dataset into subsets based on that split.\n",
    "3.  **Repeat:** Apply the same process recursively to each subset.\n",
    "4.  **Stop:** The process stops when:\n",
    "    * Maximum depth is reached.\n",
    "    * Minimum samples per leaf is reached.\n",
    "    * The node is \"pure\" (all samples belong to one class).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Splitting Criteria (The Math)\n",
    "\n",
    "The goal is to select the split that results in the most homogenous (pure) child nodes.\n",
    "\n",
    "#### A. For Classification\n",
    "**1. Gini Impurity (Default in sklearn)**\n",
    "Measures the probability of misclassifying a randomly chosen element.\n",
    "* Range: $0$ (Pure) to $0.5$ (Random).\n",
    "$$Gini = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "**2. Entropy / Information Gain**\n",
    "Measures the disorder or uncertainty in the data.\n",
    "* Range: $0$ (Pure) to $1$ (High disorder).\n",
    "$$Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "* **Information Gain:** Entropy(Parent) - Weighted Average Entropy(Children).\n",
    "\n",
    "#### B. For Regression\n",
    "**Variance Reduction / MSE**\n",
    "Splits are chosen to minimize the variance (Mean Squared Error) within the child nodes.\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Hyperparameters (Tuning)\n",
    "Controlling these is crucial to prevent the tree from growing too complex.\n",
    "\n",
    "* **`max_depth`**: The maximum number of levels in the tree. (Lower = simpler model).\n",
    "* **`min_samples_split`**: The minimum samples required to split an internal node.\n",
    "* **`min_samples_leaf`**: The minimum samples required to be at a leaf node.\n",
    "* **`max_features`**: The number of features to consider when looking for the best split.\n",
    "* **`criterion`**: The function to measure quality (`gini`, `entropy`, `mse`).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Overfitting & Pruning\n",
    "**The Main Problem:** Decision Trees tend to fit the training data perfectly (High Variance), memorizing noise.\n",
    "\n",
    "**Solutions:**\n",
    "1.  **Pre-Pruning:** Stop the tree early using `max_depth` or `min_samples_leaf`.\n",
    "2.  **Post-Pruning (Cost Complexity Pruning):** Grow the full tree, then remove branches that don't add much power. Controlled by `ccp_alpha` in sklearn.\n",
    "3.  **Ensemble Methods:** Use Random Forest or Gradient Boosting to average out errors.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Feature Importance\n",
    "Decision Trees provide a clear metric for feature selection.\n",
    "* The more a feature is used to make splits (especially near the root), the more important it is.\n",
    "* **Code:** `print(clf.feature_importances_)`\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| **Interpretable:** Easy to visualize and explain to non-experts. | **Overfitting:** Prone to creating complex trees that don't generalize. |\n",
    "| **Versatile:** Handles both Numerical & Categorical features. | **Instability:** Small changes in data can result in a completely different tree. |\n",
    "| **No Scaling:** Requires no feature scaling or normalization. | **Bias:** Can be biased towards dominant classes (need to balance data). |\n",
    "| **Non-Linear:** Captures complex non-linear relationships. | |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. FAQ \n",
    "\n",
    "**Q: How does a decision tree decide splits?**\n",
    "**A:** It greedily selects the feature and threshold that maximizes Information Gain (Classification) or reduces Variance (Regression).\n",
    "\n",
    "**Q: How to prevent overfitting?**\n",
    "**A:** Limit `max_depth`, increase `min_samples_leaf`, or use Pruning techniques.\n",
    "\n",
    "**Q: Difference between classifier and regressor?**\n",
    "**A:** Classifier predicts discrete class labels (using Gini/Entropy). Regressor predicts continuous values (using MSE).\n",
    "\n",
    "**Q: Can Decision Trees handle missing values?**\n",
    "**A:** Conceptually yes (via surrogate splits). *Note: Standard Scikit-Learn implementation historically required imputation, though newer versions support native missing value handling.*\n",
    "\n",
    "**Q: Are Decision Trees sensitive to feature scaling?**\n",
    "**A:** **No.** Since they use rule-based thresholds (e.g., $x > 50$), the scale/magnitude of the data does not affect the split logic.\n",
    "\n",
    "**Q1: How does a Decision Tree decide where to split?**\n",
    "\n",
    "It performs a greedy search over all features and possible thresholds. For each, it calculates the purity gain (Information Gain or Gini Gain for classification, variance reduction for regression). It selects the single feature and threshold that provides the maximum gain at that specific node.\n",
    "\n",
    "**Q2: How to prevent overfitting in a Decision Tree?**\n",
    "\n",
    "Pre-pruning (Early Stopping): Restrict tree growth using max_depth, min_samples_split, min_samples_leaf.\n",
    "\n",
    "Post-pruning: Grow the full tree, then prune back branches that provide little predictive power using Cost Complexity Pruning (ccp_alpha).\n",
    "\n",
    "Ensemble it: Use the tree as a base learner in Bagging (Random Forest) or Boosting methods, which are far more robust.\n",
    "\n",
    "**Q3: What's the difference between Gini Impurity and Entropy?**\n",
    "\n",
    "Both measure node impurity. Gini calculates the probability of misclassification. Entropy measures the informational disorder. In practice, they yield very similar results, but Gini is slightly faster to compute as it doesn't require logarithms, which is why it's often the default. Entropy might produce slightly more balanced trees.\n",
    "\n",
    "**Q4: Are Decision Trees sensitive to feature scaling?**\n",
    "\n",
    "No. The splitting rule is based on feature thresholds and ordering, not on magnitude or distance. Scaling does not change the tree's structure.\n",
    "\n",
    "**Q5: Can they handle missing values?**\n",
    "\n",
    "Sklearn's implementation does NOT natively handle missing values. You must impute them before training.\n",
    "However, the classic algorithm (CART) can handle them via surrogate splits — finding splits in other features that mimic the primary split, so data with missing values can be routed down the tree.\n",
    "\n",
    "**Q6: What are the pros and cons compared to Linear Models?**\n",
    "\n",
    "Pros: No need for scaling, handles non-linearity and interactions automatically, more interpretable visualizations.\n",
    "Cons: Far more prone to overfitting (high variance), less stable, worse at extrapolation (vs. linear regression).\n",
    "\n",
    "**Q7: How would you handle a categorical variable with many levels (high cardinality)?**\n",
    "\n",
    "This is a weakness. A tree might overfit by giving it high importance. Solutions:\n",
    "\n",
    "Group rare levels into an \"Other\" category.\n",
    "\n",
    "Use target encoding (mean of target per category), but be cautious of leakage.\n",
    "\n",
    "Use a model better suited for high-cardinality features (like CatBoost).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = [[0,0], [1,1], [1,0], [0,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, criterion='gini')\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[1,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.1])\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.predict([[6]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62606f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "**Random Forest** is an **Ensemble Learning** method that operates by constructing a multitude of decision trees at training time.\n",
    "\n",
    "**Core Philosophy:** *\"The Wisdom of the Crowds.\"*\n",
    "A single decision tree is prone to errors (noise/overfitting). A thousand trees, voting together, will cancel out those errors and converge on the correct answer.\n",
    "\n",
    "\n",
    "\n",
    "### 1. How It Works (Bagging + Feature Randomness)\n",
    "\n",
    "Random Forest improves on Bagging (Bootstrap Aggregation) by adding an extra layer of randomness.\n",
    "\n",
    "**Step 1: Bootstrap Sampling (Bagging)**\n",
    "* Create $N$ different subsets of the training data by sampling **with replacement**.\n",
    "* *Result:* Each tree sees a slightly different version of the dataset.\n",
    "\n",
    "**Step 2: Random Feature Selection**\n",
    "* When building each tree, at **every split**, the model considers only a **random subset of features** (controlled by `max_features`).\n",
    "* *Why?* This prevents one strong feature from dominating every tree. It forces trees to be **diverse** (decorrelated).\n",
    "\n",
    "**Step 3: Aggregation**\n",
    "* **Classifier:** Majority Vote (Mode).\n",
    "* **Regressor:** Average Prediction (Mean).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Importance (The \"Why\")\n",
    "\n",
    "Random Forest is excellent for feature selection. It calculates importance in two main ways:\n",
    "\n",
    "#### A. Gini Importance (Mean Decrease Impurity)\n",
    "The default method. It measures the total reduction in impurity (Gini or Entropy) brought by that feature.\n",
    "$$Importance(f) = \\sum_{t \\in Trees} \\sum_{n \\in Nodes_f} (Impurity_{parent} - WeightedImpurity_{children})$$\n",
    "* *Cons:* Can be biased towards high-cardinality features (numerical values with many unique states).\n",
    "\n",
    "#### B. Permutation Importance\n",
    "More reliable. It measures the drop in model accuracy when a single feature is randomly shuffled (noise).\n",
    "$$Importance(f) = Score_{baseline} - Score_{shuffled\\_f}$$\n",
    "* If shuffling a feature destroys the accuracy, that feature is important.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Hyperparameters\n",
    "\n",
    "| Parameter | Description | Impact |\n",
    "| :--- | :--- | :--- |\n",
    "| **`n_estimators`** | Number of trees in the forest. | More is better (more stable), but slower. |\n",
    "| **`max_depth`** | Max depth of each tree. | Controls complexity. Lower = Less Overfitting. |\n",
    "| **`max_features`** | Number of features to consider at each split. | Crucial for decorrelating trees. (Default: $\\sqrt{n\\_features}$ for classification). |\n",
    "| **`min_samples_split`** | Min samples required to split a node. | Higher = Reduces Overfitting. |\n",
    "| **`bootstrap`** | Whether to use bootstrap samples. | True (Default). |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| **Robust:** Reduces overfitting compared to single trees. | **Slow:** Training and prediction are slower than a single tree. |\n",
    "| **Versatile:** Handles numerical, categorical, and missing data. | **Black Box:** Harder to interpret exact rules compared to a single Decision Tree. |\n",
    "| **No Scaling:** Like Decision Trees, it requires no feature scaling. | **Memory:** Stores the entire forest in memory. |\n",
    "| **Importance:** Provides clear feature importance scores. | |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. FAQ\n",
    "\n",
    "**Q: Why Random Forest over Decision Tree?**\n",
    "**A:** A single tree has high variance (overfits). Random Forest reduces variance by averaging many uncorrelated trees, leading to better generalization.\n",
    "\n",
    "**Q: How does it combine predictions?**\n",
    "**A:**\n",
    "* **Classification:** Majority Vote (e.g., 80 trees say \"Yes\", 20 say \"No\" $\\rightarrow$ \"Yes\").\n",
    "* **Regression:** Mean (Average of all 100 trees).\n",
    "\n",
    "**Q: What is the effect of `max_features`?**\n",
    "**A:**\n",
    "* If `max_features` = Total Features, it behaves like standard Bagging (trees are more correlated).\n",
    "* If `max_features` is small, trees are very diverse (less correlated), which usually improves performance.\n",
    "\n",
    "**Q: Can Random Forest handle missing values?**\n",
    "**A:** Yes, modern implementations (and sklearn via Imputer pipelines) handle this well. It is robust to outliers.\n",
    "\n",
    "**Q: How do you control overfitting?**\n",
    "**A:**\n",
    "1.  Limit `max_depth`.\n",
    "2.  Increase `min_samples_leaf`.\n",
    "3.  Use `max_features < total_features`.\n",
    "\n",
    "**Q1: Why does Random Forest work better than a single Decision Tree?**\n",
    "\n",
    "Three key mechanisms:\n",
    "\n",
    "Bagging (Bootstrap Aggregation): Reduces variance by averaging multiple models trained on different data samples\n",
    "\n",
    "Feature Randomness: Each split considers random subset of features → trees become decorrelated → ensemble diversity increases\n",
    "\n",
    "Ensemble Effect: Errors from individual trees cancel out; correct predictions reinforced\n",
    "\n",
    "**Q2: What's the difference between Bagging and Random Forest?**\n",
    "\n",
    "Bagging: Builds multiple models on bootstrap samples (could be any model)\n",
    "Random Forest = Bagging + Random Feature Selection\n",
    "\n",
    "Standard bagging uses all features at each split\n",
    "\n",
    "RF adds extra randomness by limiting features per split → further reduces correlation between trees\n",
    "\n",
    "**Q3: How do you prevent overfitting in Random Forest?**\n",
    "\n",
    "Control Tree Complexity: max_depth, min_samples_split, min_samples_leaf\n",
    "\n",
    "Increase Number of Trees: More trees stabilize predictions (but diminishing returns)\n",
    "\n",
    "Limit Features per Split: max_features = sqrt(n_features) or smaller\n",
    "\n",
    "Use OOB Score: Monitor out-of-bag error during training\n",
    "\n",
    "Early Stopping: Stop when OOB error plateaus\n",
    "\n",
    "**Q4: What is Out-of-Bag (OOB) error and why is it useful?**\n",
    "\n",
    "OOB Error: Prediction error on samples not included in a tree's bootstrap sample\n",
    "\n",
    "Each sample is OOB for ~36.8% of trees\n",
    "\n",
    "Provides free validation without needing separate test set\n",
    "\n",
    "In sklearn: oob_score=True enables this\n",
    "\n",
    "**Q5: How does Random Forest handle missing values?**\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "During Training: Uses surrogate splits (find similar splits using other features)\n",
    "\n",
    "In sklearn: Requires imputation first (median/mode)\n",
    "\n",
    "Smart Imputation: Can use proximity matrix from RF to impute missing values iteratively\n",
    "\n",
    "**Q6: When would you NOT use Random Forest?**\n",
    "\n",
    "Interpretability Required: Need clear decision rules\n",
    "\n",
    "Extrapolation Needed: Predicting outside training range (regression)\n",
    "\n",
    "Extremely High-dimensional Sparse Data: Like text data (use linear models)\n",
    "\n",
    "Streaming/Online Learning: RF needs batch training\n",
    "\n",
    "Memory/Time Constrained: Large forests are resource-intensive\n",
    "\n",
    "**Q7: Can Random Forest feature importance be misleading?**\n",
    "\n",
    "Yes Important caveats:\n",
    "\n",
    "Biased toward high-cardinality features: Continuous or many-category features get inflated importance\n",
    "\n",
    "Correlated features: Importance splits between correlated features\n",
    "\n",
    "Use permutation importance for more reliable measure\n",
    "\n",
    "Always validate with domain knowledge or ablation studies\n",
    "\n",
    "When Tuning:\n",
    "Start with n_estimators=100, increase until OOB error stabilizes\n",
    "\n",
    "Tune max_features first (most impactful parameter)\n",
    "\n",
    "Use n_jobs=-1 for parallel training\n",
    "\n",
    "Monitor OOB score for early stopping\n",
    "\n",
    "Common Pitfalls:\n",
    "Too many trees without benefit (waste resources)\n",
    "\n",
    "Forgetting to set random seed (non-reproducible results)\n",
    "\n",
    "Using default max_features='auto' (might not be optimal)\n",
    "\n",
    "Ignoring OOB score as free validation\n",
    "\n",
    "```bash\n",
    "\n",
    "Aspect\t            Decision Tree\t                Random Forest\n",
    "Overfitting     \tHigh risk                   \tMuch lower risk\n",
    "Interpretability\tHigh (white box)\t            Low (black box)\n",
    "Prediction Speed\tVery fast\t                    Slower (needs all trees)\n",
    "Feature Importance\tYes, but unreliable\t            More robust\n",
    "Handling Noise\t        Poor\t                        Good\n",
    "```\n",
    "\n",
    "Extremely Randomized Trees (ExtraTrees)\n",
    "Even more randomness: random thresholds for splits (not best threshold)\n",
    "\n",
    "Faster training, sometimes better performance\n",
    "\n",
    "Introduces more bias but reduces variance further\n",
    "\n",
    "Balanced Random Forest\n",
    "For imbalanced data: bootstrap samples maintain class ratio\n",
    "\n",
    "Or use class_weight='balanced' parameter\n",
    "\n",
    "Quantile Regression Forest\n",
    "Predicts full distribution, not just mean\n",
    "\n",
    "Useful for prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331ce33",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "**Key Idea:** Train multiple models in PARALLEL on different data subsets\n",
    "\n",
    "**Goal:** Reduce VARIANCE without increasing bias\n",
    "\n",
    "**Examples**: Random Forest, Bagged Trees\n",
    "\n",
    "## Boosting \n",
    "\n",
    "**Key Idea:** Train models SEQUENTIALLY, each correcting previous errors\n",
    "\n",
    "**Goal:** Reduce BIAS (and eventually variance)\n",
    "\n",
    "**Examples:** AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = [[0,0], [1,1], [1,0], [0,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[1,0]]))\n",
    "print(clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b079e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.1])\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.predict([[6]]))\n",
    "print(reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfeb57e",
   "metadata": {},
   "source": [
    "# Extra Trees\n",
    "# Extra Trees (Extremely Randomized Trees)\n",
    "\n",
    "**Extra Trees** is an ensemble learning method very similar to Random Forest. It builds many decision trees and combines their results via **Majority Vote** (Classification) or **Averaging** (Regression).\n",
    "\n",
    "**The Key Idea:**\n",
    "While Random Forest injects randomness by subsampling data (bagging) and features, **Extra Trees** takes it a step further by **randomizing the cut thresholds** for splits.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. How It Works\n",
    "\n",
    "The algorithm follows these steps to build the ensemble:\n",
    "\n",
    "1.  **Data Sampling:** It typically uses the **entire dataset** (unlike Random Forest which uses Bootstrap samples), though bootstrapping can be enabled.\n",
    "2.  **Random Splits (The Core Difference):**\n",
    "    * **Random Forest:** For a selected feature, it calculates the *optimal* split point (e.g., searches for the exact age that maximizes Information Gain).\n",
    "    * **Extra Trees:** For a selected feature, it picks a **random cut point** within the feature's range. It doesn't search for the best one.\n",
    "3.  **Aggregation:**\n",
    "    * **Classifier:** Majority Vote.\n",
    "    * **Regressor:** Average prediction.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Random Forest vs. Extra Trees (The Showdown)\n",
    "\n",
    "The most common interview question regarding Extra Trees is how it compares to Random Forest.\n",
    "\n",
    "| Feature | **Random Forest** (RF) | **Extra Trees** (ET) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Split Selection** | Searches for the **Best** split (Greedy). | Selects a **Random** split value. |\n",
    "| **Training Speed** | Slower (calculating optimal splits is heavy). | **Faster** (skips optimal split calculation). |\n",
    "| **Variance** | Medium (Trees are somewhat correlated). | **Low** (Trees are highly uncorrelated/diverse). |\n",
    "| **Bias** | Low (Tries to fit data perfectly). | **Medium** (Random splits might miss optimal patterns). |\n",
    "| **Overfitting** | Moderate risk. | **Lower risk** (Harder to memorize noise). |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Math: Bias-Variance Decomposition\n",
    "\n",
    "The prediction error of any model can be decomposed as:\n",
    "$$Error = Bias^2 + Variance + Noise$$\n",
    "\n",
    "**Why does Extra Trees work?**\n",
    "* **Random Forest:** Low Bias, Medium Variance.\n",
    "    * Since all trees try to find the \"best\" split, they often end up looking similar (correlated).\n",
    "* **Extra Trees:** Medium Bias, Low Variance.\n",
    "    * The random thresholds make the trees much more diverse (less correlated).\n",
    "\n",
    "**Variance Reduction Formula:**\n",
    "For an ensemble of $M$ trees, the variance is roughly:\n",
    "$$Variance_{ensemble} \\approx \\rho \\sigma^2 + \\frac{1-\\rho}{M}\\sigma^2$$\n",
    "* $\\rho$ (rho): Correlation between trees.\n",
    "* Because Extra Trees chooses random splits, the correlation $\\rho$ is much lower than in Random Forest.\n",
    "* **Result:** $Variance_{ET} < Variance_{RF}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Hyperparameters\n",
    "\n",
    "The hyperparameters are almost identical to Random Forest.\n",
    "\n",
    "* **`n_estimators`**: Number of trees.\n",
    "* **`max_depth`**: Controls complexity/overfitting.\n",
    "* **`min_samples_split`**: Minimum samples required to split.\n",
    "* **`max_features`**: Number of features to consider (crucial for randomization).\n",
    "* **`bootstrap`**:\n",
    "    * **Random Forest:** Default = `True`.\n",
    "    * **Extra Trees:** Default = `False` (uses whole dataset), but can be set to `True`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| **Speed:** Much faster to train on large datasets. | **Accuracy:** Can be slightly less accurate than RF on small datasets (due to higher bias). |\n",
    "| **Variance:** superior reduction of variance (smoother boundaries). | **Interpretability:** Even harder to interpret than RF due to randomness. |\n",
    "| **Noise:** Less likely to overfit noisy data. | **File Size:** Trees can grow larger/deeper if not constrained. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. FAQ (Interview Questions)\n",
    "\n",
    "**Q: Why use Extra Trees over Random Forest?**\n",
    "**A:** When you need **faster training** or when Random Forest is **overfitting** significantly. The extra randomness helps generalize better on high-dimensional data.\n",
    "\n",
    "**Q: Does Extra Trees require feature scaling?**\n",
    "**A:** **No.** Like all tree-based models, it relies on threshold rules, not distance calculations.\n",
    "\n",
    "**Q: What is the impact on Bias?**\n",
    "**A:** Extra Trees typically has slightly **higher bias** because the splits are not optimal. However, the drastic reduction in **variance** often results in a lower overall error.\n",
    "\n",
    "**Q: Feature Importance?**\n",
    "**A:** Yes, it provides feature importance (`clf.feature_importances_`) just like Random Forest, measuring how much each feature contributed to reducing impurity.\n",
    "\n",
    "**Q1: Why is Extra Trees faster than Random Forest?**\n",
    "Three reasons:\n",
    "\n",
    "No split optimization: RF evaluates multiple thresholds per feature, ET picks random threshold\n",
    "\n",
    "Simpler computation: ET doesn't sort feature values or compute impurity for many splits\n",
    "\n",
    "Parallel efficiency: While both are parallel, ET has less overhead per split\n",
    "\n",
    "Complexity: RF: O(k⋅d⋅n⋅log n) vs ET: O(d⋅n⋅log n) where k is # of split evaluations\n",
    "\n",
    "**Q2: When would Extra Trees perform worse than Random Forest?**\n",
    "Four scenarios:\n",
    "\n",
    "Very small datasets (< 1000 samples) - RF's optimal splits matter more\n",
    "\n",
    "Clean, deterministic data - RF can find perfect splits\n",
    "\n",
    "Features with critical thresholds - ET might miss important cutpoints\n",
    "\n",
    "Competition settings - RF usually achieves slightly higher accuracy with tuning\n",
    "\n",
    "\n",
    "**Q3: How does the bootstrap parameter affect Extra Trees?**\n",
    "\n",
    "bootstrap=True (default):\n",
    "  - Creates diversity through data sampling\n",
    "  - Enables OOB error estimates\n",
    "  - Better for variance reduction\n",
    "\n",
    "bootstrap=False:\n",
    "  - Uses entire dataset for each tree\n",
    "  - Lower bias, especially with small datasets\n",
    "  - No OOB estimates available\n",
    "  - Faster training (no sampling overhead)\n",
    "\n",
    "**Q4: Can Extra Trees handle categorical features better than RF?**\n",
    "Yes, in some cases:\n",
    "\n",
    "For high-cardinality categorical features, ET's random splits can be beneficial\n",
    "\n",
    "RF might overfit to specific category thresholds\n",
    "\n",
    "ET treats all splits equally randomly\n",
    "\n",
    "Best practice: Use proper encoding (target encoding for high-cardinality)\n",
    "\n",
    "**Q5: How to choose between sqrt(n_features) and all features for max_features?**\n",
    "\n",
    "Use sqrt(n_features) when:\n",
    "  - Many irrelevant features\n",
    "  - Want stronger regularization\n",
    "  - Training time is concern\n",
    "\n",
    "Use all features (max_features=None) when:\n",
    "  - Few features (< 20)\n",
    "  - Most features are informative\n",
    "  - Want lower bias\n",
    "  - Dataset is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55962b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = [[0,0], [1,1], [1,0], [0,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=100, max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[1,0]]))\n",
    "print(clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.1])\n",
    "\n",
    "reg = ExtraTreesRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.predict([[6]]))\n",
    "print(reg.feature_importances_)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
