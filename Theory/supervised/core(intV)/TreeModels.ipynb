{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68448f98",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "A Decision Tree (DT) is a tree-structured model used for classification or regression.\n",
    "\n",
    "Each internal node = feature test\n",
    "\n",
    "Each branch = outcome of the test\n",
    "\n",
    "Each leaf node = prediction\n",
    "\n",
    "\n",
    "Decision Tree  Classifier\tClassification\tClass label (0,1,2…)\n",
    "\n",
    "Decision Tree Regressor   Regression   Continuous value\n",
    "\n",
    "How It Works\n",
    "\n",
    "Select the best feature to split (based on criterion)\n",
    "\n",
    "Split data into subsets\n",
    "\n",
    "Repeat recursively (recursive partitioning)\n",
    "\n",
    "Stop when:\n",
    "\n",
    "Maximum depth reached\n",
    "\n",
    "Minimum samples per leaf reached\n",
    "\n",
    "Node is pure\n",
    "\n",
    "Splitting Criteria\n",
    "For Classification\n",
    "\n",
    "Gini Impurity \n",
    "\n",
    "Gini = 1−∑​(pi)^2\n",
    "\n",
    "Entropy / Information Gain\n",
    "⁡\n",
    "Entropy = − ∑ p_i log2(p_i)\n",
    "\n",
    "For Regression\n",
    "\n",
    "Variance Reduction / MSE \n",
    "\n",
    "V =  1/N  ∑(y_i - y')^2\n",
    "\n",
    "\n",
    "\n",
    "Node split → reduces impurity (classification) or variance (regression)\n",
    "\n",
    "Advantages\n",
    "\n",
    "Easy to interpret / visualize\n",
    "\n",
    "Handles numerical & categorical features\n",
    "\n",
    "Non-linear relationships allowed\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Prone to overfitting\n",
    "\n",
    "Sensitive to noise\n",
    "\n",
    "Small changes → large changes in tree structure\n",
    "\n",
    "Solve overfitting with max_depth, min_samples_split, pruning, ensemble methods\n",
    "\n",
    "Hyperparameters\n",
    "\n",
    "max_depth → max levels of tree\n",
    "\n",
    "min_samples_split → min samples to split a node\n",
    "\n",
    "min_samples_leaf → min samples at leaf node\n",
    "\n",
    "max_features → number of features to consider for split\n",
    "\n",
    "criterion → “gini”, “entropy” (classification) / “mse”, “mae” (regression)\n",
    "\n",
    "\n",
    "\n",
    "Overfitting & Pruning\n",
    "\n",
    "Trees tend to fit training data perfectly\n",
    "\n",
    "Solve using:\n",
    "\n",
    "max_depth\n",
    "\n",
    "min_samples_leaf\n",
    "\n",
    "cost complexity pruning (ccp_alpha in sklearn)\n",
    "\n",
    "Ensemble methods (Random Forest, Gradient Boosting)\n",
    "\n",
    "Feature Importance\n",
    "\n",
    "DT can provide importance of each feature:\n",
    "\n",
    "``` print(clf.feature_importances_)```\n",
    "\n",
    "Useful for feature selection\n",
    "\n",
    "How does a decision tree decide splits?\n",
    "\n",
    "By maximizing information gain (classification) or reducing variance (regression).\n",
    "\n",
    "Q: How to prevent overfitting?\n",
    "\n",
    "Limit depth, min_samples, or prune the tree.\n",
    "\n",
    "Q: Difference between classifier and regressor?\n",
    "\n",
    "Classifier → discrete labels; Regressor → continuous values.\n",
    "\n",
    "Q: Can Decision Trees handle missing values?\n",
    "\n",
    "Yes, sklearn can handle via surrogate splits.\n",
    "\n",
    "Q: Are Decision Trees sensitive to feature scaling?\n",
    "\n",
    "No scaling required.\n",
    "\n",
    "\n",
    "Q1: How does a Decision Tree decide where to split?\n",
    "\n",
    "It performs a greedy search over all features and possible thresholds. For each, it calculates the purity gain (Information Gain or Gini Gain for classification, variance reduction for regression). It selects the single feature and threshold that provides the maximum gain at that specific node.\n",
    "\n",
    "Q2: How to prevent overfitting in a Decision Tree?\n",
    "\n",
    "Pre-pruning (Early Stopping): Restrict tree growth using max_depth, min_samples_split, min_samples_leaf.\n",
    "\n",
    "Post-pruning: Grow the full tree, then prune back branches that provide little predictive power using Cost Complexity Pruning (ccp_alpha).\n",
    "\n",
    "Ensemble it: Use the tree as a base learner in Bagging (Random Forest) or Boosting methods, which are far more robust.\n",
    "\n",
    "Q3: What's the difference between Gini Impurity and Entropy?\n",
    "\n",
    "Both measure node impurity. Gini calculates the probability of misclassification. Entropy measures the informational disorder. In practice, they yield very similar results, but Gini is slightly faster to compute as it doesn't require logarithms, which is why it's often the default. Entropy might produce slightly more balanced trees.\n",
    "\n",
    "Q4: Are Decision Trees sensitive to feature scaling?\n",
    "\n",
    "No. The splitting rule is based on feature thresholds and ordering, not on magnitude or distance. Scaling does not change the tree's structure.\n",
    "\n",
    "Q5: Can they handle missing values?\n",
    "\n",
    "Sklearn's implementation does NOT natively handle missing values. You must impute them before training.\n",
    "However, the classic algorithm (CART) can handle them via surrogate splits — finding splits in other features that mimic the primary split, so data with missing values can be routed down the tree.\n",
    "\n",
    "Q6: What are the pros and cons compared to Linear Models?\n",
    "\n",
    "Pros: No need for scaling, handles non-linearity and interactions automatically, more interpretable visualizations.\n",
    "Cons: Far more prone to overfitting (high variance), less stable, worse at extrapolation (vs. linear regression).\n",
    "\n",
    "Q7: How would you handle a categorical variable with many levels (high cardinality)?\n",
    "\n",
    "This is a weakness. A tree might overfit by giving it high importance. Solutions:\n",
    "\n",
    "Group rare levels into an \"Other\" category.\n",
    "\n",
    "Use target encoding (mean of target per category), but be cautious of leakage.\n",
    "\n",
    "Use a model better suited for high-cardinality features (like CatBoost).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = [[0,0], [1,1], [1,0], [0,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, criterion='gini')\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[1,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.1])\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.predict([[6]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62606f",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random Forest  is an ensemble learning method that builds many decision trees and combines their results.\n",
    "\n",
    "Classifier → majority vote (mode)\n",
    "\n",
    "Regressor → average prediction\n",
    "\n",
    "Many trees reduce overfitting and improve accuracy\n",
    "\n",
    "How It Works\n",
    "\n",
    "Create bootstrap samples from training data (sampling with replacement)\n",
    "\n",
    "Train a decision tree on each sample\n",
    "\n",
    "For each split, select a random subset of features\n",
    "\n",
    "Combine predictions:\n",
    "\n",
    "Classification → majority vote\n",
    "\n",
    "Regression → mean value\n",
    "\n",
    "This is called bagging (Bootstrap Aggregation)\n",
    "\n",
    "Why Random Forest?\n",
    "\n",
    "Reduces overfitting of individual trees\n",
    "\n",
    "Handles high-dimensional data\n",
    "\n",
    "Works with numerical & categorical features\n",
    "\n",
    "Provides feature importance\n",
    "\n",
    "Hyperparameters \n",
    "\n",
    "n_estimators → number of trees\n",
    "\n",
    "max_depth → max depth of each tree\n",
    "\n",
    "min_samples_split → min samples to split\n",
    "\n",
    "min_samples_leaf → min samples at leaf\n",
    "\n",
    "max_features → number of features considered for split\n",
    "\n",
    "bootstrap → True (default) for bagging\n",
    "\n",
    "Feature Importance\n",
    "\n",
    "- Gini Importance (Mean Decrease Impurity)\n",
    "\n",
    "Importance(f) = Σ (Weighted impurity decrease across all nodes using f)\n",
    "\n",
    "Sum of impurity reduction when feature is used for splitting\n",
    "\n",
    "Weighted by number of samples reaching each node\n",
    "\n",
    "- Permutation Importance\n",
    "\n",
    "Importance(f) = Baseline Score - Score with feature f shuffled\n",
    "\n",
    "More reliable for correlated features\n",
    "\n",
    "Measures actual predictive power\n",
    "\n",
    "\n",
    "\n",
    "Advantages\n",
    "\n",
    "Reduces overfitting vs single tree\n",
    "\n",
    "Handles high-dimensional & large datasets\n",
    "\n",
    "Robust to outliers & noise\n",
    "\n",
    "Can estimate feature importance\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Less interpretable than a single tree\n",
    "\n",
    "Can be slow for large datasets\n",
    "\n",
    "Large memory usage\n",
    "\n",
    "\n",
    "\n",
    "Q: Why Random Forest over Decision Tree?\n",
    "Reduces overfitting, improves accuracy using bagging.\n",
    "\n",
    "Q: How does it combine predictions?\n",
    "Classifier → majority vote, Regressor → average.\n",
    "\n",
    "Q: How is feature importance calculated?\n",
    "Mean decrease in impurity or permutation importance.\n",
    "\n",
    "Q: Can Random Forest handle missing values?\n",
    "Yes (sklearn handles natively).\n",
    "\n",
    "Overfitting Control\n",
    "\n",
    "Limit max_depth\n",
    "\n",
    "Increase n_estimators\n",
    "\n",
    "Use max_features < total features\n",
    "\n",
    "Q1: Why does Random Forest work better than a single Decision Tree?\n",
    "Three key mechanisms:\n",
    "\n",
    "Bagging (Bootstrap Aggregation): Reduces variance by averaging multiple models trained on different data samples\n",
    "\n",
    "Feature Randomness: Each split considers random subset of features → trees become decorrelated → ensemble diversity increases\n",
    "\n",
    "Ensemble Effect: Errors from individual trees cancel out; correct predictions reinforced\n",
    "\n",
    "Q2: What's the difference between Bagging and Random Forest?\n",
    "Bagging: Builds multiple models on bootstrap samples (could be any model)\n",
    "Random Forest = Bagging + Random Feature Selection\n",
    "\n",
    "Standard bagging uses all features at each split\n",
    "\n",
    "RF adds extra randomness by limiting features per split → further reduces correlation between trees\n",
    "\n",
    "Q3: How do you prevent overfitting in Random Forest?\n",
    "Control Tree Complexity: max_depth, min_samples_split, min_samples_leaf\n",
    "\n",
    "Increase Number of Trees: More trees stabilize predictions (but diminishing returns)\n",
    "\n",
    "Limit Features per Split: max_features = sqrt(n_features) or smaller\n",
    "\n",
    "Use OOB Score: Monitor out-of-bag error during training\n",
    "\n",
    "Early Stopping: Stop when OOB error plateaus\n",
    "\n",
    "Q4: What is Out-of-Bag (OOB) error and why is it useful?\n",
    "OOB Error: Prediction error on samples not included in a tree's bootstrap sample\n",
    "\n",
    "Each sample is OOB for ~36.8% of trees\n",
    "\n",
    "Provides free validation without needing separate test set\n",
    "\n",
    "In sklearn: oob_score=True enables this\n",
    "\n",
    "Q5: How does Random Forest handle missing values?\n",
    "Two approaches:\n",
    "\n",
    "During Training: Uses surrogate splits (find similar splits using other features)\n",
    "\n",
    "In sklearn: Requires imputation first (median/mode)\n",
    "\n",
    "Smart Imputation: Can use proximity matrix from RF to impute missing values iteratively\n",
    "\n",
    "Q6: When would you NOT use Random Forest?\n",
    "Interpretability Required: Need clear decision rules\n",
    "\n",
    "Extrapolation Needed: Predicting outside training range (regression)\n",
    "\n",
    "Extremely High-dimensional Sparse Data: Like text data (use linear models)\n",
    "\n",
    "Streaming/Online Learning: RF needs batch training\n",
    "\n",
    "Memory/Time Constrained: Large forests are resource-intensive\n",
    "\n",
    "Q7: Can Random Forest feature importance be misleading?\n",
    "Yes! Important caveats:\n",
    "\n",
    "Biased toward high-cardinality features: Continuous or many-category features get inflated importance\n",
    "\n",
    "Correlated features: Importance splits between correlated features\n",
    "\n",
    "Use permutation importance for more reliable measure\n",
    "\n",
    "Always validate with domain knowledge or ablation studies\n",
    "\n",
    "When Tuning:\n",
    "Start with n_estimators=100, increase until OOB error stabilizes\n",
    "\n",
    "Tune max_features first (most impactful parameter)\n",
    "\n",
    "Use n_jobs=-1 for parallel training\n",
    "\n",
    "Monitor OOB score for early stopping\n",
    "\n",
    "Common Pitfalls:\n",
    "Too many trees without benefit (waste resources)\n",
    "\n",
    "Forgetting to set random seed (non-reproducible results)\n",
    "\n",
    "Using default max_features='auto' (might not be optimal)\n",
    "\n",
    "Ignoring OOB score as free validation\n",
    "```bash\n",
    "\n",
    "Aspect\t            Decision Tree\t                Random Forest\n",
    "Overfitting     \tHigh risk                   \tMuch lower risk\n",
    "Interpretability\tHigh (white box)\t            Low (black box)\n",
    "Prediction Speed\tVery fast\t                    Slower (needs all trees)\n",
    "Feature Importance\tYes, but unreliable\t            More robust\n",
    "Handling Noise\t        Poor\t                        Good\n",
    "```\n",
    "\n",
    "Extremely Randomized Trees (ExtraTrees)\n",
    "Even more randomness: random thresholds for splits (not best threshold)\n",
    "\n",
    "Faster training, sometimes better performance\n",
    "\n",
    "Introduces more bias but reduces variance further\n",
    "\n",
    "Balanced Random Forest\n",
    "For imbalanced data: bootstrap samples maintain class ratio\n",
    "\n",
    "Or use class_weight='balanced' parameter\n",
    "\n",
    "Quantile Regression Forest\n",
    "Predicts full distribution, not just mean\n",
    "\n",
    "Useful for prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331ce33",
   "metadata": {},
   "source": [
    "Bagging\n",
    "\n",
    "Key Idea: Train multiple models in PARALLEL on different data subsets\n",
    "\n",
    "Goal: Reduce VARIANCE without increasing bias\n",
    "\n",
    "Examples: Random Forest, Bagged Trees\n",
    "\n",
    "Boosting \n",
    "\n",
    "Key Idea: Train models SEQUENTIALLY, each correcting previous errors\n",
    "\n",
    "Goal: Reduce BIAS (and eventually variance)\n",
    "\n",
    "Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = [[0,0], [1,1], [1,0], [0,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[1,0]]))\n",
    "print(clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b079e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.1])\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.predict([[6]]))\n",
    "print(reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfeb57e",
   "metadata": {},
   "source": [
    "# Extra Trees\n",
    "\n",
    "Extra Trees (Extremely Randomized Trees) is an ensemble of decision trees, similar to Random Forest, but with more randomness.\n",
    "\n",
    "Classifier → majority vote\n",
    "\n",
    "Regressor → average prediction\n",
    "\n",
    "Key idea:\n",
    "\n",
    "Randomize both data samples and splits → reduces variance and overfitting faster than Random Forest.\n",
    "\n",
    "How Extra Trees Work\n",
    "\n",
    "Use entire dataset or bootstrap samples (optional)\n",
    "\n",
    "For each node, randomly select a subset of features\n",
    "\n",
    "Choose random split values (not optimal like RF)\n",
    "\n",
    "Aggregate predictions:\n",
    "\n",
    "Classifier → majority vote\n",
    "\n",
    "Regressor → average\n",
    "\n",
    "Difference from RF:\n",
    "RF chooses best split; Extra Trees chooses random split → faster, more diverse trees\n",
    "\n",
    "Advantages\n",
    "\n",
    "Faster to train than Random Forest\n",
    "\n",
    "Reduces overfitting due to extreme randomness\n",
    "\n",
    "Handles high-dimensional data\n",
    "\n",
    "Works with numerical & categorical features\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "Slightly less accurate than Random Forest if data is small\n",
    "\n",
    "Less interpretable than a single tree\n",
    "\n",
    "Hyperparameters \n",
    "\n",
    "n_estimators → number of trees\n",
    "\n",
    "max_depth → max depth of each tree\n",
    "\n",
    "min_samples_split → min samples to split\n",
    "\n",
    "min_samples_leaf → min samples at leaf\n",
    "\n",
    "max_features → number of features considered per split\n",
    "\n",
    "bootstrap → True/False (sampling method)\n",
    "\n",
    "\n",
    "Feature Importance\n",
    "\n",
    "Built-in, similar to Random Forest:\n",
    "\n",
    "```print(clf.feature_importances_)```\n",
    "\n",
    "\n",
    "Use Cases\n",
    "\n",
    "High-dimensional data (genomics, text)\n",
    "\n",
    "Fraud detection\n",
    "\n",
    "Regression / classification tasks requiring fast training\n",
    "\n",
    "Situations with large datasets\n",
    "\n",
    "Q: How is Extra Trees different from Random Forest?\n",
    "\n",
    "Extra randomness → splits chosen randomly → faster & less overfitting.\n",
    "\n",
    "Q: Classifier vs Regressor?\n",
    "\n",
    "Classifier → majority vote; Regressor → average prediction.\n",
    "\n",
    "Q: When to use Extra Trees over RF?\n",
    "\n",
    "Faster training, reduce variance on large datasets.\n",
    "\n",
    "Q: Does it require feature scaling?\n",
    "\n",
    "No scaling needed.\n",
    "\n",
    "difference between Random Forest and Extra Trees\n",
    "\n",
    "Split selection: Random Forest chooses the best split at each node, while Extra Trees selects a random split.\n",
    "\n",
    "Training speed: Random Forest is slower because it searches for the best split, whereas Extra Trees is faster due to random splits.\n",
    "\n",
    "Variance: Random Forest has medium variance, while Extra Trees generally has lower variance because of the extra randomness.\n",
    "\n",
    "Bias: Random Forest has low bias, whereas Extra Trees has slightly higher bias due to random splits.\n",
    "\n",
    "Overfitting: Random Forest can overfit moderately, while Extra Trees is less prone to overfitting.\n",
    "\n",
    "Accuracy: Random Forest usually achieves higher accuracy, but Extra Trees can be slightly less accurate on small datasets.\n",
    "\n",
    "Bias-Variance Decomposition:\n",
    "\n",
    "Error = Bias² + Variance + Noise\n",
    "\n",
    "Random Forest: Low Bias, Medium Variance\n",
    "\n",
    "Extra Trees: Medium Bias, Low Variance\n",
    "\n",
    "Why Less Variance?\n",
    "\n",
    "Random Forest trees are correlated because they all search for \"best\" splits\n",
    "\n",
    "Extra Trees trees are more diverse due to random thresholds\n",
    "\n",
    "More diversity → better error cancellation in ensemble\n",
    "\n",
    "Expected Error Reduction:\n",
    "\n",
    "For M trees:\n",
    "\n",
    "Variance_reduction ≈ 1/M * (Average pair-wise correlation between trees)\n",
    "\n",
    "Since Extra Trees has lower correlation:\n",
    "\n",
    "Variance_ExtraTrees < Variance_RandomForest\n",
    "\n",
    "\n",
    "Q1: Why is Extra Trees faster than Random Forest?\n",
    "Three reasons:\n",
    "\n",
    "No split optimization: RF evaluates multiple thresholds per feature, ET picks random threshold\n",
    "\n",
    "Simpler computation: ET doesn't sort feature values or compute impurity for many splits\n",
    "\n",
    "Parallel efficiency: While both are parallel, ET has less overhead per split\n",
    "\n",
    "Complexity: RF: O(k⋅d⋅n⋅log n) vs ET: O(d⋅n⋅log n) where k is # of split evaluations\n",
    "\n",
    "Q2: When would Extra Trees perform worse than Random Forest?\n",
    "Four scenarios:\n",
    "\n",
    "Very small datasets (< 1000 samples) - RF's optimal splits matter more\n",
    "\n",
    "Clean, deterministic data - RF can find perfect splits\n",
    "\n",
    "Features with critical thresholds - ET might miss important cutpoints\n",
    "\n",
    "Competition settings - RF usually achieves slightly higher accuracy with tuning\n",
    "\n",
    "\n",
    "Q3: How does the bootstrap parameter affect Extra Trees?\n",
    "\n",
    "bootstrap=True (default):\n",
    "  - Creates diversity through data sampling\n",
    "  - Enables OOB error estimates\n",
    "  - Better for variance reduction\n",
    "\n",
    "bootstrap=False:\n",
    "  - Uses entire dataset for each tree\n",
    "  - Lower bias, especially with small datasets\n",
    "  - No OOB estimates available\n",
    "  - Faster training (no sampling overhead)\n",
    "\n",
    "Q4: Can Extra Trees handle categorical features better than RF?\n",
    "Yes, in some cases:\n",
    "\n",
    "For high-cardinality categorical features, ET's random splits can be beneficial\n",
    "\n",
    "RF might overfit to specific category thresholds\n",
    "\n",
    "ET treats all splits equally randomly\n",
    "\n",
    "Best practice: Use proper encoding (target encoding for high-cardinality)\n",
    "\n",
    "Q5: How to choose between sqrt(n_features) and all features for max_features?\n",
    "\n",
    "Use sqrt(n_features) when:\n",
    "  - Many irrelevant features\n",
    "  - Want stronger regularization\n",
    "  - Training time is concern\n",
    "\n",
    "Use all features (max_features=None) when:\n",
    "  - Few features (< 20)\n",
    "  - Most features are informative\n",
    "  - Want lower bias\n",
    "  - Dataset is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55962b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = [[0,0], [1,1], [1,0], [0,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=100, max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[1,0]]))\n",
    "print(clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.1])\n",
    "\n",
    "reg = ExtraTreesRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.predict([[6]]))\n",
    "print(reg.feature_importances_)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
