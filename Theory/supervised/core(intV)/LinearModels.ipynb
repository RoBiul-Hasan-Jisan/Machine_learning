{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b57065",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "Linear Regression (Simple/Multiple)\n",
    "\n",
    "Ridge Regression (L2 regularization) \n",
    "\n",
    "Lasso Regression (L1 regularization) \n",
    "\n",
    "ElasticNet Regression (L1 + L2)  \n",
    "\n",
    "Bayesian Linear Regression\n",
    "\n",
    "Polynomial Regression  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6c75c",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear Regression is a **supervised learning** algorithm used to predict a **continuous value** by finding a linear relationship between input features ($X$) and output ($Y$).\n",
    "\n",
    "\n",
    "\n",
    "[Image of linear regression best fit line]\n",
    "\n",
    "\n",
    "### 1. The Equation\n",
    "\n",
    "**Simple Linear Regression (One Feature):**\n",
    "$$y = mx + c$$\n",
    "* $y$: Predicted value (Target)\n",
    "* $x$: Input feature\n",
    "* $m$: Slope (Weight/Coefficient)\n",
    "* $c$: Intercept (Bias)\n",
    "\n",
    "**Multiple Linear Regression (Many Features):**\n",
    "$$y = w_1x_1 + w_2x_2 + \\dots + w_n x_n + b$$\n",
    "\n",
    "### 2. Goal of Linear Regression\n",
    "To find the **best-fit line** that minimizes the prediction error between actual and predicted values.\n",
    "\n",
    "### 3. Cost Function (Loss Function)\n",
    "We use the **Mean Squared Error (MSE)** to measure how \"wrong\" the model is.\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_{actual}^{(i)} - y_{predicted}^{(i)})^2$$\n",
    "\n",
    "> **Why Squared Error?** It penalizes large errors more strongly than small errors (squaring a large number makes it huge).\n",
    "\n",
    "### 4. How the Model Learns\n",
    "It uses **Gradient Descent** to iteratively update the weights and move in the direction of minimum error.\n",
    "\n",
    "**Update Rule:**\n",
    "$$w := w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "* $\\alpha$ (alpha): **Learning Rate** (controls the step size).\n",
    "* $\\frac{\\partial J}{\\partial w}$: Gradient (slope of the error curve).\n",
    "\n",
    "### 5. Assumptions of Linear Regression\n",
    "1.  **Linearity:** Linear relationship between $X$ and $Y$.\n",
    "2.  **No Multicollinearity:** Independent variables should not be highly correlated.\n",
    "3.  **Normality:** Errors (residuals) are normally distributed.\n",
    "4.  **Homoscedasticity:** Constant variance of errors.\n",
    "5.  **Independence:** Observations are independent of each other.\n",
    "\n",
    "### 6. Types of Linear Regression\n",
    "* **Simple Linear Regression:** 1 independent variable.\n",
    "* **Multiple Linear Regression:** Multiple independent variables.\n",
    "* **Polynomial Regression:** Models non-linear patterns using a linear model (by adding powers of features, e.g., $x^2$).\n",
    "\n",
    "### 7. Evaluation Metrics\n",
    "* **MSE:** Mean Squared Error.\n",
    "* **RMSE:** Root Mean Squared Error.\n",
    "* **MAE:** Mean Absolute Error.\n",
    "* **$R^2$ Score:** Goodness of fit (Explains how much variance in $Y$ is explained by $X$).\n",
    "\n",
    "### 8. Advantages vs. Limitations\n",
    "\n",
    "| Advantages | Limitations |\n",
    "| :--- | :--- |\n",
    "| Simple & fast to train. | Poor for non-linear data. |\n",
    "| Easy to interpret (coefficients tell you the impact). | Sensitive to outliers. |\n",
    "| Works well for linearly separable data. | Strongly dependent on assumptions. |\n",
    "\n",
    "### 9. Real-World Use Cases\n",
    "* **House Price Prediction:** Based on square footage, location, etc.\n",
    "* **Salary Prediction:** Based on years of experience.\n",
    "* **Sales Forecasting:** Predicting future revenue.\n",
    "* **Risk Analysis:** Insurance premium calculation.\n",
    "\n",
    "### FAQ\n",
    "**When NOT to use Linear Regression?**\n",
    "* When data is highly non-linear.\n",
    "* When the dataset has many outliers (unless removed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb94b9",
   "metadata": {},
   "source": [
    "# Linear Regression: Deep Dive & Nuances\n",
    "\n",
    "This section covers the \"why\" and \"how\" behind the scenes, perfect for interview prep or advanced tuning.\n",
    "\n",
    "### 1. Cost Function - The \"Why\" of MSE\n",
    "\n",
    "#### MSE vs. MAE\n",
    "* **MSE (Mean Squared Error):** Used because it is **differentiable everywhere**, which is essential for Gradient Descent to work smoothly. It penalizes large errors quadratically ($error^2$), making the model **sensitive to outliers**.\n",
    "* **MAE (Mean Absolute Error):** Less sensitive to outliers, but it is not smoothly differentiable at $0$ (the gradient is undefined at the exact bottom), which can cause convergence issues.\n",
    "\n",
    "#### The Normal Equation (The Alternative)\n",
    "Gradient Descent is an iterative approach. The **Normal Equation** provides a closed-form solution to find the optimal weights $\\theta$ in one step using linear algebra:\n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "* **Pros:** Exact solution, no learning rate needed.\n",
    "* **Cons:** Computationally expensive ($O(n^3)$) for large feature sets because inverting the matrix $X^T X$ is slow.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Gradient Descent - Nuances\n",
    "\n",
    "\n",
    "\n",
    "#### Learning Rate ($\\alpha$)\n",
    "* This is a critical hyperparameter.\n",
    "* **Too High:** The model overshoots the minimum and diverges.\n",
    "* **Too Low:** Convergence is extremely slow.\n",
    "\n",
    "#### Types of Gradient Descent\n",
    "1.  **Batch GD:** Uses **all** training data for every step. Accurate but very slow for large data.\n",
    "2.  **Stochastic GD (SGD):** Uses **one** sample per step. Fast but noisy (jumps around).\n",
    "3.  **Mini-batch GD:** Uses a small subset (e.g., 32 or 64 samples) per step. The **standard choice** in Deep Learning (best of both worlds).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Assumptions - A Deeper Dive\n",
    "\n",
    "\n",
    "\n",
    "| Assumption | How to Check | Consequence if Violated | Fix |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Linearity** | Scatter plots of $y$ vs. each $x$. | Poor predictions. | Log-transform features or use Polynomial Regression. |\n",
    "| **No Multicollinearity** | **VIF (Variance Inflation Factor)**. If VIF $> 5$ or $10$, it's an issue. | Inflates coefficient variance (makes specific weights unreliable). | Drop features, PCA, or **Ridge Regression**. |\n",
    "| **Normality of Errors** | **Q-Q Plot** of residuals. | P-values & confidence intervals become invalid. | Check for outliers; rely on Central Limit Theorem (CLT) for large $n$. |\n",
    "| **Homoscedasticity** | Plot **Residuals vs. Fitted Values**. | Standard errors are unreliable. | Log-transform target ($y$) or use Weighted Least Squares. |\n",
    "| **Independence** | **Durbin-Watson statistic** (for time series). | Invalidates statistical tests. | Use Time Series models (ARIMA). |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Interpretation & Pitfalls\n",
    "\n",
    "#### Interpretation of Coefficients\n",
    "*\"Holding all other features constant, a one-unit increase in $X_1$ is associated with an average change of $w_1$ units in $Y$.\"*\n",
    "\n",
    "#### The P-Value Trap\n",
    "A low p-value ($< 0.05$) means a relationship is statistically **significant**, not necessarily **important**.\n",
    "* **Always check the Effect Size** (the magnitude of the coefficient). A feature can be significant but have a tiny impact.\n",
    "\n",
    "#### Overfitting\n",
    "Even linear models can overfit if you have too many features ($p$) relative to samples ($n$).\n",
    "* **Solution:** Regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Regularization Connection\n",
    "\n",
    "#### Ridge Regression (L2)\n",
    "Adds a penalty based on the **squared magnitude** of coefficients.\n",
    "$$Cost = MSE + \\lambda \\sum w_i^2$$\n",
    "* **Effect:** Shrinks coefficients toward zero but **does not** zero them out.\n",
    "* **Use Case:** Best for handling **Multicollinearity**.\n",
    "\n",
    "#### Lasso Regression (L1)\n",
    "Adds a penalty based on the **absolute value** of coefficients.\n",
    "$$Cost = MSE + \\lambda \\sum |w_i|$$\n",
    "* **Effect:** Can shrink coefficients to **exactly zero**.\n",
    "* **Use Case:** Performs automatic **Feature Selection**.\n",
    "\n",
    "#### ElasticNet\n",
    "Combines L1 and L2 penalties.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Advanced Interview One-Liners\n",
    "\n",
    "**Q: What if residuals are not normally distributed?**\n",
    "* **A:** The model's coefficient estimates are still unbiased (correct on average), but hypothesis tests (p-values, confidence intervals) become invalid. For large sample sizes, the **Central Limit Theorem (CLT)** often saves us.\n",
    "\n",
    "**Q: Is Linear Regression a parametric or non-parametric model?**\n",
    "* **A:** **Parametric.** It makes a strong assumption about the form of the underlying function (linear in parameters) and has a fixed number of parameters ($w_0, w_1...$).\n",
    "\n",
    "**Q: Can you use Linear Regression for classification?**\n",
    "* **A:** Technically yes (e.g., predict a probability), but it is unsuitable because:\n",
    "    1.  Outputs can be outside $[0, 1]$.\n",
    "    2.  It assumes normality of errors, while classification errors are Bernoulli distributed.\n",
    "    * **Use Logistic Regression instead.**\n",
    "\n",
    "**Q: How do you handle categorical variables?**\n",
    "* **A:** Use **One-Hot Encoding**. *Crucial:* Drop one category to avoid the **Dummy Variable Trap** (perfect multicollinearity, where one variable can be predicted perfectly from the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression from scratch using Gradient Descen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = self.w * X + self.b\n",
    "\n",
    "            dw = (-2/n) * np.sum(X * (y - y_pred))\n",
    "            db = (-2/n) * np.sum(y - y_pred)\n",
    "\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.w * X + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70528b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.98848257]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict(np.array([6])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8c1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "prediction = model.predict([[6]])\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5960d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: [2.]\n",
      "Intercept: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Slope:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffccdcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0\n",
      "R2 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62076b",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "Polynomial Regression is a special case of Linear Regression that models a **non-linear relationship** between the independent variable $x$ and the dependent variable $y$ by adding polynomial terms to the linear equation.\n",
    "\n",
    "\n",
    "\n",
    "### 1. The Equation\n",
    "Instead of a straight line ($y = w_1x + b$), we fit a curve:\n",
    "\n",
    "$$y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_n x^n$$\n",
    "\n",
    "* **Linear in Parameters:** The weights ($w_0, w_1, \\dots$) are still linear (power of 1).\n",
    "* **Non-Linear in Features:** The features ($x, x^2, x^3$) are non-linear.\n",
    "\n",
    "> **Key Insight:** Because the parameters ($w$) are linear, we can still use the standard `LinearRegression` algorithm to solve this! We simply \"trick\" the model by creating new features ($x^2, x^3$) and feeding them in as if they were distinct variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When to Use It?\n",
    "* **Curved Data:** When a scatter plot shows a clear curve (parabola, S-shape) rather than a straight line.\n",
    "* **Underfitting:** When a simple Linear Regression model performs poorly (high bias).\n",
    "* **Smooth Relationships:** When the change in $y$ is continuous and smooth relative to $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Degree of the Polynomial ($d$)\n",
    "Choosing the right degree is the most critical step.\n",
    "\n",
    "* **Low Degree (e.g., $d=1$):** **Underfitting** (High Bias). The model is too simple to capture the pattern.\n",
    "* **Optimal Degree:** Captures the underlying trend without fitting the noise.\n",
    "* **High Degree (e.g., $d=10$):** **Overfitting** (High Variance). The curve typically becomes \"wiggly,\" passing through every data point but failing to generalize.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Overfitting Risk & Solutions\n",
    "Polynomial Regression is notorious for overfitting, especially with high degrees. The curve can shoot off to $\\pm \\infty$ at the edges.\n",
    "\n",
    "**Solutions:**\n",
    "1.  **Regularization:** Use **Ridge** or **Lasso** regression instead of standard Linear Regression to penalize large coefficients.\n",
    "2.  **Cross-Validation:** Test different degrees (e.g., 1 to 5) and choose the one with the lowest validation error.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. FAQ\n",
    "\n",
    "**Q: Why not always use a high-degree polynomial?**\n",
    "* **A:** It causes massive overfitting. The model starts modeling the random noise in the data rather than the actual signal.\n",
    "\n",
    "**Q: Is Polynomial Regression considered a non-linear model?**\n",
    "* **A:** It is **non-linear in features** (the shape is curved) but **linear in parameters** (mathematically solvable using linear algebra).\n",
    "\n",
    "**Q: How do I choose the best degree?**\n",
    "* **A:** Use a loop with **Cross-Validation**. Plot the training error vs. validation error. The \"sweet spot\" is where validation error is lowest before it starts rising again.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Code Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Create the Polynomial Features (e.g., degree=2 generates x and x^2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# 2. Use a Pipeline to streamline the process\n",
    "# This automatically transforms data, then fits the model\n",
    "model = make_pipeline(poly, LinearRegression())\n",
    "\n",
    "# 3. Train\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_poly, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a258d",
   "metadata": {},
   "source": [
    "# Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge Regression is a regularized version of Linear Regression that adds an **L2 penalty** to the loss function to prevent overfitting. It is particularly useful when **multicollinearity** (high correlation between independent variables) exists.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Why Ridge Regression?\n",
    "Standard Linear Regression often suffers from:\n",
    "* **Overfitting:** Memorizing the noise in the training data.\n",
    "* **Large Coefficients:** The model assigns huge weights to features to fit the data perfectly.\n",
    "* **Multicollinearity:** Correlated features cause unstable, high-variance weights.\n",
    "\n",
    "**Ridge Solution:** It forces the learning algorithm to not only fit the data but also keep the model weights ($w$) as small as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Loss Function\n",
    "The objective is to minimize the standard error **plus** a penalty term based on the squared magnitude of the weights.\n",
    "\n",
    "$$Loss = MSE + \\lambda \\sum_{j=1}^{p} w_j^2$$\n",
    "\n",
    "* **MSE:** Mean Squared Error (Prediction error).\n",
    "* **$\\lambda$ (Lambda):** Regularization strength (called `alpha` in Scikit-Learn).\n",
    "* **$\\sum w^2$:** **L2 Penalty**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Effect of L2 Penalty\n",
    "1.  **Penalizes Large Weights:** The term $\\lambda w^2$ explodes if $w$ gets large, forcing the optimizer to choose smaller $w$ values.\n",
    "2.  **Shrinkage:** It shrinks coefficients **towards zero**.\n",
    "3.  **Non-Sparse:** Unlike Lasso, it **never** makes weights exactly zero. It keeps all features but reduces their impact.\n",
    "4.  **Variance Reduction:** It slightly increases bias (underfitting) to drastically reduce variance (overfitting).\n",
    "\n",
    "### 4. Role of $\\lambda$ (Lambda/Alpha)\n",
    "* **$\\lambda = 0$:** Identical to normal Linear Regression.\n",
    "* **Small $\\lambda$:** Slight regularization.\n",
    "* **Large $\\lambda$:** Strong regularization $\\rightarrow$ Coefficients shrink to near zero $\\rightarrow$ **Underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. When to Use Ridge Regression?\n",
    "* When you have **Multicollinearity** (many correlated features).\n",
    "* When **all features are important** (you don't want to delete any, just reduce their noise).\n",
    "* When you want a **stable model** where small changes in input data don't cause wild swings in predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Technical Deep Dive\n",
    "\n",
    "#### Gradient Descent Update\n",
    "Because of the extra penalty term, the gradient calculation changes.\n",
    "* **Normal Gradient:** $\\nabla_{MSE}$\n",
    "* **Ridge Gradient:** $\\nabla_{MSE} + 2\\lambda w$\n",
    "\n",
    "This extra term ($2\\lambda w$) is often called \"Weight Decay\" because it subtracts a fraction of the weight from itself at every step.\n",
    "\n",
    "#### Handling Multicollinearity\n",
    "If Feature A and Feature B are highly correlated, standard Linear Regression might make $w_A = 100$ and $w_B = -99$. Ridge Regression will force them to share the weight, e.g., $w_A = 0.5, w_B = 0.5$, which is much more stable.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. FAQ\n",
    "\n",
    "**Q: Does Ridge remove features?**\n",
    "**A:** No. It shrinks weights close to zero but not *exactly* to zero. Use Lasso if you need feature selection.\n",
    "\n",
    "**Q: What happens if alpha is too high?**\n",
    "**A:** The model underfits. It becomes a horizontal line (intercept only) because all weights are forced to be essentially zero.\n",
    "\n",
    "**Q: Is Ridge linear?**\n",
    "**A:** Yes. It is still a linear model (linear in parameters).\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# alpha corresponds to lambda in the math formula\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "# ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "print(ridge_reg.coef_) \n",
    "# You will see smaller coefficients compared to standard LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d31e2",
   "metadata": {},
   "source": [
    "# Lasso Regression (L1 Regularization)\n",
    "\n",
    "**Lasso** stands for **L**east **A**bsolute **S**hrinkage and **S**election **O**perator. It is a regularized regression technique that adds an **L1 penalty** to the loss function.\n",
    "\n",
    "**Main Purpose:** Reduce overfitting + **Perform Feature Selection**.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Why Lasso?\n",
    "Standard Linear Regression struggles when:\n",
    "* There is **overfitting** due to high variance.\n",
    "* There are **too many irrelevant features** (noise).\n",
    "\n",
    "**Lasso Solution:** It solves this by forcing the weights of irrelevant features to become **exactly zero**. This creates \"sparse models\" (models with fewer features).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Loss Function\n",
    "The objective is to minimize the error plus the sum of the **absolute values** of the weights.\n",
    "\n",
    "$$Loss = MSE + \\lambda \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "* **MSE:** Mean Squared Error.\n",
    "* **$\\lambda$ (Lambda/Alpha):** Regularization strength.\n",
    "* **$\\sum |w|$:** **L1 Penalty**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Effect of L1 Penalty (The \"Magic\")\n",
    "1.  **Penalizes Absolute Values:** Instead of squaring the weights (Ridge), Lasso takes the absolute value.\n",
    "2.  **Feature Selection:** This mathematical property pushes coefficients of less important features to **exactly zero**.\n",
    "3.  **Sparsity:** The result is a model that effectively \"deletes\" useless columns from your dataset.\n",
    "\n",
    "### 4. Geometric Intuition\n",
    "Why does Lasso hit zero while Ridge doesn't?\n",
    "* **Ridge (L2):** The constraint region is a **Circle**. The error contours usually touch the circle at a point *close* to the axis but rarely *on* it.\n",
    "* **Lasso (L1):** The constraint region is a **Diamond** (with sharp corners). The error contours are statistically much more likely to hit the \"corners\" of the diamond. These corners lie exactly on the axis, where coefficients are zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. When to Use Lasso?\n",
    "* When your dataset has **many irrelevant features** (you suspect only a few are actually useful).\n",
    "* When you need **Feature Selection** built into the model.\n",
    "* When you want a **simpler, interpretable model** (Sparse Model).\n",
    "\n",
    "### 6. Limitations\n",
    "* **Multicollinearity:** If features are highly correlated, Lasso arbitrarily picks **one** and reduces the others to zero. It doesn't handle groups of correlated features well.\n",
    "    * **Solution:** Use **ElasticNet** (which combines L1 and L2).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. FAQ\n",
    "\n",
    "**Q: Why does Lasso perform feature selection?**\n",
    "**A:** The L1 penalty (absolute value) creates a constraint with \"sharp corners\" at zero. During optimization, the weights get stuck at these zero points.\n",
    "\n",
    "**Q: Can Lasso handle multicollinearity well?**\n",
    "**A:** No. It is unstable with correlated features. It tends to pick one variable from a correlated group and ignore the rest randomly.\n",
    "\n",
    "**Q: What happens if alpha is too large?**\n",
    "**A:** Underfitting. The model will zero out *all* weights, resulting in a flat line prediction (the mean of $y$).\n",
    "\n",
    "**Q: Ridge or Lasso for interpretability?**\n",
    "**A:** **Lasso.** Because it removes irrelevant features, looking at the remaining non-zero weights gives a clearer picture of what actually matters.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# alpha=1.0 is the default regularization strength\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "# lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Check which coefficients are zero\n",
    "print(lasso_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f01080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aaf324",
   "metadata": {},
   "source": [
    "# ElasticNet Regression (L1 + L2)\n",
    "\n",
    "ElasticNet is a regularized linear regression technique that **combines** the penalties of **Lasso (L1)** and **Ridge (L2)**.\n",
    "\n",
    "**Use When:**\n",
    "* You have many features (high-dimensional data).\n",
    "* Features are highly correlated (multicollinearity).\n",
    "* You need both **feature selection** (sparsity) and **stability**.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Why ElasticNet? (The Best of Both Worlds)\n",
    "* **Lasso Limitation:** It is unstable with correlated features (it picks one randomly and drops the others).\n",
    "* **Ridge Limitation:** It keeps all features (no feature selection), leading to complex models.\n",
    "* **ElasticNet Solution:** It groups correlated features together (Ridge effect) and can select the whole group or drop the whole group (Lasso effect).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Loss Function\n",
    "The objective is to minimize the prediction error plus *both* penalty terms.\n",
    "\n",
    "$$Loss = MSE + \\lambda_1 \\sum |w| + \\lambda_2 \\sum w^2$$\n",
    "\n",
    "* **MSE:** Mean Squared Error.\n",
    "* **$\\lambda_1 \\sum |w|$:** L1 Penalty (promotes sparsity/zeroing weights).\n",
    "* **$\\lambda_2 \\sum w^2$:** L2 Penalty (promotes stability/shrinking weights).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Hyperparameters\n",
    "In Scikit-Learn, this is controlled by two main parameters:\n",
    "\n",
    "**1. `alpha` ($a + b$)**\n",
    "* Controls the **overall regularization strength**.\n",
    "* Higher $\\alpha$ $\\rightarrow$ Stronger regularization $\\rightarrow$ Simpler model.\n",
    "\n",
    "**2. `l1_ratio` ($\\rho$)**\n",
    "* Controls the **balance** between L1 and L2.\n",
    "* `l1_ratio = 1`: **Lasso Regression** (Only L1).\n",
    "* `l1_ratio = 0`: **Ridge Regression** (Only L2).\n",
    "* `0 < l1_ratio < 1`: Mixed (ElasticNet).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Comparison Table\n",
    "\n",
    "| Feature | Ridge (L2) | Lasso (L1) | ElasticNet |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **L1 Penalty** | No | Yes | **Yes** |\n",
    "| **L2 Penalty** | Yes | No | **Yes** |\n",
    "| **Feature Selection** | No | Yes | **Yes** |\n",
    "| **Correlated Features** | Best (Shrinks together) | Poor (Randomly picks one) | **Best** (Groups them) |\n",
    "| **Stability** | High | Medium | **High** |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. When to Use ElasticNet?\n",
    "* **High-dimensional data:** e.g., Genomics (Gene expression) or Text data where features > samples.\n",
    "* **Correlated features:** When variables move together (e.g., height and weight).\n",
    "* **Unsure:** If you don't know whether to use Lasso or Ridge, ElasticNet is usually the safest bet.\n",
    "\n",
    "> **Crucial Note on Feature Scaling:**\n",
    "> Always **scale your features** (StandardScaler) before using ElasticNet. Regularization is sensitive to the magnitude of the data; large numbers will be penalized more heavily unfairly.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. FAQ\n",
    "\n",
    "**Q: Why choose ElasticNet over Lasso?**\n",
    "**A:** Lasso can behave erratically when features are correlated. ElasticNet stabilizes this by allowing groups of correlated features to be selected together.\n",
    "\n",
    "**Q: What if `l1_ratio = 0.5`?**\n",
    "**A:** The penalty is a perfect 50/50 mix of Lasso and Ridge.\n",
    "\n",
    "**Q: Does ElasticNet set coefficients to zero?**\n",
    "**A:** Yes. Because it includes the L1 term, it can still reduce coefficients to exactly zero, performing feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# l1_ratio=0.5 means 50% Lasso, 50% Ridge\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    ")\n",
    "\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f34318",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ElasticNet\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ElasticNet(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m, y)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mcoef_)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mintercept_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217467d9",
   "metadata": {},
   "source": [
    "# Quick Comparison: Ridge vs. Lasso vs. ElasticNet\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Feature | **Ridge (L2)** | **Lasso (L1)** | **ElasticNet (L1 + L2)** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Penalty Type** | **L2** ($\\sum w^2$) | **L1** ($\\sum |w|$) | **L1 + L2** |\n",
    "| **Effect on Weights** | Shrinks coefficients towards zero, but **never exactly zero**. | Can shrink coefficients to **exactly zero**. | Shrinks coefficients + can set some to zero. |\n",
    "| **Feature Selection** | **No** (keeps all features). | **Yes** (removes irrelevant features). | **Yes**. |\n",
    "| **Correlated Features** | **Good**: Shrinks them together (shares weight). | **Poor**: Randomly keeps one, drops others. | **Best**: Groups them and shrinks/selects together. |\n",
    "| **Stability** | **High**. | **Medium** (sensitive to data changes). | **High**. |\n",
    "| **Best For** | When **all features are potentially important** and you want to reduce noise. | When you suspect **many features are irrelevant** (need sparsity). | When you have **many correlated features** and need feature selection. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* **Ridge (L2):** Good for keeping all features but reducing their magnitude. Ideal when you have many small effects.\n",
    "* **Lasso (L1):** The \"Sledgehammer.\" Good for aggressive feature selection. Ideal when you have a few strong signals in a sea of noise.\n",
    "* **ElasticNet:** The \"Smart Compromise.\" Best of both worlds. Use this if you are unsure or have complex, correlated data.\n",
    "\n",
    "---\n",
    "\n",
    "### How it Fits with Regression Types\n",
    "\n",
    "* **Linear Regression:** Regularization is optional but recommended if the model is overfitting or variances are high.\n",
    "* **Multiple Regression:** Ridge/Lasso/ElasticNet are standard tools when $N_{features}$ is large (or $> N_{samples}$).\n",
    "* **Polynomial Regression:** Regularization is **essential** here. High-degree polynomials almost always overfit, and Ridge/Lasso constraints tame the \"wiggles\" of the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e88a30",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "Bayesian Linear Regression is a **probabilistic approach** to linear regression.\n",
    "\n",
    "Instead of finding a single \"best\" set of weights (like in Ordinary Least Squares), it computes a **probability distribution** over all possible weights.\n",
    "\n",
    "\n",
    "\n",
    "### 1. The Core Concept\n",
    "$$y = Xw + \\epsilon$$\n",
    "\n",
    "* **Classical Linear Regression:** Estimates a single fixed weight vector $\\hat{w}$.\n",
    "* **Bayesian Regression:** Estimates a **distribution** for $w$:\n",
    "    $$p(w|X,y) \\propto p(y|X,w) \\cdot p(w)$$\n",
    "\n",
    "### 2. The Equation (Bayes' Theorem)\n",
    "\n",
    "1.  **$p(w)$ - Prior:** Our belief about the weights *before* seeing any data (e.g., \"weights should be small, close to zero\").\n",
    "2.  **$p(y|X,w)$ - Likelihood:** How likely is the data given a specific set of weights?\n",
    "3.  **$p(w|X,y)$ - Posterior:** Our updated belief about the weights *after* observing the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Advantages\n",
    "* **Uncertainty Estimates:** It doesn't just give a prediction; it gives a **confidence interval** (e.g., \"The price is $200k ± $10k\").\n",
    "* **Incorporates Prior Knowledge:** You can inject domain expertise into the model via the Prior.\n",
    "* **Reduces Overfitting:** The Prior acts as a natural regularizer (similar to Ridge/Lasso).\n",
    "* **Small Data:** Works exceptionally well when data is scarce, as the Prior helps guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Comparison: Classical vs. Bayesian\n",
    "\n",
    "| Feature | Classical (Frequentist) | Bayesian |\n",
    "| :--- | :--- | :--- |\n",
    "| **Weights ($w$)** | Single fixed values | Probability distribution |\n",
    "| **Output** | Single point prediction | Prediction + Uncertainty (Std Dev) |\n",
    "| **Overfitting** | Prone (needs explicit regularization) | Resistant (Regularization via Priors) |\n",
    "| **Philosophy** | \"Let the data speak.\" | \"Update beliefs with data.\" |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. FAQ\n",
    "\n",
    "**Q: Difference between classical and Bayesian regression?**\n",
    "**A:** Classical finds the single best line. Bayesian finds a *distribution* of likely lines.\n",
    "\n",
    "**Q: Why use Bayesian regression?**\n",
    "**A:** It provides uncertainty (confidence intervals), handles small data better, and resists overfitting naturally.\n",
    "\n",
    "**Q: Can it work with multiple features or polynomial regression?**\n",
    "**A:** Yes. The math extends naturally to multiple dimensions; the prior is just a multivariate distribution.\n",
    "\n",
    "**Q: What prior is commonly used?**\n",
    "**A:** A **Gaussian (Normal) Prior** is most common. This assumes weights are likely to be small (centered around zero), which behaves mathematically like Ridge Regression.\n",
    "\n",
    "> **Summary:**\n",
    "> * **Classical:** \"Here is the best guess.\"\n",
    "> * **Bayesian:** \"Here is the best guess, and here is how confident I am about it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d911d",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques\n",
    "\n",
    "Data preprocessing is the step where we translate \"human data\" (words, varying scales) into \"machine data\" (matrices, standardized numbers).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. One-Hot Encoding\n",
    "\n",
    "**What is it?**\n",
    "It converts categorical variables (words/labels) into a binary matrix ($0$s and $1$s).\n",
    "\n",
    "**Why use it?**\n",
    "* Models cannot perform arithmetic on words (e.g., \"Red\" + \"Blue\" = ?).\n",
    "* **Avoids Ordinality Trap:** If you assign $Red=1, Blue=2, Green=3$, the model assumes $Blue > Red$ or $Blue = \\frac{Red + Green}{2}$. This is false logic.\n",
    "* One-Hot Encoding treats all categories as **orthogonal vectors** (independent and equal).\n",
    "\n",
    "**When to use it?**\n",
    "* **Nominal Data:** Categories with **no inherent order** (e.g., Color, City, Gender, Brand).\n",
    "* *Note:* Do not use for Ordinal Data (e.g., Low/Medium/High) — use Label/Ordinal Encoding instead.\n",
    "\n",
    "**How it Works (The Transformation):**\n",
    "We map the categorical column vector $C$ to a binary matrix $M$.\n",
    "\n",
    "**Original Column:**\n",
    "$$C = \\begin{bmatrix} \\text{Red} \\\\ \\text{Blue} \\\\ \\text{Red} \\end{bmatrix}$$\n",
    "\n",
    "**Encoded Matrix:**\n",
    "$$M = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\quad \\begin{matrix} \\leftarrow \\text{(Is Red)} \\\\ \\leftarrow \\text{(Is Blue)} \\\\ \\leftarrow \\text{(Is Red)} \\end{matrix}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. StandardScaler (Standardization)\n",
    "\n",
    "**What is it?**\n",
    "Rescales data so it has a **Mean ($\\mu$) of 0** and a **Standard Deviation ($\\sigma$) of 1**. This transforms the feature into a **Standard Normal Distribution**.\n",
    "\n",
    "\n",
    "\n",
    "**Why use it?**\n",
    "* **Feature Dominance:** If Feature A ranges $[0, 1]$ and Feature B ranges $[1000, 100000]$, Feature B will dominate the distance calculations.\n",
    "* **Convergence:** It speeds up the convergence of Gradient Descent algorithms.\n",
    "\n",
    "**When to use it?**\n",
    "* **Distance-Based Algorithms:** KNN, K-Means, SVM (Crucial).\n",
    "* **Linear Models:** Linear Regression, Logistic Regression.\n",
    "* When data follows a **Gaussian (Bell Curve)** distribution.\n",
    "\n",
    "**The Math:**\n",
    "For every data point $x$:\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "* $\\mu$: Mean of the column.\n",
    "* $\\sigma$: Standard Deviation of the column.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. MinMaxScaler (Normalization)\n",
    "\n",
    "**What is it?**\n",
    "Scales data to a fixed range, usually $[0, 1]$.\n",
    "\n",
    "**Why use it?**\n",
    "* Ensures all features have the exact same scale.\n",
    "* Preserves the *shape* of the original distribution but \"squishes\" the axis.\n",
    "\n",
    "**When to use it?**\n",
    "* **Neural Networks:** Activation functions prefer inputs in $[0, 1]$.\n",
    "* **Image Processing:** Pixel intensities ($0$–$255$) are scaled to $[0, 1]$.\n",
    "* When data is **NOT** normally distributed.\n",
    "* **Warning:** Sensitive to outliers. A single outlier can push all other data points to $0$.\n",
    "\n",
    "**The Math:**\n",
    "$$x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Feature | **StandardScaler** | **MinMaxScaler** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Formula** | $$z = \\frac{x - \\mu}{\\sigma}$$|$$x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$ |\n",
    "| **Range** | No fixed range (Centered at $0$) | Fixed $[0, 1]$ |\n",
    "| **Outliers** | Robust (handles them okay) | Sensitive (ruins the scale) |\n",
    "| **Assumption** | Assumes Normal Distribution | No assumption |\n",
    "| **Best For** | SVM, KNN, Logistic Regression | Neural Networks, Images, KNN |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782d05e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
