{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b57065",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "Linear Regression (Simple/Multiple)\n",
    "\n",
    "Ridge Regression (L2 regularization) \n",
    "\n",
    "Lasso Regression (L1 regularization) \n",
    "\n",
    "ElasticNet Regression (L1 + L2)  \n",
    "\n",
    "Bayesian Linear Regression\n",
    "\n",
    "Polynomial Regression  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6c75c",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "\n",
    "Linear Regression is a supervised learning algorithm used to predict a continuous value by finding a linear relationship between input features (X) and output (Y).\n",
    "\n",
    "Equation\n",
    "\n",
    "Simple Linear Regression:\n",
    "\n",
    "\n",
    "y=mx+c\n",
    "\n",
    "y ‚Üí predicted value\n",
    "\n",
    "x ‚Üí input feature\n",
    "\n",
    "m ‚Üí slope (weight)\n",
    "\n",
    "c ‚Üí intercept (bias)\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "\n",
    "y = w1x1 + w2x2 + ... + b\n",
    "Goal of Linear Regression\n",
    "\n",
    "To find the best-fit line that minimizes prediction error between actual and predicted values.\n",
    "\n",
    "Cost Function (Loss Function)\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "\n",
    "J(Œ∏)= 1/n ‚àë(y actual‚àíy predicted)^2\n",
    "\n",
    "Measures how wrong the model is.\n",
    "\n",
    "How Model Learns\n",
    "Gradient Descent\n",
    "\n",
    "Iteratively updates weights\n",
    "\n",
    "Moves in the direction of minimum error\n",
    "\n",
    "\n",
    "w=w‚àíŒ± (‚àÇJ/‚àÇw)\n",
    "\n",
    "\t\n",
    "\n",
    "Œ± (learning rate) controls step size\n",
    "\n",
    "Assumptions of Linear Regression\n",
    "\n",
    "Linear relationship between X and Y\n",
    "\n",
    "No multicollinearity\n",
    "\n",
    "Errors are normally distributed\n",
    "\n",
    "Homoscedasticity (constant variance)\n",
    "\n",
    "Independence of errors\n",
    "\n",
    "\n",
    "\n",
    "Types of Linear Regression\n",
    "\n",
    "Simple Linear Regression (1 feature)\n",
    "\n",
    "Multiple Linear Regression (many features)\n",
    "\n",
    "Polynomial Regression (non-linear pattern using linear model)\n",
    "\n",
    "Evaluation Metrics\n",
    "\n",
    "MSE\n",
    "\n",
    "RMSE\n",
    "\n",
    "MAE\n",
    "\n",
    "R¬≤ Score (goodness of fit)\n",
    "\n",
    "Advantages\n",
    "\n",
    "Simple & fast\n",
    "\n",
    "Easy to interpret\n",
    "\n",
    "Works well for linear data\n",
    "\n",
    "Limitations\n",
    "\n",
    " Poor for non-linear data\n",
    " Sensitive to outliers\n",
    " Assumption-dependent\n",
    "\n",
    "\n",
    "\n",
    "Why squared error?\n",
    "\n",
    "Penalizes large errors more strongly.\n",
    "\n",
    "When not to use Linear Regression?\n",
    "\n",
    "When data is non-linear or has many outliers.\n",
    "\n",
    "What is R¬≤?\n",
    "Explains how much variance in Y is explained by X.\n",
    "\n",
    "Real-World Use Cases\n",
    "\n",
    "House price prediction\n",
    "\n",
    "Salary prediction\n",
    "\n",
    "Sales forecasting\n",
    "\n",
    "Risk analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb94b9",
   "metadata": {},
   "source": [
    "1. Cost Function - The \"Why\" of MSE\n",
    "\n",
    "MSE vs. MAE: MSE is used because it's differentiable everywhere, which is essential for Gradient Descent. It penalizes large errors quadratically, making the model more sensitive to outliers. MAE is less sensitive to outliers but isn't as smoothly differentiable (gradient issues at 0).\n",
    "\n",
    "The Normal Equation: Mention this as an alternative to Gradient Descent. It provides a closed-form solution for finding optimal weights: Œ∏ = (X·µÄX)‚Åª¬πX·µÄy. It's fast for small datasets but computationally expensive (O(n¬≥)) for large feature sets.\n",
    "\n",
    "2. Gradient Descent - Nuances\n",
    "\n",
    "Learning Rate (Œ±): Critical hyperparameter. Too high ‚Üí overshoot & diverge. Too low ‚Üí slow convergence.\n",
    "\n",
    "Types: Batch GD (uses all data, slow), Mini-batch GD (uses a subset, best of both worlds), Stochastic GD (uses one sample per step, noisy but fast).\n",
    "\n",
    "3. Assumptions - Deeper Dive \n",
    "\n",
    "Linearity: Check with scatter plots of y vs each X. If violated, consider transformations or Polynomial Regression.\n",
    "\n",
    "No Multicollinearity: Check with Variance Inflation Factor (VIF). If high (>5 or 10), it inflates coefficient variance. Fix with feature removal, PCA, or regularization (Ridge Regression).\n",
    "\n",
    "Normality of Errors: Check with a Q-Q plot of residuals. Violation affects confidence intervals & p-values, but predictions might still be okay. Central Limit Theorem helps with large n.\n",
    "\n",
    "Homoscedasticity: Check with residuals vs. fitted values plot. If violated (heteroscedasticity), standard errors are unreliable. Consider transformations (log(y)) or weighted least squares.\n",
    "\n",
    "Independence of Errors: Critical for time series. Violation (autocorrelation) invalidates tests. Use time series models (ARIMA) or check with Durbin-Watson statistic.\n",
    "\n",
    "4. Model Interpretation & Pitfalls\n",
    "\n",
    "Interpretation of Coefficients: \"Holding all other features constant, a one-unit increase in X1 is associated with an average change of w1 units in Y.\"\n",
    "\n",
    "The P-Value Trap: A low p-value for a coefficient doesn't mean the predictor is important, just that the relationship is precise. Always check the effect size (coefficient magnitude).\n",
    "\n",
    "Overfitting: Even linear models can overfit with many features. Solution: Regularization (L1/Lasso, L2/Ridge).\n",
    "\n",
    "5. Regularization Connection \n",
    "\n",
    "Ridge Regression (L2): Adds penalty Œª * Œ£(w·µ¢¬≤) to MSE. Shrinks coefficients but doesn't zero them out. Good for multicollinearity.\n",
    "\n",
    "Lasso Regression (L1): Adds penalty Œª * Œ£|w·µ¢|. Can shrink coefficients to exactly zero, performing feature selection.\n",
    "\n",
    "ElasticNet: Combines L1 and L2 penalties.\n",
    "\n",
    "6. Advanced Interview One-Liners\n",
    "\n",
    "\n",
    "Q: What if residuals are not normally distributed?\n",
    "\n",
    "A: The model's coefficient estimates are still unbiased, but hypothesis tests (p-values, confidence intervals) become invalid. For large sample sizes, CLT often saves us.\n",
    "\n",
    "Q: Is Linear Regression a parametric or non-parametric model?\n",
    "\n",
    "A: Parametric. It makes a strong assumption about the form of the underlying function (linear in parameters).\n",
    "\n",
    "Q: Can you use Linear Regression for classification?\n",
    "\n",
    "A: Technically yes (e.g., predict probability), but it's unsuitable as outputs can be outside [0,1] and error distribution is wrong. Use Logistic Regression instead.\n",
    "\n",
    "Q: How do you handle categorical variables?\n",
    "\n",
    "A: Use One-Hot Encoding. Remember to drop one category to avoid the dummy variable trap (perfect multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression from scratch using Gradient Descen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = self.w * X + self.b\n",
    "\n",
    "            dw = (-2/n) * np.sum(X * (y - y_pred))\n",
    "            db = (-2/n) * np.sum(y - y_pred)\n",
    "\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.w * X + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70528b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.98848257]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict(np.array([6])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8c1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "prediction = model.predict([[6]])\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5960d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: [2.]\n",
      "Intercept: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Slope:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffccdcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0\n",
      "R2 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62076b",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "Polynomial Regression is a type of Linear Regression that models a non-linear relationship by transforming input features into polynomial terms.\n",
    "\n",
    "It is linear in parameters, but non-linear in features.\n",
    "\n",
    "Because the model is linear in weights (w), even though x is raised to powers.\n",
    "\n",
    "When to Use Polynomial Regression?\n",
    "\n",
    "Data shows curved / non-linear pattern\n",
    "\n",
    "Linear Regression underfits\n",
    "\n",
    "Relationship is smooth and continuous\n",
    "\n",
    "Degree of Polynomial\n",
    "\n",
    "Low degree ‚Üí Underfitting (high bias)\n",
    "\n",
    "High degree ‚Üí Overfitting (high variance)\n",
    "\n",
    "\n",
    "Overfitting Risk \n",
    "\n",
    "Polynomial Regression overfits easily, especially with high degree.\n",
    "\n",
    "Regularization (Ridge/Lasso) is commonly used\n",
    "\n",
    "Cross-validation to choose degree\n",
    "\n",
    "\n",
    "Q: Why not always use high-degree polynomial?\n",
    "\n",
    "Causes overfitting.\n",
    "\n",
    "Q: Is Polynomial Regression non-linear?\n",
    "\n",
    "Non-linear in features, linear in parameters.\n",
    "\n",
    "Q: How to choose degree?\n",
    "\n",
    "Cross-validation.\n",
    "\n",
    "Q: How to reduce overfitting?\n",
    "\n",
    "Regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_poly, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac01d4",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "Ridge Regression is a regularized version of Linear / Multiple Linear Regression that adds an L2 penalty to the loss function to reduce overfitting.\n",
    "Mainly used when multicollinearity exists.\n",
    "Why Ridge Regression?\n",
    "\n",
    "Linear Regression problems:\n",
    "\n",
    "Overfitting\n",
    "\n",
    "Large coefficients\n",
    "\n",
    "Unstable weights (multicollinearity)\n",
    "\n",
    "\n",
    "Ridge solves these by shrinking weights\n",
    "\n",
    "Loss Function \n",
    "\n",
    "Loss = MSE + Œª‚àëw^2\n",
    "\n",
    "Where:\n",
    "\n",
    "MSE ‚Üí prediction error\n",
    "\n",
    "Œª ‚Üí regularization strength\n",
    "\n",
    "‚àëw^2‚Üí L2 penalty\n",
    "\n",
    "Effect of L2 Penalty\n",
    "\n",
    "Penalizes large weights :\n",
    "\n",
    "Shrinks coefficients towards zero\n",
    "\n",
    "Never makes weights exactly zero\n",
    "\n",
    "\n",
    "\n",
    "Ridge reduces variance without feature elimination.\n",
    "\n",
    "\n",
    "Role of Œª (Lambda)\n",
    "\n",
    "Œª = 0 ‚Üí Normal Linear Regression\n",
    "\n",
    "Small Œª ‚Üí Slight regularization\n",
    "\n",
    "Large Œª ‚Üí Strong regularization ‚Üí Underfitting\n",
    "\n",
    "When to Use Ridge Regression?\n",
    "\n",
    "Many correlated features\n",
    "\n",
    "All features are important\n",
    "\n",
    "Want stable model\n",
    "\n",
    "#### Loss = MSE + lambda * sum(w^2)\n",
    "#### Gradient update adds extra term: 2 * lambda * w\n",
    "\n",
    "Why Ridge handles multicollinearity?\n",
    "Shrinks correlated feature weights together.\n",
    "\n",
    "Q: Does Ridge remove features?\n",
    "No.\n",
    "\n",
    "Q: What happens if alpha is too high?\n",
    "Underfitting.\n",
    "\n",
    "Q: Is Ridge linear?\n",
    "Yes, linear in parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d31e2",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "Lasso Regression is a regularized linear regression technique that adds an L1 penalty to the loss function.\n",
    "\n",
    "Main purpose: Reduce overfitting + perform feature selection\n",
    "\n",
    "Why Lasso?\n",
    "\n",
    "Problems with Linear Regression:\n",
    "Overfitting\n",
    "Too many irrelevant features\n",
    "\n",
    "Lasso solves this by forcing some weights to exactly zero\n",
    "\n",
    "Lasso creates sparse models.\n",
    "\n",
    "Effect of L1 Penalty\n",
    "\n",
    "Penalizes absolute value of weights\n",
    "\n",
    "Pushes some weights to exactly zero\n",
    "\n",
    "Automatically performs feature selection\n",
    "\n",
    "When to Use Lasso?\n",
    "\n",
    "Dataset has many irrelevant features\n",
    "Need feature selection\n",
    "Want simpler, interpretable model\n",
    "\n",
    "\n",
    "Geometric Intuition \n",
    "\n",
    "L1 penalty has sharp corners\n",
    "\n",
    "Optimization often hits axis ‚Üí zero coefficients\n",
    "\n",
    "Lasso Limitation\n",
    "\n",
    "If features are highly correlated, Lasso picks only one and ignores others\n",
    "\n",
    "Solution: ElasticNet\n",
    "\n",
    "Why does Lasso perform feature selection?\n",
    "L1 penalty forces weights to zero.\n",
    "\n",
    "Q: Can Lasso handle multicollinearity well?\n",
    "No.\n",
    "\n",
    "Q: What happens if alpha is too large?\n",
    "Underfitting.\n",
    "\n",
    "Q: Ridge or Lasso for interpretability?\n",
    "Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f01080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aaf324",
   "metadata": {},
   "source": [
    "# ElasticNet Regression (L1 + L2)\n",
    "\n",
    "What is ElasticNet Regression?\n",
    "\n",
    "ElasticNet is a regularized linear regression technique that combines L1 (Lasso) and L2 (Ridge) penalties.\n",
    "Used when:\n",
    "\n",
    "Many features\n",
    "\n",
    "Features are highly correlated\n",
    "\n",
    "Need feature selection + stability\n",
    "\n",
    "\n",
    "Why ElasticNet?\n",
    "\n",
    "Problems:\n",
    "\n",
    "Lasso ‚Üí unstable with correlated features\n",
    "\n",
    "Ridge ‚Üí no feature selection\n",
    "\n",
    "ElasticNet gets the best of both\n",
    "\n",
    "ElasticNet balances sparsity and stability\n",
    "\n",
    "Hyperparameters\n",
    "\n",
    "Œ± (alpha)\n",
    "\n",
    "Overall regularization strength\n",
    "\n",
    "Higher Œ± ‚Üí more regularization\n",
    "\n",
    "l1_ratio\n",
    "\n",
    "Controls balance between L1 and L2\n",
    "\n",
    "l1_ratio = 1 ‚Üí Lasso\n",
    "\n",
    "l1_ratio = 0 ‚Üí Ridge\n",
    "\n",
    "\n",
    "Effect of ElasticNet\n",
    "\n",
    "Shrinks coefficients (L2 effect)\n",
    "\n",
    "Sets some coefficients to zero (L1 effect)\n",
    "\n",
    "Handles correlated features better than Lasso\n",
    "\n",
    "``` bash\n",
    " Feature              Ridge  Lasso   ElasticNet \n",
    " -------------------  -----  ------  ---------- \n",
    " L1 Penalty           no     yes     yes          \n",
    " L2 Penalty           yes    no      yes          \n",
    " Feature Selection    no     yes     yes          \n",
    " Correlated Features  Best   Poor    Best       \n",
    " Stability            High   Medium  High       \n",
    "\n",
    "```\n",
    "\n",
    "When to Use ElasticNet?\n",
    "\n",
    "High-dimensional data\n",
    "Correlated features\n",
    "Want feature selection + stability\n",
    "Text / Genomics data\n",
    "\n",
    "Feature Scaling \n",
    "\n",
    "Always scale features before ElasticNet\n",
    "Regularization depends on coefficient magnitude\n",
    "\n",
    "\n",
    "Q: Why ElasticNet over Lasso?\n",
    "Handles correlated features better.\n",
    "\n",
    "Q: What does l1_ratio control?\n",
    "Balance between L1 and L2.\n",
    "\n",
    "Q: What if l1_ratio = 1?\n",
    "Lasso Regression.\n",
    "\n",
    "Q: What if l1_ratio = 0?\n",
    "Ridge Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f34318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217467d9",
   "metadata": {},
   "source": [
    "\n",
    "# Ridge vs Lasso vs ElasticNet\n",
    "```bash\n",
    " Feature                      Ridge (L2)                                Lasso (L1)                                        ElasticNet (L1 + L2)                    \n",
    " ---------------------------  ----------------------------------------  ------------------------------------------------  --------------------------------------- \n",
    " Penalty Type                 L2 ‚Üí sum of squares of weights            L1 ‚Üí sum of absolute weights                      L1 + L2                                 \n",
    " Effect on Weights            Shrinks coefficients, but never zero      Can shrink some coefficients to exactly zero      Shrinks coefficients +\n",
    "                                                                                                                            some can be zero \n",
    " Feature Selection             No                                       Yes                                               Yes                                   \n",
    " Handles Correlated Features   Well                                     Poor                                              Better than Lasso                     \n",
    " Stability                    High                                      Medium                                            High                                    \n",
    " Overfitting                  Reduced                                   Reduced                                           Reduced                                 \n",
    " Best for                     All features important                    Sparse/irrelevant features                        Many correlated features \n",
    "                                                                                                                            & sparsity\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e88a30",
   "metadata": {},
   "source": [
    "Ridge ‚Üí L2 ‚Üí good for many small correlated features, shrinks weights, no feature selection.\n",
    "\n",
    "Lasso ‚Üí L1 ‚Üí good for feature selection, some weights become zero, unstable with correlated features.\n",
    "\n",
    "ElasticNet ‚Üí L1 + L2 ‚Üí best of both worlds, shrinks weights, performs feature selection, handles correlated features.\n",
    "\n",
    "How it Fits with Regression Types\n",
    "\n",
    "Linear Regression ‚Üí Any of them can be used, mainly for overfitting control\n",
    "\n",
    "Multiple Regression ‚Üí Ridge/Lasso/ElasticNet are most used when features > 1\n",
    "\n",
    "Polynomial Regression ‚Üí Regularization is essential for high-degree polynomials to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d911d",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "Bayesian Linear Regression is a probabilistic approach to linear regression.\n",
    "\n",
    "Instead of finding single best weights, it computes a distribution over weights.\n",
    "\n",
    "Incorporates prior knowledge and updates beliefs with data (posterior).\n",
    "\n",
    "y=Xw+œµ\n",
    "\n",
    "In classical Linear Regression, we estimate a single weight vector \n",
    "ùë§\n",
    "\n",
    "\n",
    "In Bayesian Regression, we estimate a probability distribution for \n",
    "ùë§ :\n",
    "p(w‚à£X,y)‚àùp(y‚à£X,w)‚ãÖp(w)\n",
    "\n",
    "Where:\n",
    "\n",
    "p(w) = prior (belief before seeing data)\n",
    "\n",
    "\n",
    "p(y‚à£X,w) = likelihood (probability of data given weights)\n",
    "\n",
    "\n",
    "p(w‚à£X,y) = posterior (updated belief after seeing data)\n",
    "\n",
    "ADV:\n",
    "\n",
    "Provides uncertainty estimates for predictions (confidence intervals)\n",
    "\n",
    "Can incorporate prior knowledge\n",
    "\n",
    "Reduces overfitting via priors\n",
    "\n",
    "Works well with small datasets\n",
    "\n",
    "\n",
    "Difference between classical and Bayesian regression?\n",
    "Classical: single weights, Bayesian: distribution over weights.\n",
    "\n",
    "Q: Why use Bayesian regression?\n",
    "Gives uncertainty + reduces overfitting + allows priors.\n",
    "\n",
    "Q: Can it work with multiple features or polynomial regression?\n",
    "Yes, same as linear regression, just the prior and posterior are over multiple weights.\n",
    "\n",
    "Q: What prior is commonly used?\n",
    "Gaussian prior over weights.\n",
    "\n",
    "Classical ‚Üí Best guess\n",
    "Bayesian ‚Üí ‚ÄúBelief + data = updated belief‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782d05e",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1, 3, 2, 3, 5])\n",
    "\n",
    "model = BayesianRidge()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Mean prediction\n",
    "y_pred = model.predict([[6]])\n",
    "# Standard deviation of prediction\n",
    "y_std = model.predict([[6]], return_std=True)\n",
    "\n",
    "print(\"Prediction:\", y_pred)\n",
    "print(\"Uncertainty:\", y_std)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
