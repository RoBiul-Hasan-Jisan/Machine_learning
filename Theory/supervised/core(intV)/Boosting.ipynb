{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea132f4f",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "**XGBoost** stands for *Extreme Gradient Boosting*. It is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library.\n",
    "\n",
    "**Purpose:** Primarily used for **Regression**, **Classification**, and **Ranking** tasks.\n",
    "**Fame:** It dominates Kaggle competitions and industry applications due to its superior execution speed and model performance on structured/tabular data.\n",
    "\n",
    "**Core Concept:**\n",
    "XGBoost builds trees **sequentially**. Each new tree is trained to correct the prediction errors (residuals) made by all the previous trees combined, using a gradient descent approach to minimize the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Mathematical Intuition\n",
    "\n",
    "### A. Prediction (Additive Model)\n",
    "The final prediction $\\hat{y}_i$ for a given instance $i$ is the sum of predictions from $K$ sequential trees ($f_k$):\n",
    "\n",
    "$$\\hat{y}_i = \\sum_{k=1}^{K} f_k(x_i)$$\n",
    "\n",
    "* $f_k$: The function represented by the $k$-th tree.\n",
    "* $K$: Total number of trees.\n",
    "\n",
    "### B. Regularized Objective Function (The XGBoost Genius)\n",
    "Unlike standard Gradient Boosting, XGBoost explicitly includes a regularization term in its objective function to control complexity.\n",
    "\n",
    "$$\\text{Obj}(\\theta) = \\underbrace{\\sum_{i=1}^{n} L(y_i, \\hat{y}_i)}_{\\text{Loss Term}} + \\underbrace{\\sum_{k=1}^{K} \\Omega(f_k)}_{\\text{Regularization Term}}$$\n",
    "\n",
    "1.  **Loss Term ($L$):** Measures how well the model fits the data (e.g., MSE for regression, Log Loss for classification).\n",
    "2.  **Regularization Term ($\\Omega$):** Penalizes complex models to prevent overfitting.\n",
    "    $$\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2$$\n",
    "    * $T$: Number of leaves in the tree.\n",
    "    * $w$: Vector of leaf weights (scores).\n",
    "    * $\\gamma$ (gamma): Minimum loss reduction required to make a split.\n",
    "    * $\\lambda$ (lambda): L2 regularization term on weights.\n",
    "\n",
    "**Note:** XGBoost uses **second-order approximations** (Taylor Expansion using both Gradient and Hessian) of the loss function, making optimization faster and more precise than traditional GBDT (which uses only first-order gradients).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why is XGBoost So Powerful?\n",
    "\n",
    "| Feature | Description |\n",
    "| :--- | :--- |\n",
    "| **Built-in Regularization** | Controls overfitting via L1 ($\\alpha$) and L2 ($\\lambda$) penalties on leaf weights. |\n",
    "| **Sparse Data Handling** | Automatically learns the best \"default direction\" for missing values during training. |\n",
    "| **Parallel Processing** | Builds trees sequentially, but parallelizes the **node splitting** phase (feature sorting). |\n",
    "| **Tree Pruning** | Uses \"max_depth\" parameter and prunes trees backwards using the $\\gamma$ threshold. |\n",
    "| **Hardware Optimization** | Out-of-core computing for datasets larger than RAM; Cache-aware access. |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Hyperparameters\n",
    "\n",
    "### Boosting Process\n",
    "* **`n_estimators`**: Number of boosting rounds (trees). Increasing this improves fit but increases overfitting risk.\n",
    "\n",
    "### Tree Structure\n",
    "* **`learning_rate` ($\\eta$):** Shrinks the contribution of each new tree. Low values ($0.01 - 0.1$) with high `n_estimators` usually yield the best results.\n",
    "* **`max_depth`**: Maximum depth of a tree. Controls model complexity (Typical: $3-10$).\n",
    "* **`min_child_weight`**: Minimum sum of instance weight (Hessian) needed in a child node. Higher values $\\rightarrow$ More conservative (prevents overfitting).\n",
    "* **`gamma` ($\\gamma$):** Minimum loss reduction required to make a further partition. Acts as a pseudo-regularizer.\n",
    "\n",
    "### Randomization (Stochastic Boosting)\n",
    "* **`subsample`**: Fraction of training rows sampled for each tree. (Typical: $0.5 - 0.9$).\n",
    "* **`colsample_bytree`**: Fraction of columns (features) sampled for each tree. Reduces correlation between trees.\n",
    "\n",
    "### Regularization\n",
    "* **`reg_alpha` ($\\alpha$):** L1 regularization term on weights. Good for high dimensionality (feature selection).\n",
    "* **`reg_lambda` ($\\lambda$):** L2 regularization term on weights. (Default: 1). Makes predictions smoother.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. XGBoost Workflow (Step-by-Step)\n",
    "\n",
    "1.  **Initialize:** Start with a simple prediction (e.g., the mean of the target $y$).\n",
    "2.  **Iterate ($m = 1$ to $M$):**\n",
    "    * **a.** Compute the **Gradient** ($g_i$) and **Hessian** ($h_i$) for all data points based on the current error.\n",
    "    * **b.** Fit a new Decision Tree to these gradients/hessians.\n",
    "    * **c.** Calculate leaf scores (weights) using the regularization formula:\n",
    "        $$w^* = -\\frac{\\sum g_i}{\\sum h_i + \\lambda}$$\n",
    "    * **d.** Update the model:\n",
    "        $$\\hat{y}_{new} = \\hat{y}_{old} + \\eta \\cdot \\text{Tree}_{prediction}$$\n",
    "3.  **Output:** Final sum of all weighted tree predictions.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. When to Use XGBoost?\n",
    "\n",
    " **Use When:**\n",
    "* You have **Structured / Tabular data** (Excel-like data).\n",
    "* **Predictive Accuracy** is the #1 priority (Kaggle style).\n",
    "* The relationship between features and target is complex and non-linear.\n",
    "* You have missing values (XGBoost handles them natively).\n",
    "\n",
    " **Avoid When:**\n",
    "* **Unstructured data:** Images (use CNNs) or Text (use Transformers/LLMs).\n",
    "* **Interpretability:** You need a strictly explainable formula (use Linear/Logistic Regression).\n",
    "* **Tiny Datasets:** Might be overkill and prone to overfitting; standard Random Forest or Linear Models might suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f869c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize and train\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3a2f6",
   "metadata": {},
   "source": [
    "**Q: Why is XGBoost often better than standard Gradient Boosting Machines (GBM)?**\n",
    "\n",
    "A: Regularization, efficient handling of missing data, use of second-order derivatives (Hessian) for faster convergence, and advanced tree pruning.\n",
    "\n",
    "**Q: How does XGBoost handle missing values?**\n",
    "\n",
    "A: During training, it learns the default direction (left or right child) for missing values at each split that minimizes loss.\n",
    "\n",
    "**Q: How can you prevent overfitting in XGBoost?**\n",
    "\n",
    "A: Use a combination of:\n",
    "1) Lower max_depth,\n",
    "2) Increase min_child_weight and gamma\n",
    "3) Use subsample and colsample_bytree\n",
    "4) Apply stronger L1/L2 regularization (alpha, lambda)\n",
    "5) Reduce learning_rate while increasing n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "# For binary classification, objective='binary:logistic' (default)\n",
    "# For multi-class, set objective='multi:softmax' and num_class\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # For multi-class classification\n",
    "    num_class=3,                 # Number of classes in the Iris dataset\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)  # Predicts class labels\n",
    "# y_pred_proba = model.predict_proba(X_test)  # Predicts class probabilities\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61516cdb",
   "metadata": {},
   "source": [
    "**Important Parameters for Classification**\n",
    "\n",
    "While you already know many parameters (like max_depth, eta) from the regressor, a few are particularly important for classification:\n",
    "\n",
    "**scale_pos_weight :** Crucial for imbalanced datasets. A common value is (number of negative class samples) / (number of positive class samples). This tells the model to pay more attention to the minority class.\n",
    "\n",
    "**eval_metric:** While training, it's helpful to monitor metrics like 'logloss', 'error' (classification error), or 'auc' (for binary classification).\n",
    "\n",
    "**max_delta_step:** Can sometimes help stabilize training in logistic regression for extremely imbalanced classes.\n",
    "\n",
    "**Q: How does XGBoost handle a multi-class classification problem?**\n",
    "\n",
    "A: It uses a one-vs-all (OvA) strategy internally. When you set objective='multi:softmax', it essentially trains multiple binary classifiers (one for each class) and selects the class with the highest probability.\n",
    "\n",
    "**Q: When would you choose XGBoost Classifier over a Random Forest?**\n",
    "\n",
    "A: When your dataset is large, you need the highest possible accuracy and have the time/resources for careful tuning. Random Forest is excellent and more robust to overfitting with less tuning, but XGBoost's gradient boosting often achieves a slightly higher performance ceiling at the cost of complexity.\n",
    "\n",
    "**Q: The classifier is overfitting. Which parameters would you adjust first?**\n",
    "\n",
    "A: \n",
    "1) Increase reg_alpha (L1) and reg_lambda (L2) for stronger regularization. \n",
    "2) Reduce max_depth to make trees simpler. \n",
    "3) Lower learning_rate and increase n_estimators.\n",
    "4) Use subsample and colsample_bytree to introduce more randomness.\n",
    "\n",
    "### XGBoost: Regressor vs. Classifier\n",
    "\n",
    "| Aspect | XGBoost Regressor | XGBoost Classifier |\n",
    "| :--- | :--- | :--- |\n",
    "| **Primary Task** | Predicts continuous numeric values. | Predicts discrete class labels. |\n",
    "| **Core Objective** | Minimizes residuals (e.g., Squared Error). | Maximizes class probability (Log Loss). |\n",
    "| **Default Objective**| `reg:squarederror` | `binary:logistic` or `multi:softprob`. |\n",
    "| **Output Type** | Real numbers ($y \\in \\mathbb{R}$). | Probabilities or Class labels. |\n",
    "| **Unique Params** | Standard tuning. | `num_class` (required for multi-class). |\n",
    "| **Metrics** | RMSE, MAE, $R^2$. | Accuracy, F1, Log Loss, AUC. |\n",
    "\n",
    "\n",
    "**XGBoost = Boosted Decision Trees → sequentially reduce errors.**\n",
    "\n",
    "**Regressor → continuous predictions, Classifier → class labels.**\n",
    "\n",
    "**Regularization + tree parameters = key to performance.**\n",
    "\n",
    "**Handles missing/sparse data, fast, accurate, widely used in ML competition**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef319471",
   "metadata": {},
   "source": [
    "# LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "**LightGBM** is a gradient boosting framework developed by **Microsoft**. It is designed to be distributed and efficient with the following advantages:\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data.\n",
    "\n",
    "**Core Philosophy:**\n",
    "While XGBoost focuses on exactness and regularization, LightGBM focuses on **Speed** and **Scalability** by approximating the split-finding process using histograms.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Key Innovations (The \"Secret Sauce\")\n",
    "\n",
    "LightGBM introduces architectural changes that make it significantly different from traditional boosting algorithms.\n",
    "\n",
    "### A. Leaf-wise vs. Level-wise Growth (Crucial Difference)\n",
    "Most boosting algorithms (like XGBoost) grow trees **Level-wise** (horizontally). They maintain a balanced tree.\n",
    "LightGBM grows trees **Leaf-wise** (vertically/asymmetrically).\n",
    "\n",
    "* **Leaf-wise (LightGBM):** It chooses the leaf with the **max delta loss** to grow. It expands the tree deeper in promising areas rather than wasting time on non-informative branches.\n",
    "    * *Pros:* Lower loss, better accuracy on complex patterns.\n",
    "    * *Cons:* Can grow very deep and overfit on small datasets. (Must control with `max_depth`).\n",
    "\n",
    "\n",
    "\n",
    "### B. Histogram-based Learning\n",
    "Instead of sorting all data points for every feature to find the best split (which is slow $O(N \\log N)$), LightGBM buckets continuous feature values into discrete **bins** (histograms).\n",
    "\n",
    "* **Efficiency:** Reduces calculation complexity from $O(\\text{data} \\times \\text{features})$ to $O(\\text{data} \\times \\text{bins})$.\n",
    "* **Memory:** Significantly reduces memory usage because it stores discrete bins (integers) instead of raw floats.\n",
    "\n",
    "### C. GOSS (Gradient-based One-Side Sampling)\n",
    "This deals with the **number of data samples**.\n",
    "* **Logic:** Data points with large gradients (large errors) are \"hard\" to learn. Points with small gradients are \"easy\" (already well-learned).\n",
    "* **The Trick:** GOSS keeps all instances with **large gradients** and performs random sampling on instances with **small gradients**.\n",
    "* **Result:** Focuses computation on the under-trained data without changing the data distribution.\n",
    "\n",
    "### D. EFB (Exclusive Feature Bundling)\n",
    "This deals with the **number of features**.\n",
    "* **Logic:** In high-dimensional sparse data (like One-Hot encoded), many features are mutually exclusive (they are never non-zero at the same time).\n",
    "* **The Trick:** EFB bundles these features into a single feature.\n",
    "* **Result:** Reduces dimensionality without losing information.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Comparison: LightGBM vs. XGBoost\n",
    "\n",
    "| Feature | **XGBoost** | **LightGBM** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Tree Growth** | **Level-wise** (Horizontal/Balanced) | **Leaf-wise** (Vertical/Asymmetrical) |\n",
    "| **Split Finding** | Pre-sorted (Exact) or Histogram (Approx) | **Histogram-based** (Fast & Low Memory) |\n",
    "| **Categorical Data** | Requires One-Hot/Label Encoding | **Native Support** (Auto-handles categories) |\n",
    "| **Missing Values** | Auto-learned direction | Auto-learned direction |\n",
    "| **Memory Usage** | Higher | **Very Low** |\n",
    "| **Speed** | Fast | **Very Fast** (Often 2-10x XGBoost) |\n",
    "| **Best For** | Accuracy on medium data | Large datasets, High-dimensional data |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Handling Categorical Features\n",
    "One of LightGBM's biggest advantages is **Native Categorical Support**.\n",
    "* You do **not** need to One-Hot Encode your data.\n",
    "* You simply define the column as `categorical` type.\n",
    "* LightGBM partitions categorical features by using a \"Many-vs-Many\" split strategy, which is often more accurate than One-Hot encoding for high-cardinality features.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Hyperparameters\n",
    "\n",
    "### Control Overfitting\n",
    "Because Leaf-wise growth is aggressive, these parameters are vital:\n",
    "\n",
    "1.  **`num_leaves`**: The main parameter to control complexity. Theoretical limit is $2^{\\text{max\\_depth}}$, but usually set smaller to prevent overfitting.\n",
    "2.  **`max_depth`**: Explicitly limits how deep the tree can grow.\n",
    "3.  **`min_data_in_leaf`**: Minimum samples required in a leaf. Setting this high prevents the tree from picking up noise.\n",
    "\n",
    "### Tuning Speed\n",
    "1.  **`feature_fraction`**: Randomly select a subset of features for each iteration (like `colsample_bytree` in XGB).\n",
    "2.  **`bagging_fraction`**: Randomly select a subset of data (like `subsample` in XGB).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary: When to Use LightGBM?\n",
    "\n",
    "**Use When:**\n",
    "* You have **Huge Datasets** ($100k+$ rows) and speed is a concern.\n",
    "* You have **High-dimensional** sparse data.\n",
    "* You have many **Categorical Features** and don't want to deal with encoding.\n",
    "* You have limited RAM/Memory.\n",
    "\n",
    " **Be Careful When:**\n",
    "* **Small Datasets:** Leaf-wise growth can overfit easily. If used, limit `max_depth`.\n",
    "* **Noise:** It is sensitive to noise in the data due to its aggressive splitting.\n",
    "\n",
    "| Parameter          | Type   | Description                             |\n",
    "| ------------------ | ------ | --------------------------------------- |\n",
    "| `num_leaves`       | int    | Max number of leaves in one tree.       |\n",
    "| `max_depth`        | int    | Max depth of each tree.                 |\n",
    "| `learning_rate`    | float  | Shrinks weight of new trees.            |\n",
    "| `n_estimators`     | int    | Number of boosting rounds.              |\n",
    "| `min_data_in_leaf` | int    | Minimum number of samples per leaf.     |\n",
    "| `feature_fraction` | float  | Fraction of features used per tree.     |\n",
    "| `bagging_fraction` | float  | Fraction of data sampled for each tree. |\n",
    "| `bagging_freq`     | int    | Frequency for bagging (0 = disabled).   |\n",
    "| `lambda_l1`        | float  | L1 regularization.                      |\n",
    "| `lambda_l2`        | float  | L2 regularization.                      |\n",
    "| `objective`        | string | Task type (regression/classification).  |\n",
    "\n",
    "\n",
    "\n",
    "## Common objectives:\n",
    "\n",
    "* **Regression: regression, huber, fair**\n",
    "\n",
    "* **Binary classification: binary**\n",
    "\n",
    "* **Multi-class classification: multiclass**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Feature           | LGBMRegressor                | LGBMClassifier                   |\n",
    "| ----------------- | ---------------------------- | -------------------------------- |\n",
    "| Task              | Regression (predict numbers) | Classification (predict classes) |\n",
    "| Objective         | `regression` (default)       | `binary`, `multiclass`           |\n",
    "| Output            | Continuous values            | Class labels / probabilities     |\n",
    "| Evaluation Metric | RMSE, MAE, R²                | Accuracy, AUC, Log Loss          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cfb7d",
   "metadata": {},
   "source": [
    "# LightGBM Strategy Guide\n",
    "\n",
    "**LightGBM** (Light Gradient Boosting Machine) is the \"Speedster\" of the gradient boosting family. It is optimized for high efficiency, low memory usage, and handling large-scale data.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. When to Choose LightGBM?\n",
    "You should prefer LightGBM over Random Forest or standard XGBoost when:\n",
    "* **Large Datasets:** You have $>10,000$ samples (often millions).\n",
    "* **High Dimensions:** You have many features (wide data).\n",
    "* **Speed is Critical:** You need fast training times (iterating quickly).\n",
    "* **Categorical Features:** Your data contains many categories (it handles them natively without one-hot encoding).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Advantages\n",
    "\n",
    "| Advantage | Powered By... |\n",
    "| :--- | :--- |\n",
    "| **Extreme Speed** | **Histogram Algorithm** (buckets continuous values) + **GOSS** (Gradient-based One-Side Sampling). |\n",
    "| **Low Memory** | **EFB** (Exclusive Feature Bundling) + Histogram binning (uses integers instead of floats). |\n",
    "| **High Accuracy** | **Leaf-wise Growth** (Can model complex, non-linear patterns better than level-wise). |\n",
    "| **Convenience** | Native handling of **Categorical Features** and **Missing Values**. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Deep Dive: Leaf-wise Growth\n",
    "\n",
    "LightGBM uses a different tree-growing strategy than XGBoost.\n",
    "\n",
    "* **XGBoost (Level-wise):** Grows the tree horizontally. It splits all nodes at the same depth. It is balanced and \"safe\" but slower.\n",
    "* **LightGBM (Leaf-wise):** It picks the **single leaf** with the highest loss reduction (error) and splits it. It creates deeper, asymmetrical trees.\n",
    "    * *Benefit:* More efficient; focuses on the \"hard\" parts of the data.\n",
    "    * *Risk:* Can overfit easily on small datasets if not controlled.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Hyperparameter Tuning Priority\n",
    "Tuning LightGBM requires a specific order to get the best results without over-complicating.\n",
    "\n",
    "**Priority 1: The Core (Structure)**\n",
    "* **`num_leaves`**: The most important parameter. Controls complexity. (Theoretical max $\\approx 2^{max\\_depth}$).\n",
    "* **`max_depth`**: Limit this to prevent the tree from growing too deep and overfitting.\n",
    "\n",
    "**Priority 2: The Learning (Optimization)**\n",
    "* **`learning_rate`** & **`n_estimators`**: Lower learning rate + Higher estimators usually = Better accuracy (but slower). Use **Early Stopping**.\n",
    "\n",
    "**Priority 3: Regularization (Prevent Overfitting)**\n",
    "* **`min_data_in_leaf`**: Very important for leaf-wise growth. Prevents the model from isolating noise in a leaf.\n",
    "* **`lambda_l1` / `lambda_l2`**: Standard regularization.\n",
    "\n",
    "**Priority 4: Sampling (Speed & Diversity)**\n",
    "* **`feature_fraction`**: Randomly select subsets of features (like `colsample_bytree`).\n",
    "* **`bagging_fraction`**: Randomly select subsets of data rows.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Missing Value Handling\n",
    "**How it works:**\n",
    "LightGBM does **not** need imputation (filling with mean/median).\n",
    "* During training, it learns the \"best direction\" (left or right) to send missing values for every single split.\n",
    "* It calculates which direction reduces the loss the most and assigns `NaN` to that path.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "> **LightGBM** = Fast Gradient Boosting using **leaf-wise trees**.\n",
    "> * **Regressor** $\\rightarrow$ Predicts numeric values.\n",
    "> * **Classifier** $\\rightarrow$ Predicts classes.\n",
    "> * **Winning Edge:** Handles large data, categories, and missing values efficiently.\n",
    "> * **Watch Out:** Always tune `num_leaves` and `max_depth` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,           # Large number with early stopping\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,              # Bagging fraction\n",
    "    colsample_bytree=0.8,       # Feature fraction\n",
    "    reg_alpha=0.1,              # L1\n",
    "    reg_lambda=0.1,             # L2\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    importance_type='gain'\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {model.best_iteration_}\")\n",
    "print(f\"Best score: {model.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75fc27",
   "metadata": {},
   "source": [
    "# CatBoost (Categorical Boosting)\n",
    "\n",
    "**CatBoost** (developed by **Yandex**) stands for **\"Category Boosting\"**.\n",
    "It is a high-performance open-source library for gradient boosting on decision trees.\n",
    "\n",
    "**The \"Killer Feature\":**\n",
    "It handles **Categorical Features** automatically and natively. You do not need to preprocess your data with One-Hot Encoding or Label Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Key Innovations\n",
    "\n",
    "#### A. Native Categorical Handling (Target Statistics)\n",
    "Most algorithms require you to convert text categories into numbers before training.\n",
    "* **Standard approach:** One-Hot Encoding (Explodes dimensionality) or Label Encoding (Imposes false order).\n",
    "* **CatBoost approach:** It converts categories into numbers using **Target Statistics** (the average value of the target for that category).\n",
    "    * *Twist:* To prevent overfitting (Target Leakage), it uses **Ordered Target Statistics**. It effectively shuffles the data and calculates the average target for a category based only on the rows *before* the current one in that random permutation.\n",
    "\n",
    "#### B. Ordered Boosting (Solving Prediction Shift)\n",
    "Standard Gradient Boosting suffers from **Prediction Shift** (a type of target leakage). The model calculates residuals using the same data points it trains on, leading to biased gradients.\n",
    "* **CatBoost Solution:** It uses a permutation-driven approach. It maintains multiple random permutations of the dataset to calculate residuals for a data point using a model trained *only* on other data points.\n",
    "* **Result:** Reduces overfitting significantly, especially on **Small Datasets**.\n",
    "\n",
    "#### C. Symmetric Trees (Oblivious Trees)\n",
    "XGBoost and LightGBM build flexible trees (Level-wise or Leaf-wise).\n",
    "CatBoost builds **Symmetric (Oblivious) Trees**.\n",
    "* **Concept:** In a symmetric tree, the same split condition is applied to **all nodes** at the same depth.\n",
    "* **Example:** If Depth 1 splits on \"Age > 30\", *every* node at that level splits on \"Age > 30\".\n",
    "* **Benefits:**\n",
    "    1.  **Extremely Fast Prediction:** The structure is simple and fits perfectly into CPU caches.\n",
    "    2.  **Less Overfitting:** The structure is constrained and regularized.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Comparison: The \"Big Three\"\n",
    "\n",
    "| Feature | **XGBoost** | **LightGBM** | **CatBoost** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Categorical Data** | Needs One-Hot/Label | Native (Good) | **Native (Best)** |\n",
    "| **Tree Structure** | Level-wise | Leaf-wise (Asymmetric) | **Symmetric (Balanced)** |\n",
    "| **Overfitting** | Good regularization | Prone on small data | **Very Robust** (Ordered Boosting) |\n",
    "| **Speed** | Fast | **Fastest Training** | **Fastest Prediction** |\n",
    "| **Tuning** | Needs Tuning | Needs Tuning | **Great Defaults** (\"Set & Forget\") |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Key Features\n",
    "\n",
    "1.  **Robust to Overfitting:** Due to Ordered Boosting, it works exceptionally well on small datasets where other boosting models might memorize the noise.\n",
    "2.  **Missing Values:** Like XGBoost/LightGBM, it supports missing values (\"NaN\") automatically.\n",
    "3.  **GPU Support:** Efficient GPU implementation for faster training.\n",
    "4.  **Feature Importance:** Provides built-in methods (`model.get_feature_importance()`) to understand which features drive predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How to Use It? (Workflow)\n",
    "\n",
    "CatBoost is famous for providing great results with default hyperparameters.\n",
    "\n",
    "**Python Example:**\n",
    "```python\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define categorical features indices\n",
    "cat_features = [0, 2, 5] \n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    cat_features=cat_features, # Pass indices directly!\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "| Parameter             | Type   | Description                               |\n",
    "| --------------------- | ------ | ----------------------------------------- |\n",
    "| `iterations`          | int    | Number of trees / boosting rounds.        |\n",
    "| `learning_rate`       | float  | Shrinks weight of new trees.              |\n",
    "| `depth`               | int    | Depth of each tree.                       |\n",
    "| `l2_leaf_reg`         | float  | L2 regularization coefficient.            |\n",
    "| `border_count`        | int    | Number of splits for numeric features.    |\n",
    "| `bagging_temperature` | float  | Controls randomness in selecting samples. |\n",
    "| `random_seed`         | int    | Seed for reproducibility.                 |\n",
    "| `task_type`           | string | `'CPU'` or `'GPU'`.                       |\n",
    "| `loss_function`       | string | Objective function for task.              |\n",
    "| `eval_metric`         | string | Metric for validation.                    |\n",
    "\n",
    "\n",
    "## Common loss_function values:\n",
    "\n",
    "* **Regression: RMSE, MAE, Quantile**\n",
    "\n",
    "* **Binary classification: Logloss, CrossEntropy**\n",
    "\n",
    "* **Multi-class classification: MultiClass, MultiClassOneVsAll**\n",
    "\n",
    "\n",
    "\n",
    "| Feature              | CatBoostRegressor            | CatBoostClassifier               |\n",
    "| -------------------- | ---------------------------- | -------------------------------- |\n",
    "| Task                 | Regression (predict numbers) | Classification (predict classes) |\n",
    "| Loss function        | RMSE, MAE                    | Logloss, CrossEntropy            |\n",
    "| Output               | Continuous values            | Class labels / probabilities     |\n",
    "| Categorical Features | Supported automatically      | Supported automatically          |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=0)\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0)\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff147d",
   "metadata": {},
   "source": [
    "**Mention categorical feature handling without one-hot encoding.**\n",
    "\n",
    "**Ordered boosting → reduces overfitting on small datasets.**\n",
    "\n",
    "**Symmetric trees → faster prediction.**\n",
    "\n",
    "**Hyperparameter tuning: iterations, learning_rate, depth, l2_leaf_reg.**\n",
    "\n",
    "\n",
    "\n",
    "CatBoost = Gradient Boosting with native categorical handling & ordered boosting.\n",
    "\n",
    "Regressor → numeric output, Classifier → class labels.\n",
    "\n",
    "Reduces overfitting → good for small & medium datasets.\n",
    "\n",
    "Efficient, accurate, GPU-compatibl\n",
    "\n",
    "\n",
    "\n",
    "| Feature                           | **XGBoost**                                                                                                                                                         | **LightGBM**                                                                                                                                                                                            | **CatBoost**                                                                                                                                                                        |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**                    | eXtreme Gradient Boosting; gradient boosting using decision trees with regularization                                                                               | Light Gradient Boosting Machine; faster gradient boosting using **leaf-wise growth**                                                                                                                    | Categorical Boosting; gradient boosting with **native categorical handling** and **ordered boosting**                                                                               |\n",
    "| **Key Idea**                      | Sequentially builds trees to reduce residual error                                                                                                                  | Leaf-wise tree growth → splits largest loss leaf → faster & more accurate                                                                                                                               | Ordered boosting → reduces overfitting, handles categorical features automatically                                                                                                  |\n",
    "| **Why Use / Advantages**          | - Accurate, robust<br>- Handles missing values<br>- Regularization to reduce overfitting<br>- Widely used in competitions                                           | - Very fast & memory efficient<br>- Handles large datasets<br>- Native categorical support<br>- High accuracy due to leaf-wise trees                                                                    | - Handles categorical features **without encoding**<br>- Reduces overfitting on small datasets<br>- Symmetric trees → faster prediction<br>- GPU support                            |\n",
    "| **Why Not / Disadvantages**       | - Slower than LightGBM on large datasets<br>- More memory usage<br>- Sensitive to hyperparameters                                                                   | - Can overfit on small datasets (leaf-wise growth)<br>- Slightly complex hyperparameter tuning                                                                                                          | - Slower than LightGBM on very large datasets<br>- Slightly higher memory usage<br>- Less flexible for some advanced tasks                                                          |\n",
    "| **When to Use**                   | - Small/medium datasets<br>- Need highly accurate model<br>- Want **regularization control**                                                                        | - Very large datasets<br>- Need **fast training** & prediction<br>- Want high accuracy and memory efficiency                                                                                            | - Datasets with **categorical features**<br>- Small to medium datasets<br>- Want low overfitting on small samples                                                                   |\n",
    "| **When Not to Use**               | - Extremely large datasets where speed is critical                                                                                                                  | - Small datasets prone to overfitting                                                                                                                                                                   | - Extremely large datasets where memory & speed matter more than categorical handling                                                                                               |\n",
    "| **Key Hyperparameters**           | - `n_estimators` (trees)<br>- `learning_rate`<br>- `max_depth`<br>- `min_child_weight`<br>- `subsample`, `colsample_bytree`<br>- `gamma`, `reg_alpha`, `reg_lambda` | - `num_leaves` (leaf nodes)<br>- `max_depth`<br>- `learning_rate`<br>- `n_estimators`<br>- `min_data_in_leaf`<br>- `feature_fraction`, `bagging_fraction`, `bagging_freq`<br>- `lambda_l1`, `lambda_l2` | - `iterations`<br>- `depth`<br>- `learning_rate`<br>- `l2_leaf_reg`<br>- `border_count` (numeric splits)<br>- `bagging_temperature`<br>- `loss_function`<br>- `task_type` (CPU/GPU) |\n",
    "| **Handling Categorical Features** |  Must encode manually (one-hot, label encoding)                                                                                                                    |  Partial support (can encode manually or use categorical indices)                                                                                                                                      |  Fully automatic, no preprocessing needed                                                                                                                                          |\n",
    "| **Training Speed**                | Moderate                                                                                                                                                            | Very fast                                                                                                                                                                                               | Moderate                                                                                                                                                                            |\n",
    "| **Prediction Speed**              | Fast                                                                                                                                                                | Fastest                                                                                                                                                                                                 | Fast                                                                                                                                                                                |\n",
    "| **Memory Usage**                  | High                                                                                                                                                                | Low                                                                                                                                                                                                     | Moderate                                                                                                                                                                            |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
