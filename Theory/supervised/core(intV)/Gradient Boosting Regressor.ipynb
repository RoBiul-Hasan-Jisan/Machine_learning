{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f808c8db",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea132f4f",
   "metadata": {},
   "source": [
    "XGBoost stands for Extreme Gradient Boosting. It's a powerful and scalable supervised machine learning algorithm based on the gradient boosting framework that uses decision trees as base learners.\n",
    "\n",
    "Purpose: Primarily used for regression and classification tasks.\n",
    "\n",
    "Fame: Dominates Kaggle competitions and is widely used in industry for its performance on structured/tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57be5f8",
   "metadata": {},
   "source": [
    "XGBoost builds trees sequentially, where each new tree is trained to correct the prediction errors (residuals) made by all the previous trees combined, using a gradient descent approach to minimize a defined loss function.\n",
    "\n",
    "Mathematical Intuition \n",
    "1. Prediction (Additive Model):\n",
    "ŷ_i = Σ (from k=1 to K) f_k(x_i)\n",
    "\n",
    "The final prediction ŷ_i for instance i is the sum of predictions from K sequential trees (f_k).\n",
    "\n",
    "2. Regularized Objective Function (The XGBoost Genius):\n",
    "Obj(θ) = Σ L(y_i, ŷ_i) + Σ Ω(f_k)\n",
    "\n",
    "Loss Term (L): Measures how well the model fits the data ( squared error).\n",
    "\n",
    "Regularization Term (Ω): Penalizes model complexity to prevent overfitting. A typical form is: Ω(f) = γT + (1/2)λ||w||²\n",
    "\n",
    "T: Number of leaves in the tree.\n",
    "\n",
    "w: Vector of leaf weights (predictions).\n",
    "\n",
    "γ (gamma) and λ (lambda): Hyperparameters controlling the penalty.\n",
    "\n",
    "XGBoost uses second-order approximations (both the gradient and the Hessian) of the loss function, making its optimization faster and more accurate than traditional gradient boosting.\n",
    "\n",
    "Why XGBoost is So Powerful\n",
    "\n",
    "Built-in Regularization  : Directly controls overfitting via L1/L2 penalties on leaf weights and tree structure\n",
    "\n",
    "Handles Sparse/Missing : Data\tAutomatically learns the best direction to handle missing values during tree construction.\n",
    "\n",
    "Computational Speed & Scalability :\tParallel processing, out-of-core computation, and cache-aware block structure for large datasets.\n",
    "\n",
    "High Predictive Accuracy :\tConsistently delivers state-of-the-art results on diverse problems.\n",
    "\n",
    "\n",
    "Boosting Process\n",
    "\n",
    "-> n_estimators\t ->Number of boosting rounds/trees.-> \tIncrease for better fit, but risk overfitting.\n",
    "\n",
    "\n",
    "Tree Structure\n",
    "\n",
    "-> learning_rate (eta) ->\tShrinks contribution of each tree.\t->  Low value ( 0.01-0.3) with high n_estimators often works best.\n",
    "\n",
    "-> max_depth ->\tMaximum depth of a tree.-> \tControls complexity; typical 3-10.\n",
    "\n",
    "-> min_child_weight ->\tMinimum sum of instance weight needed in a child node.\t->  Higher values prevent overfitting.\n",
    "\n",
    "-> gamma ->\tMinimum loss reduction required to make a further partition on a leaf node. -> \tActs as a complexity check.\n",
    "\n",
    "Randomization\n",
    "\n",
    "->subsample\t->  Fraction of training data sampled for each tree.\t-> Introduces randomness (like in Random Forest).\n",
    "\n",
    "->colsample_bytree -> \tFraction of features sampled for each tree.\t->  Reduces overfitting and correlation between trees.\n",
    "\n",
    "Regularization\n",
    "->reg_alpha (alpha)  ->\tL1 regularization on leaf weights.\t  ->Adds feature selection sparsity.\n",
    "\n",
    "->reg_lambda (lambda)\t ->L2 regularization on leaf weights.\t -> More common; makes predictions smoother.\n",
    "\n",
    "\n",
    "XGBoost Regressor Workflow \n",
    "\n",
    "Initialize with a simple prediction (often the mean of the target variable).\n",
    "\n",
    "For m = 1 to M (number of trees):\n",
    "a. Compute the residuals (negative gradient) for all data points.\n",
    "b. Fit a new weak learner (decision tree) to predict these residuals.\n",
    "c. Update the model by adding the new tree's predictions (shrunken by the learning_rate).\n",
    "\n",
    "Output the final model as the sum of all tree predictions.\n",
    "\n",
    "When to Use XGBoost Regressor?\n",
    "You have structured/tabular data (not images/text).\n",
    "\n",
    "Predictive accuracy is the primary goal over model interpretability.\n",
    "\n",
    "You have sufficient computational resources for training and hyperparameter tuning.\n",
    "\n",
    "The relationships in your data are complex and non-linear.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59693044",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize and train\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(\n\u001b[0;32m      9\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     10\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     11\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     12\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize and train\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3a2f6",
   "metadata": {},
   "source": [
    "Q: Why is XGBoost often better than standard Gradient Boosting Machines (GBM)?\n",
    "\n",
    "A: Regularization, efficient handling of missing data, use of second-order derivatives (Hessian) for faster convergence, and advanced tree pruning.\n",
    "\n",
    "Q: How does XGBoost handle missing values?\n",
    "\n",
    "A: During training, it learns the default direction (left or right child) for missing values at each split that minimizes loss.\n",
    "\n",
    "Q: How can you prevent overfitting in XGBoost?\n",
    "\n",
    "A: Use a combination of: 1) Lower max_depth, 2) Increase min_child_weight and gamma, 3) Use subsample and colsample_bytree, 4) Apply stronger L1/L2 regularization (alpha, lambda), 5) Reduce learning_rate while increasing n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier\n",
    "# For binary classification, objective='binary:logistic' (default)\n",
    "# For multi-class, set objective='multi:softmax' and num_class\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # For multi-class classification\n",
    "    num_class=3,                 # Number of classes in the Iris dataset\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)  # Predicts class labels\n",
    "# y_pred_proba = model.predict_proba(X_test)  # Predicts class probabilities\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61516cdb",
   "metadata": {},
   "source": [
    "mportant Parameters for Classification\n",
    "While you already know many parameters (like max_depth, eta) from the regressor, a few are particularly important for classification:\n",
    "\n",
    "scale_pos_weight : Crucial for imbalanced datasets. A common value is (number of negative class samples) / (number of positive class samples). This tells the model to pay more attention to the minority class.\n",
    "\n",
    "eval_metric: While training, it's helpful to monitor metrics like 'logloss', 'error' (classification error), or 'auc' (for binary classification).\n",
    "\n",
    "max_delta_step: Can sometimes help stabilize training in logistic regression for extremely imbalanced classes.\n",
    "\n",
    "Q: How does XGBoost handle a multi-class classification problem?\n",
    "\n",
    "A: It uses a one-vs-all (OvA) strategy internally. When you set objective='multi:softmax', it essentially trains multiple binary classifiers (one for each class) and selects the class with the highest probability.\n",
    "\n",
    "Q: When would you choose XGBoost Classifier over a Random Forest?\n",
    "\n",
    "A: When your dataset is large, you need the highest possible accuracy and have the time/resources for careful tuning. Random Forest is excellent and more robust to overfitting with less tuning, but XGBoost's gradient boosting often achieves a slightly higher performance ceiling at the cost of complexity.\n",
    "\n",
    "Q: The classifier is overfitting. Which parameters would you adjust first?\n",
    "\n",
    "A: 1) Increase reg_alpha (L1) and reg_lambda (L2) for stronger regularization. 2) Reduce max_depth to make trees simpler. 3) Lower learning_rate and increase n_estimators. 4) Use subsample and colsample_bytree to introduce more randomness.\n",
    "\n",
    "### XGBoost: Regressor vs. Classifier\n",
    "\n",
    "| Aspect | XGBoost Regressor | XGBoost Classifier |\n",
    "| :--- | :--- | :--- |\n",
    "| **Primary Task** | Predicts continuous numeric values. | Predicts discrete class labels. |\n",
    "| **Core Objective** | Minimizes residuals (e.g., Squared Error). | Maximizes class probability (Log Loss). |\n",
    "| **Default Objective**| `reg:squarederror` | `binary:logistic` or `multi:softprob`. |\n",
    "| **Output Type** | Real numbers ($y \\in \\mathbb{R}$). | Probabilities or Class labels. |\n",
    "| **Unique Params** | Standard tuning. | `num_class` (required for multi-class). |\n",
    "| **Metrics** | RMSE, MAE, $R^2$. | Accuracy, F1, Log Loss, AUC. |\n",
    "\n",
    "\n",
    "### XGBoost = Boosted Decision Trees → sequentially reduce errors.\n",
    "\n",
    "### Regressor → continuous predictions, Classifier → class labels.\n",
    "\n",
    "### Regularization + tree parameters = key to performance.\n",
    "\n",
    "### Handles missing/sparse data, fast, accurate, widely used in ML competition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e1b96",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef319471",
   "metadata": {},
   "source": [
    "LightGBM = Gradient Boosting framework from Microsoft.\n",
    "\n",
    "Similar to XGBoost but faster and more memory-efficient.\n",
    "\n",
    "Uses Decision Trees as base learners.\n",
    "\n",
    "Designed for large datasets and high-dimensional data.\n",
    "\n",
    "Works for regression and classification.\n",
    "\n",
    "Key Features\n",
    "\n",
    "Gradient Boosting algorithm with leaf-wise growth (more aggressive than depth-wise growth in XGBoost).\n",
    "\n",
    "Handles categorical features directly (no need for one-hot encoding).\n",
    "\n",
    "Supports missing values automatically.\n",
    "\n",
    "Efficient for sparse data.\n",
    "\n",
    "Faster training than XGBoost for large datasets.\n",
    "\n",
    "Regularization reduces overfitting.\n",
    "\n",
    "Leaf-wise vs Level-wise Growth\n",
    "\n",
    "Leaf-wise (LightGBM): Splits the leaf with the largest loss reduction.\n",
    "\n",
    "More accurate but can overfit on small datasets.\n",
    "\n",
    "Level-wise (XGBoost): Splits all nodes at the same depth.\n",
    "\n",
    "Safer for small datasets.\n",
    "\n",
    "Histogram-based Learning\n",
    "Buckets continuous features into discrete bins\n",
    "\n",
    "Uses histogram-based gradient calculation\n",
    "\n",
    "Faster than XGBoost's pre-sorted algorithm\n",
    "\n",
    "Reduces memory usage significantly\n",
    "\n",
    "Leaf-wise Tree Growth\n",
    "Traditional (Level-wise): Expands all leaves at same depth (balanced but less accurate)\n",
    "\n",
    "Leaf-wise (LightGBM): Expands leaf with max delta loss\n",
    "\n",
    "Better accuracy (can capture more complex patterns)\n",
    "\n",
    "Risk of overfitting on small datasets\n",
    "\n",
    "Add max_depth constraint for control\n",
    "\n",
    "Gradient-based One-Side Sampling (GOSS)\n",
    "Keeps all instances with large gradients\n",
    "\n",
    "Randomly samples instances with small gradients\n",
    "\n",
    "Focuses computation where it matters most\n",
    "\n",
    "Maintains accuracy while reducing data size\n",
    "\n",
    "Exclusive Feature Bundling (EFB)\n",
    "Bundles mutually exclusive features (rarely non-zero simultaneously)\n",
    "\n",
    "Reduces effective number of features\n",
    "\n",
    "Crucial for high-dimensional sparse data\n",
    "\n",
    "| Parameter          | Type   | Description                             |\n",
    "| ------------------ | ------ | --------------------------------------- |\n",
    "| `num_leaves`       | int    | Max number of leaves in one tree.       |\n",
    "| `max_depth`        | int    | Max depth of each tree.                 |\n",
    "| `learning_rate`    | float  | Shrinks weight of new trees.            |\n",
    "| `n_estimators`     | int    | Number of boosting rounds.              |\n",
    "| `min_data_in_leaf` | int    | Minimum number of samples per leaf.     |\n",
    "| `feature_fraction` | float  | Fraction of features used per tree.     |\n",
    "| `bagging_fraction` | float  | Fraction of data sampled for each tree. |\n",
    "| `bagging_freq`     | int    | Frequency for bagging (0 = disabled).   |\n",
    "| `lambda_l1`        | float  | L1 regularization.                      |\n",
    "| `lambda_l2`        | float  | L2 regularization.                      |\n",
    "| `objective`        | string | Task type (regression/classification).  |\n",
    "\n",
    "\n",
    "\n",
    "Common objectives:\n",
    "\n",
    "Regression: regression, huber, fair\n",
    "\n",
    "Binary classification: binary\n",
    "\n",
    "Multi-class classification: multiclass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Feature           | LGBMRegressor                | LGBMClassifier                   |\n",
    "| ----------------- | ---------------------------- | -------------------------------- |\n",
    "| Task              | Regression (predict numbers) | Classification (predict classes) |\n",
    "| Objective         | `regression` (default)       | `binary`, `multiclass`           |\n",
    "| Output            | Continuous values            | Class labels / probabilities     |\n",
    "| Evaluation Metric | RMSE, MAE, R²                | Accuracy, AUC, Log Loss          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cfb7d",
   "metadata": {},
   "source": [
    "When to choose LightGBM?\n",
    "\n",
    "Large datasets (>10K samples)\n",
    "\n",
    "High-dimensional data\n",
    "\n",
    "Need fast training\n",
    "\n",
    "Categorical features present\n",
    "\n",
    "Key Advantages\n",
    "\n",
    "Speed: Histogram algorithm + GOSS\n",
    "\n",
    "Memory: EFB + histogram binning\n",
    "\n",
    "Accuracy: Leaf-wise growth\n",
    "\n",
    "Convenience: Handles categorical/missing data\n",
    "\n",
    "Explain Leaf-wise Growth\n",
    "\"LightGBM uses leaf-wise growth which expands the leaf with the highest loss reduction, creating deeper, more complex trees on one side while keeping other parts shallow. This is more efficient than level-wise growth but requires careful regularization.\"\n",
    "\n",
    "Hyperparameter Tuning Priority\n",
    "\n",
    "learning_rate and n_estimators (with early stopping)\n",
    "\n",
    "num_leaves and max_depth\n",
    "\n",
    "min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "\n",
    "Regularization parameters\n",
    "\n",
    "Sampling parameters (feature_fraction, bagging_fraction)\n",
    "\n",
    "Missing Value Handling\n",
    "\"LightGBM learns the best direction to assign missing values during training by evaluating which split direction yields the largest gain. No imputation needed.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LightGBM = Fast Gradient Boosting using leaf-wise trees.\n",
    "\n",
    "Regressor → numeric, Classifier → classes.\n",
    "\n",
    "Handles categorical & missing data efficiently.\n",
    "\n",
    "Hyperparameters like num_leaves and learning_rate are key to performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,           # Large number with early stopping\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,              # Bagging fraction\n",
    "    colsample_bytree=0.8,       # Feature fraction\n",
    "    reg_alpha=0.1,              # L1\n",
    "    reg_lambda=0.1,             # L2\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    importance_type='gain'\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {model.best_iteration_}\")\n",
    "print(f\"Best score: {model.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88946b90",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75fc27",
   "metadata": {},
   "source": [
    "CatBoost = Gradient Boosting algorithm developed by Yandex.\n",
    "\n",
    "Designed to handle categorical features automatically.\n",
    "\n",
    "Uses ordered boosting → reduces overfitting.\n",
    "\n",
    "Can be used for regression and classification.\n",
    "\n",
    "Handles large datasets efficiently.\n",
    "\n",
    "Key Features\n",
    "\n",
    "Categorical Feature Handling: No need for one-hot encoding.\n",
    "\n",
    "Ordered Boosting: Reduces overfitting on small datasets.\n",
    "\n",
    "Supports missing values automatically.\n",
    "\n",
    "Fast training with GPU support.\n",
    "\n",
    "Regularization to avoid overfitting.\n",
    "\n",
    "Supports multiclass classification and regression.\n",
    "\n",
    "Feature importance available (model.get_feature_importance()).\n",
    "\n",
    "How it Works\n",
    "\n",
    "Based on gradient boosting over decision trees.\n",
    "\n",
    "Handles categorical features using “statistics on combinations of features”.\n",
    "\n",
    "Uses symmetric trees → all leaves at same depth → faster predictions.\n",
    "\n",
    "Ordered boosting prevents target leakage during training.\n",
    "\n",
    "\n",
    "| Parameter             | Type   | Description                               |\n",
    "| --------------------- | ------ | ----------------------------------------- |\n",
    "| `iterations`          | int    | Number of trees / boosting rounds.        |\n",
    "| `learning_rate`       | float  | Shrinks weight of new trees.              |\n",
    "| `depth`               | int    | Depth of each tree.                       |\n",
    "| `l2_leaf_reg`         | float  | L2 regularization coefficient.            |\n",
    "| `border_count`        | int    | Number of splits for numeric features.    |\n",
    "| `bagging_temperature` | float  | Controls randomness in selecting samples. |\n",
    "| `random_seed`         | int    | Seed for reproducibility.                 |\n",
    "| `task_type`           | string | `'CPU'` or `'GPU'`.                       |\n",
    "| `loss_function`       | string | Objective function for task.              |\n",
    "| `eval_metric`         | string | Metric for validation.                    |\n",
    "\n",
    "\n",
    "Common loss_function values:\n",
    "\n",
    "Regression: RMSE, MAE, Quantile\n",
    "\n",
    "Binary classification: Logloss, CrossEntropy\n",
    "\n",
    "Multi-class classification: MultiClass, MultiClassOneVsAll\n",
    "\n",
    "\n",
    "\n",
    "| Feature              | CatBoostRegressor            | CatBoostClassifier               |\n",
    "| -------------------- | ---------------------------- | -------------------------------- |\n",
    "| Task                 | Regression (predict numbers) | Classification (predict classes) |\n",
    "| Loss function        | RMSE, MAE                    | Logloss, CrossEntropy            |\n",
    "| Output               | Continuous values            | Class labels / probabilities     |\n",
    "| Categorical Features | Supported automatically      | Supported automatically          |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=0)\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0)\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff147d",
   "metadata": {},
   "source": [
    "Mention categorical feature handling without one-hot encoding.\n",
    "\n",
    "Ordered boosting → reduces overfitting on small datasets.\n",
    "\n",
    "Symmetric trees → faster prediction.\n",
    "\n",
    "Hyperparameter tuning: iterations, learning_rate, depth, l2_leaf_reg.\n",
    "\n",
    "Can use GPU for faster training.\n",
    "\n",
    "Popular in Kaggle competitions for tabular data with categorical features.\n",
    "\n",
    "CatBoost = Gradient Boosting with native categorical handling & ordered boosting.\n",
    "\n",
    "Regressor → numeric output, Classifier → class labels.\n",
    "\n",
    "Reduces overfitting → good for small & medium datasets.\n",
    "\n",
    "Efficient, accurate, GPU-compatibl\n",
    "\n",
    "\n",
    "\n",
    "| Feature                           | **XGBoost**                                                                                                                                                         | **LightGBM**                                                                                                                                                                                            | **CatBoost**                                                                                                                                                                        |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**                    | eXtreme Gradient Boosting; gradient boosting using decision trees with regularization                                                                               | Light Gradient Boosting Machine; faster gradient boosting using **leaf-wise growth**                                                                                                                    | Categorical Boosting; gradient boosting with **native categorical handling** and **ordered boosting**                                                                               |\n",
    "| **Key Idea**                      | Sequentially builds trees to reduce residual error                                                                                                                  | Leaf-wise tree growth → splits largest loss leaf → faster & more accurate                                                                                                                               | Ordered boosting → reduces overfitting, handles categorical features automatically                                                                                                  |\n",
    "| **Why Use / Advantages**          | - Accurate, robust<br>- Handles missing values<br>- Regularization to reduce overfitting<br>- Widely used in competitions                                           | - Very fast & memory efficient<br>- Handles large datasets<br>- Native categorical support<br>- High accuracy due to leaf-wise trees                                                                    | - Handles categorical features **without encoding**<br>- Reduces overfitting on small datasets<br>- Symmetric trees → faster prediction<br>- GPU support                            |\n",
    "| **Why Not / Disadvantages**       | - Slower than LightGBM on large datasets<br>- More memory usage<br>- Sensitive to hyperparameters                                                                   | - Can overfit on small datasets (leaf-wise growth)<br>- Slightly complex hyperparameter tuning                                                                                                          | - Slower than LightGBM on very large datasets<br>- Slightly higher memory usage<br>- Less flexible for some advanced tasks                                                          |\n",
    "| **When to Use**                   | - Small/medium datasets<br>- Need highly accurate model<br>- Want **regularization control**                                                                        | - Very large datasets<br>- Need **fast training** & prediction<br>- Want high accuracy and memory efficiency                                                                                            | - Datasets with **categorical features**<br>- Small to medium datasets<br>- Want low overfitting on small samples                                                                   |\n",
    "| **When Not to Use**               | - Extremely large datasets where speed is critical                                                                                                                  | - Small datasets prone to overfitting                                                                                                                                                                   | - Extremely large datasets where memory & speed matter more than categorical handling                                                                                               |\n",
    "| **Key Hyperparameters**           | - `n_estimators` (trees)<br>- `learning_rate`<br>- `max_depth`<br>- `min_child_weight`<br>- `subsample`, `colsample_bytree`<br>- `gamma`, `reg_alpha`, `reg_lambda` | - `num_leaves` (leaf nodes)<br>- `max_depth`<br>- `learning_rate`<br>- `n_estimators`<br>- `min_data_in_leaf`<br>- `feature_fraction`, `bagging_fraction`, `bagging_freq`<br>- `lambda_l1`, `lambda_l2` | - `iterations`<br>- `depth`<br>- `learning_rate`<br>- `l2_leaf_reg`<br>- `border_count` (numeric splits)<br>- `bagging_temperature`<br>- `loss_function`<br>- `task_type` (CPU/GPU) |\n",
    "| **Handling Categorical Features** |  Must encode manually (one-hot, label encoding)                                                                                                                    |  Partial support (can encode manually or use categorical indices)                                                                                                                                      |  Fully automatic, no preprocessing needed                                                                                                                                          |\n",
    "| **Training Speed**                | Moderate                                                                                                                                                            | Very fast                                                                                                                                                                                               | Moderate                                                                                                                                                                            |\n",
    "| **Prediction Speed**              | Fast                                                                                                                                                                | Fastest                                                                                                                                                                                                 | Fast                                                                                                                                                                                |\n",
    "| **Memory Usage**                  | High                                                                                                                                                                | Low                                                                                                                                                                                                     | Moderate                                                                                                                                                                            |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
