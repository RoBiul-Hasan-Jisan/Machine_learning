{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b072a3a",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is one of the simplest and most intuitive supervised learning algorithms. It is often called a **\"Lazy Learner\"** because it doesn't learn a discriminative function from the training data but memorizes the training dataset instead.\n",
    "\n",
    "**Core Philosophy:** *\"Tell me who your neighbors are, and I'll tell you who you are.\"*\n",
    "\n",
    "\n",
    "\n",
    "### 1. How It Works\n",
    "KNN assumes that similar things exist in close proximity.\n",
    "\n",
    "**The Algorithm Steps:**\n",
    "1.  **Choose K:** Select the number of neighbors (e.g., $K=5$).\n",
    "2.  **Calculate Distance:** Find the distance between the new data point and every point in the training set.\n",
    "3.  **Find Neighbors:** Sort distances and pick the $K$ nearest points.\n",
    "4.  **Vote (Classification):** Assign the class that is most frequent among the neighbors (Mode).\n",
    "    * *Regression:* Assign the average value of the neighbors (Mean).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Distance Metrics (The Math)\n",
    "How do we measure \"closeness\"?\n",
    "\n",
    "* **Euclidean Distance (L2 Norm):** The straight line distance (standard).\n",
    "    $$d(p, q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}$$\n",
    "\n",
    "* **Manhattan Distance (L1 Norm):** The distance traveling along grid lines (blocks). Good for high dimensions.\n",
    "    $$d(p, q) = \\sum_{i=1}^{n} |q_i - p_i|$$\n",
    "\n",
    "* **Minkowski Distance:** Generalized form.\n",
    "    * $p=1 \\rightarrow$ Manhattan.\n",
    "    * $p=2 \\rightarrow$ Euclidean.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Choosing the Right 'K'\n",
    "The value of $K$ controls the Bias-Variance Tradeoff.\n",
    "\n",
    "* **Small K (e.g., $K=1$):**\n",
    "    * **Overfitting (High Variance):** The decision boundary is very jagged. The model memorizes noise. If the nearest point is an outlier, the prediction is wrong.\n",
    "* **Large K (e.g., $K=100$):**\n",
    "    * **Underfitting (High Bias):** The decision boundary becomes overly smooth. It ignores local patterns and just predicts the majority class of the entire dataset.\n",
    "* **Optimal K:** Usually found via **Cross-Validation** (Square root of $N$ is a common starting rule of thumb).\n",
    "    * *Tip:* Choose an **Odd Number** for $K$ to avoid ties in binary classification.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Critical Requirement: Feature Scaling\n",
    "KNN calculates distances based on absolute numbers.\n",
    "\n",
    "* **Scenario:**\n",
    "    * Feature A: Age (18–90)\n",
    "    * Feature B: Salary (20,000–200,000)\n",
    "* **Problem:** A difference of 50 in Salary is tiny, but a difference of 50 in Age is massive. However, the Euclidean formula treats \"50\" equally. The Salary feature will dominate the distance calculation.\n",
    "* **Solution:** You **MUST** use **StandardScaler** or **MinMaxScaler** before running KNN.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Pros & Cons\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| Simple to understand and implement. | **Slow Prediction:** It must calculate the distance to *every* training point for *every* prediction ($O(N)$ complexity). |\n",
    "| No training period (Lazy Learner). | **Memory Intensive:** Must store the entire dataset. |\n",
    "| Non-parametric (makes no assumptions about data distribution). | **Curse of Dimensionality:** Performance drops drastically as features increase (space becomes sparse). |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. FAQ (Interview Questions)\n",
    "\n",
    "**Q: Why is KNN called a \"Lazy Learner\"?**\n",
    "**A:** Because it does not build a model during the training phase. It essentially does nothing until you ask it to make a prediction.\n",
    "\n",
    "**Q: How does KNN handle outliers?**\n",
    "**A:** Poorly, especially with small $K$. If $K=1$, a single outlier can change the decision boundary. Increasing $K$ smooths out the effect of outliers.\n",
    "\n",
    "**Q: What is the \"Curse of Dimensionality\" in KNN?**\n",
    "**A:** In high-dimensional space (many features), data points become very sparse. The distance between the \"nearest\" neighbor and the \"farthest\" neighbor becomes negligible, making the distance metric meaningless.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Code Implementation\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Always scale data for KNN!\n",
    "knn = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    ")\n",
    "\n",
    "knn.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
