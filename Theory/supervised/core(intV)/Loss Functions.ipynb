{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555854e3",
   "metadata": {},
   "source": [
    "Hinge Loss\n",
    "Used for: SVM classification, maximum-margin classification\n",
    "\n",
    "Binary Hinge Loss:\n",
    "\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    \"\"\"y_true ∈ {-1, 1}\"\"\"\n",
    "    return np.maximum(0, 1 - y_true * y_pred)\n",
    "\n",
    "# PyTorch implementation\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_true: 1 or -1\n",
    "        loss = torch.clamp(self.margin - y_true * y_pred, min=0)\n",
    "        return loss.mean()\n",
    "Multi-class Hinge Loss (Crammer-Singer):\n",
    "\n",
    "\n",
    "def multiclass_hinge_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: one-hot encoded or class indices\n",
    "    y_pred: raw scores/logits\n",
    "    \"\"\"\n",
    "    correct_class_score = y_pred[y_true]\n",
    "    margins = y_pred - correct_class_score[:, np.newaxis] + 1\n",
    "    margins[np.arange(len(y_true)), y_true] = 0\n",
    "    return np.maximum(0, margins).sum(axis=1).mean()\n",
    "2. Huber Loss (Smooth L1 Loss)\n",
    "Used for: Robust regression, less sensitive to outliers than MSE\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    \"\"\"\n",
    "    delta: threshold where loss transitions from quadratic to linear\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    abs_error = np.abs(error)\n",
    "    \n",
    "    quadratic = np.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    \n",
    "    return 0.5 * quadratic**2 + delta * linear\n",
    "\n",
    "# PyTorch implementation\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        error = y_true - y_pred\n",
    "        abs_error = torch.abs(error)\n",
    "        \n",
    "        quadratic = torch.min(abs_error, torch.tensor(self.delta))\n",
    "        linear = abs_error - quadratic\n",
    "        \n",
    "        loss = 0.5 * quadratic**2 + self.delta * linear\n",
    "        return loss.mean()\n",
    "3. Quantile Loss (Pinball Loss)\n",
    "Used for: Quantile regression, uncertainty estimation\n",
    "\n",
    "\n",
    "def quantile_loss(y_true, y_pred, quantile=0.5):\n",
    "    \"\"\"\n",
    "    quantile: target quantile (0.5 for median)\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    loss = np.maximum(quantile * error, (quantile - 1) * error)\n",
    "    return np.mean(loss)\n",
    "\n",
    "# For multiple quantiles simultaneously\n",
    "def multi_quantile_loss(y_true, y_preds, quantiles):\n",
    "    \"\"\"\n",
    "    y_preds: predictions for each quantile [q1, q2, ...]\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    for q, y_pred in zip(quantiles, y_preds):\n",
    "        error = y_true - y_pred\n",
    "        loss = np.maximum(q * error, (q - 1) * error)\n",
    "        total_loss += np.mean(loss)\n",
    "    return total_loss / len(quantiles)\n",
    "4. Tweedie Loss\n",
    "Used for: Compound Poisson-Gamma distributions (insurance claims, zero-inflated data)\n",
    "\n",
    "\n",
    "def tweedie_loss(y_true, y_pred, p=1.5):\n",
    "    \"\"\"\n",
    "    p: power parameter\n",
    "    1 < p < 2: Compound Poisson-Gamma\n",
    "    p = 1: Poisson\n",
    "    p = 2: Gamma\n",
    "    p = 3: Inverse Gaussian\n",
    "    \"\"\"\n",
    "    # Stabilize predictions\n",
    "    y_pred = np.clip(y_pred, 1e-10, None)\n",
    "    \n",
    "    if p == 1:\n",
    "        # Poisson\n",
    "        loss = y_pred - y_true * np.log(y_pred)\n",
    "    elif p == 2:\n",
    "        # Gamma\n",
    "        loss = y_true / y_pred + np.log(y_pred)\n",
    "    else:\n",
    "        # General Tweedie\n",
    "        loss = (y_pred**(2-p)) / (2-p) - (y_true * y_pred**(1-p)) / (1-p)\n",
    "    \n",
    "    return np.mean(loss)\n",
    "5. Focal Loss\n",
    "Used for: Class imbalance in object detection/classification\n",
    "\n",
    "\n",
    "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    gamma: focusing parameter (γ)\n",
    "    alpha: balancing parameter\n",
    "    \"\"\"\n",
    "    # Binary cross-entropy\n",
    "    bce = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    # Focal weight\n",
    "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "    focal_weight = (1 - p_t) ** gamma\n",
    "    \n",
    "    # Alpha weighting\n",
    "    alpha_weight = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    \n",
    "    loss = alpha_weight * focal_weight * bce\n",
    "    return np.mean(loss)\n",
    "\n",
    "# PyTorch implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "6. Dice Loss (F1 Loss)\n",
    "Used for: Image segmentation, imbalanced segmentation tasks\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Dice coefficient = 2|X∩Y| / (|X| + |Y|)\n",
    "    Dice Loss = 1 - Dice coefficient\n",
    "    \"\"\"\n",
    "    # Flatten\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    \n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    union = np.sum(y_true_f) + np.sum(y_pred_f)\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "# With PyTorch\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred: [B, C, H, W], y_true: [B, C, H, W]\n",
    "        intersection = torch.sum(y_pred * y_true)\n",
    "        union = torch.sum(y_pred) + torch.sum(y_true)\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice\n",
    "7. Triplet Loss\n",
    "Used for: Metric learning, face recognition, Siamese networks\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    \"\"\"\n",
    "    anchor, positive, negative: embeddings\n",
    "    margin: minimum distance between positive and negative pairs\n",
    "    \"\"\"\n",
    "    pos_dist = np.sum((anchor - positive)**2, axis=1)\n",
    "    neg_dist = np.sum((anchor - negative)**2, axis=1)\n",
    "    \n",
    "    loss = np.maximum(0, pos_dist - neg_dist + margin)\n",
    "    return np.mean(loss)\n",
    "\n",
    "# PyTorch implementation\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        loss = torch.relu(pos_dist - neg_dist + self.margin)\n",
    "        return loss.mean()\n",
    "8. Contrastive Loss\n",
    "Used for: Siamese networks, similarity learning\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, distance, margin=1.0):\n",
    "    \"\"\"\n",
    "    y_true: 1 for similar pairs, 0 for dissimilar\n",
    "    distance: Euclidean distance between embeddings\n",
    "    \"\"\"\n",
    "    # For similar pairs\n",
    "    similar_loss = y_true * distance**2\n",
    "    \n",
    "    # For dissimilar pairs\n",
    "    dissimilar_loss = (1 - y_true) * torch.maximum(margin - distance, 0)**2\n",
    "    \n",
    "    return torch.mean(similar_loss + dissimilar_loss) / 2\n",
    "\n",
    "# Complete PyTorch implementation\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        # Euclidean distance\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        \n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "Regression Algorithms\n",
    "Linear Models\n",
    "Linear Regression\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "Ridge Regression (L2 regularization)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1.0)\n",
    "Lasso Regression (L1 regularization)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.1)\n",
    "ElasticNet (L1 + L2)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "Tree-based Models\n",
    "Decision Tree Regressor\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor(max_depth=5)\n",
    "Random Forest Regressor\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "Gradient Boosting\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    ")\n",
    "XGBoost\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5\n",
    ")\n",
    "LightGBM\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1  # No limit\n",
    ")\n",
    "Kernel Methods\n",
    "Support Vector Regression (SVR)\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "Gaussian Process Regression\n",
    "\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "kernel = 1.0 * RBF(length_scale=1.0)\n",
    "model = GaussianProcessRegressor(kernel=kernel)\n",
    "Neural Networks\n",
    "MLP Regressor\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    max_iter=1000\n",
    ")\n",
    "Deep Learning (TensorFlow/PyTorch)\n",
    "\n",
    "\n",
    "# TensorFlow example\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer\n",
    "])\n",
    "Specialized Regressors\n",
    "Quantile Regressor\n",
    "\n",
    "\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "model = QuantileRegressor(quantile=0.5, alpha=1.0)\n",
    "Poisson Regressor (for count data)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "model = PoissonRegressor(alpha=1.0)\n",
    "Bayesian Ridge Regression\n",
    "\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "model = BayesianRidge()\n",
    "Ensemble Methods\n",
    "Stacking Regressor\n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor()),\n",
    "    ('xgb', xgb.XGBRegressor())\n",
    "]\n",
    "model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "Voting Regressor\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "model = VotingRegressor([\n",
    "    ('lr', LinearRegression()),\n",
    "    ('rf', RandomForestRegressor()),\n",
    "    ('xgb', xgb.XGBRegressor())\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
