{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc9c016",
   "metadata": {},
   "source": [
    "\n",
    "# Encoding\n",
    "| Type            | When to Use                 |\n",
    "| --------------- | --------------------------- |\n",
    "| Label / Ordinal | Ordered categories          |\n",
    "| One-Hot         | No order (Linear, SVM, KNN) |\n",
    "| Target Encoding | High-cardinality + Boosting |\n",
    "| Frequency       | Many categories             |\n",
    "\n",
    "\n",
    "# Scaling\n",
    "| Method         | Use With                   |\n",
    "| -------------- | -------------------------- |\n",
    "| StandardScaler | Linear, Logistic, SVM, PCA |\n",
    "| MinMaxScaler   | KNN, Neural Networks       |\n",
    "| RobustScaler   | Outliers                   |\n",
    "| No Scaling     | Tree, RF, XGBoost          |\n",
    "\n",
    "# Feature Selection\n",
    "| Method     | Example                 |\n",
    "| ---------- | ----------------------- |\n",
    "| Filter     | Correlation, Chi-square |\n",
    "| Wrapper    | RFE                     |\n",
    "| Embedded   | L1, Tree importance     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497586c",
   "metadata": {},
   "source": [
    "# Feature Engineering Guide\n",
    "\n",
    "Feature Engineering is often more important than the model selection itself. It involves transforming raw data into a format that best represents the underlying problem to the predictive models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Encoding (Categorical $\\rightarrow$ Numeric)\n",
    "\n",
    "**Why Encoding?**\n",
    "Machine Learning models require numerical input. They cannot perform matrix multiplication on strings like \"Red\" or \"Blue\".\n",
    "\n",
    "### Types of Encoding\n",
    "\n",
    "#### A. Label Encoding / Ordinal Encoding\n",
    "Assigns a unique integer to each category.\n",
    "* **Mapping:** $\\text{Red} \\rightarrow 0, \\quad \\text{Blue} \\rightarrow 1, \\quad \\text{Green} \\rightarrow 2$\n",
    "* **Use When:** Categories have an inherent **order** (e.g., Low $<$ Medium $<$ High).\n",
    "* **Warning:** If used on non-ordered data (nominal), the model might infer false relationships (e.g., assuming $2 > 1$, so Green is \"greater\" than Blue).\n",
    "\n",
    "#### B. One-Hot Encoding (The Standard )\n",
    "Creates a new binary column for each category.\n",
    "* **Mapping:**\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\text{Color} & \\rightarrow & \\text{Is\\_Red} & \\text{Is\\_Blue} & \\text{Is\\_Green} \\\\\n",
    "\\text{Red} & \\rightarrow & 1 & 0 & 0 \\\\\n",
    "\\text{Blue} & \\rightarrow & 0 & 1 & 0\n",
    "\\end{matrix}\n",
    "$$\n",
    "* **Use When:** Nominal data (No natural order). Essential for **Linear Models, Logistic Regression, SVM, KNN**.\n",
    "* **Warning:** Causes **High Dimensionality** (Curse of Dimensionality) if the cardinality is high.\n",
    "\n",
    "\n",
    "\n",
    "#### C. Target Encoding (Mean Encoding)\n",
    "Replaces the category with the average target value for that category.\n",
    "$$Value(c) = \\text{mean}(y \\mid x = c)$$\n",
    "* **Use When:** High-cardinality features (e.g., Zip Codes, Product IDs).\n",
    "* **Warning:** High risk of **Data Leakage**. Must be computed within Cross-Validation folds.\n",
    "\n",
    "#### D. Frequency / Count Encoding\n",
    "Replaces the category with the count of times it appears in the dataset.\n",
    "* **Use When:** High cardinality; Tree-based models (XGBoost, LightGBM).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Scaling (Feature Magnitude Control)\n",
    "\n",
    "**Why Scaling?**\n",
    "Models based on **Distance** (KNN, SVM) or **Gradient Descent** (Linear, Logistic, Neural Networks) are sensitive to the scale of input features.\n",
    "* *Example:* Age ($0-100$) vs. Salary ($10k-1M$). Salary will dominate the gradients/distances.\n",
    "\n",
    "### Types of Scaling\n",
    "\n",
    "#### A. Standardization (Z-Score Normalization) \n",
    "Centers data around 0 with a standard deviation of 1.\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "* **Use When:** Linear Regression, Logistic Regression, SVM, PCA, Neural Networks.\n",
    "* **Assumption:** Assumes data is roughly Gaussian (Bell Curve).\n",
    "\n",
    "#### B. Min-Max Scaling (Normalization)\n",
    "Squishes data into a fixed range, usually $[0, 1]$.\n",
    "$$x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "* **Use When:** Neural Networks, Image Data (pixels), KNN.\n",
    "* **Warning:** Very sensitive to **outliers**. A single large outlier squashes all other data to $0$.\n",
    "\n",
    "\n",
    "\n",
    "#### C. Robust Scaling\n",
    "Scales using median and quantiles (Interquartile Range).\n",
    "$$x' = \\frac{x - \\text{median}}{IQR}$$\n",
    "* **Use When:** The dataset contains significant **outliers**.\n",
    "\n",
    "###  No Scaling Needed For:\n",
    "* **Tree-Based Models:** Decision Trees, Random Forests, XGBoost, LightGBM. (Splits are based on ordering, not distance).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Selection (Keep Useful Features)\n",
    "\n",
    "**Why?** reduces overfitting, speeds up training, and improves interpretability.\n",
    "\n",
    "### Methods\n",
    "\n",
    "1.  **Filter Methods (Pre-processing):**\n",
    "    * Statistical tests done *before* the model runs.\n",
    "    * **Techniques:** Correlation Matrix, Chi-Square ($\\chi^2$), ANOVA, Mutual Information.\n",
    "    * *Pros:* Fast, model-agnostic.\n",
    "\n",
    "2.  **Wrapper Methods (Iterative):**\n",
    "    * Trains the model multiple times with different subsets.\n",
    "    * **Techniques:** Recursive Feature Elimination (RFE), Forward Selection, Backward Elimination.\n",
    "    * *Pros:* Very accurate. *Cons:* Computationally expensive/slow.\n",
    "\n",
    "3.  **Embedded Methods (During Training) :**\n",
    "    * The model selects features as part of the learning process.\n",
    "    * **Techniques:** Lasso (L1 Regularization), Ridge (L2), Tree Feature Importance.\n",
    "    * *Pros:* Best balance of accuracy and speed.\n",
    "\n",
    "---\n",
    "\n",
    "##  The Golden Table: When to Use What\n",
    "\n",
    "| Model | Encoding | Scaling | Feature Selection |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Linear Regression** | One-Hot | **Standard** | L1 (Lasso) / Correlation |\n",
    "| **Logistic Regression** | One-Hot | **Standard** | L1 / L2 / RFE |\n",
    "| **SVM** | One-Hot | **Standard** | RFE / PCA |\n",
    "| **KNN** | One-Hot | **MinMax** | Correlation (Critical) |\n",
    "| **Neural Networks** | One-Hot / Embedding | **Standard / MinMax** | Dropout / L1 / L2 |\n",
    "| **Decision Trees** | Label / Ordinal |  None | Gini / Entropy |\n",
    "| **Random Forest** | Label / Ordinal |  None | Feature Importance |\n",
    "| **Boosting (XGB)** | Target / Frequency |  None | Feature Importance |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
