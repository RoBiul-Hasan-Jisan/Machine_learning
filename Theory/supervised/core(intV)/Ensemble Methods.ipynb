{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ea2cde",
   "metadata": {},
   "source": [
    "# Bagging, AdaBoost, Stacking, Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa771453",
   "metadata": {},
   "source": [
    "### Comparison of Ensemble Learning Techniques\n",
    "\n",
    "| Classifier | Core Principle | Base Learners | Training Method | Key Advantages | Best For | Python Class |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Bagging** | **Bootstrap Aggregating**: Parallel training via sampling. | Same type ( Trees). | Independent training on random subsets. | Reduces variance & overfitting. | High-variance models. | `BaggingClassifier` |\n",
    "| **AdaBoost**| **Adaptive Boosting**: Each model corrects previous errors. | Weak learners (Stumps). | Sequential; weights misclassified samples higher. | Reduces bias, high accuracy. | Binary classification. | `AdaBoostClassifier`|\n",
    "| **Voting** | **Aggregates predictions** via majority or average. | Heterogeneous types. | Independent training on full dataset. | Simple, robust, no retraining needed. | Diverse model strengths. | `VotingClassifier` |\n",
    "| **Stacking** | **Stacked Generalization**: Trains a meta-model to combine results. | Heterogeneous types. | Base models output $\\rightarrow$ Meta-model input. | Highest accuracy potential. | Competitions/Final tuning. | `StackingClassifier` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7492b7a3",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Key Insight: By training on different data samples, it reduces the variance of unstable models. The classic example is the Random Forest, which is Bagging applied to Decision Trees with added random feature selection.\n",
    "\n",
    "When to Use: Your base model (like a deep Decision Tree) is accurate but overfits (high variance). Bagging stabilizes it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,  # Use 80% of data for each bootstrap sample\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0d7a1",
   "metadata": {},
   "source": [
    "AdaBoost (Adaptive Boosting)\n",
    "\n",
    "Key Insight: It's an adaptive process. The algorithm focuses more and more on the examples that previous models got wrong. This sequential correction helps reduce bias.\n",
    "\n",
    "When to Use: Your base model is too simple (high bias), like a stump (tree of depth 1). AdaBoost combines many weak learners to create a strong, complex boundary.\n",
    "\n",
    "Interview Tip: Be ready to explain the weight update formula. Misclassified samples have their weights increased, correctly classified ones have weights decreased.\n",
    "\n",
    "Voting Classifier\n",
    "\n",
    "Key Insight: It's the simplest form of \"model averaging.\" Hard Voting uses the majority class label, while Soft Voting averages predicted probabilities (often better).\n",
    "\n",
    "When to Use: You have several good but different models ( a logistic regression, an SVM, and a tree). Voting often yields a more reliable \"committee\" decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cde706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('lr', LogisticRegression()), ('svc', SVC(probability=True))],\n",
    "    voting='soft'  # Use 'soft' for probabilistic averaging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbf89b",
   "metadata": {},
   "source": [
    "Stacking (Stacked Generalization)\n",
    "\n",
    "Key Insight: It learns how to combine models. Instead of a simple vote, a meta-learner (like logistic regression) discovers the optimal way to weigh the base models' predictions.\n",
    "\n",
    "When to Use: For squeezing out the last bit of performance in competitions. It's more powerful but also more complex and prone to overfitting if not carefully cross-validated.\n",
    "\n",
    "Crucial Practice: The base models' predictions for the meta-model must be generated using cross-validation on the training set to prevent data leakage. Scikit-learn's StackingClassifier handles this internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045c1db",
   "metadata": {},
   "source": [
    "How to Choose the Right Ensemble Method\n",
    "Use this simple decision guide:\n",
    "\n",
    "Start with your base model:\n",
    "\n",
    "Is it a complex model that overfits (, a deep Decision Tree)? → Try Bagging.\n",
    "\n",
    "Is it a very simple model that underfits (, a shallow tree)? → Try AdaBoost.\n",
    "\n",
    "Look at your model collection:\n",
    "\n",
    "Do you have several well-performing models of different types? → Try Voting for simplicity or Stacking for maximum performance.\n",
    "\n",
    "Do you want a quick, robust improvement with minimal complexity? → Choose Voting.\n",
    "\n",
    "Are you in a competition or final tuning stage and need the best possible accuracy? → Invest time in Stacking.\n",
    "\n",
    "Consider computation:\n",
    "\n",
    "Bagging is easily parallelized.\n",
    "\n",
    "AdaBoost and Stacking are sequential and can be slower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc2ae8",
   "metadata": {},
   "source": [
    "Bagging → Parallel, Bootstrap, reduces Variance.\n",
    "\n",
    "Boosting (AdaBoost) → Sequential, Reweights errors, reduces Bias.\n",
    "\n",
    "Voting → Averages predictions, Simple ensemble.\n",
    "\n",
    "Stacking → Meta-learner, Complex but powerful ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
