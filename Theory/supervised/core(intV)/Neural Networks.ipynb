{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aea8cd5",
   "metadata": {},
   "source": [
    "# ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8065f1",
   "metadata": {},
   "source": [
    "What is ANN?\n",
    "\n",
    "An Artificial Neural Network (ANN) is a computational model inspired by the human brain, made of interconnected neurons arranged in layers.\n",
    "\n",
    "Used for:\n",
    "\n",
    "Regression → continuous output\n",
    "\n",
    "Classification → discrete class labels\n",
    "\n",
    "How ANN Learns\n",
    "\n",
    "Forward propagation → prediction\n",
    "\n",
    "Loss calculation\n",
    "\n",
    "Backpropagation → gradient computation\n",
    "\n",
    "Weight update → Gradient Descent\n",
    "\n",
    "ANN for Regression\n",
    "\n",
    "Output Layer\n",
    "\n",
    "1 neuron\n",
    "\n",
    "Linear activation\n",
    "\n",
    "Loss Functions\n",
    "\n",
    "MSE\n",
    "\n",
    "MAE\n",
    "\n",
    "Huber Loss\n",
    "\n",
    "ANN for Classification\n",
    "Binary Classification\n",
    "\n",
    "Output neuron: 1\n",
    "\n",
    "Activation: Sigmoid\n",
    "\n",
    "Loss: Binary Cross-Entropy\n",
    "\n",
    "Multi-Class Classification\n",
    "\n",
    "Output neurons: Number of classes\n",
    "\n",
    "Activation: Softmax\n",
    "\n",
    "Loss: Categorical Cross-Entropy\n",
    "\n",
    "\n",
    "| Function | Use                 |\n",
    "| -------- | ------------------- |\n",
    "| ReLU     | Hidden layers       |\n",
    "| Sigmoid  | Binary output       |\n",
    "| Softmax  | Multi-class         |\n",
    "| Tanh     | Alternative to ReLU |\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "| Function | Use                 |\n",
    "| -------- | ------------------- |\n",
    "| ReLU     | Hidden layers       |\n",
    "| Sigmoid  | Binary output       |\n",
    "| Softmax  | Multi-class         |\n",
    "| Tanh     | Alternative to ReLU |\n",
    "\n",
    "Regularization Techniques\n",
    "\n",
    "L1 / L2 Regularization\n",
    "\n",
    "Dropout\n",
    "\n",
    "Batch Normalization\n",
    "\n",
    "Early Stopping\n",
    "\n",
    "\n",
    "| Aspect              | ANN  | Linear / Tree |\n",
    "| ------------------- | ---- | ------------- |\n",
    "| Non-linearity       | High | Limited       |\n",
    "| Feature engineering | Less | More          |\n",
    "| Interpretability    | Low  | High          |\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "##  Activation Functions\n",
    "\n",
    "| Function | Formula | Use Case | Derivative |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **ReLU** | $f(x) = \\max(0, x)$ | Hidden layers (fast, standard). | 1 ($x>0$), else 0 |\n",
    "| **Sigmoid**| $f(x) = \\frac{1}{1+e^{-x}}$ | Binary classification output. | $f(x)(1-f(x))$ |\n",
    "| **Tanh** | $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Hidden layers (zero-centered). | $1 - f(x)^2$ |\n",
    "| **Softmax** | $f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | Multi-class output. | Vector-based |\n",
    "| **Linear** | $f(x) = x$ | Regression output. | 1 |\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Summary of Deep Learning Optimization\n",
    "\n",
    "| Technique | Goal | Key Mechanism |\n",
    "| :--- | :--- | :--- |\n",
    "| **Backpropagation** | Training | Chain Rule / Gradient Descent |\n",
    "| **ReLU** | Stability | Prevents Vanishing Gradients |\n",
    "| **Dropout** | Generalization | Randomly disables neurons |\n",
    "| **L2 Regularization**| Complexity Control | Penalizes large weight values |\n",
    "| **Gradient Clipping** | Stability | Caps maximum gradient value |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24790676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Regression ANN\n",
    "reg_model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    layers.Dropout(0.2),  # Regularization\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "# Classification ANN (Binary)\n",
    "clf_binary = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Classification ANN (Multi-class)\n",
    "clf_multiclass = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Multi-class\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification\n",
    "binary_model = models.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Single neuron for binary\n",
    "])\n",
    "\n",
    "binary_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Multi-class Classification\n",
    "num_classes = len(np.unique(y_train))\n",
    "multi_model = models.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# For integer labels\n",
    "multi_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# For one-hot encoded labels\n",
    "# loss='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6760affe",
   "metadata": {},
   "source": [
    "1. Backpropagation Explanation\n",
    "\"Backpropagation computes gradients layer-by-layer using the chain rule:\n",
    "\n",
    "Forward pass: compute predictions and loss\n",
    "\n",
    "Backward pass: compute gradient $\\frac{\\partial L}{\\partial w}$ for each weight\n",
    "\n",
    "Update weights: $w = w - \\eta \\frac{\\partial L}{\\partial w}$\n",
    "The key insight is reusing intermediate computations for efficiency.\"\n",
    "\n",
    "2. Vanishing/Exploding Gradients\n",
    "\n",
    "\"Vanishing gradients occur in deep networks with sigmoid/tanh when gradients become extremely small. Solutions:\n",
    "\n",
    "Use ReLU activation\n",
    "\n",
    "Batch Normalization stabilizes activations\n",
    "\n",
    "Residual connections (skip connections)\n",
    "\n",
    "Gradient clipping for exploding gradients\n",
    "\n",
    "Exploding gradients happen when gradients grow exponentially, often fixed with gradient clipping.\"\n",
    "\n",
    "3. Overfitting Prevention\n",
    "\n",
    "\n",
    "Dropout: Randomly disable neurons during training\n",
    "\n",
    "L1/L2 Regularization: Penalize large weights\n",
    "\n",
    "Early Stopping: Stop when validation loss plateaus\n",
    "\n",
    "Data Augmentation: Artificially increase training data\n",
    "\n",
    "Batch Normalization: Reduces internal covariate shift\"\n",
    "\n",
    "4. Batch Size vs Learning Rate\n",
    "There's a linear scaling rule: When increasing batch size by k, increase learning rate by k to maintain similar gradient noise. However, very large batches may generalize worse (generalization gap)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
