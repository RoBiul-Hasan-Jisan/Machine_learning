{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00f2f12",
   "metadata": {},
   "source": [
    "\n",
    "What is gradient boosting, and how does it differ from bagging?\n",
    "\n",
    "Explain the boosting concept: sequential learning from mistakes.\n",
    "\n",
    "What is the gradient in gradient boosting? (Hint: It's about gradients of loss function)\n",
    "\n",
    "Why is it called \"gradient\" boosting? Connection to gradient descent.\n",
    "\n",
    "Write the generic gradient boosting algorithm pseudocode.\n",
    "\n",
    "\n",
    "What is the role of weak learners (typically decision trees)?\n",
    "\n",
    "How does each new tree correct errors of the previous ensemble?\n",
    "\n",
    "\n",
    "Loss Functions:\n",
    "\n",
    "Regression: MSE, MAE, Huber loss\n",
    "\n",
    "Classification: Log loss (binary), Multi-class log loss, Exponential loss\n",
    "\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "How are leaf weights calculated in gradient boosting trees?\n",
    "\n",
    "Explain the Taylor expansion approximation used in XGBoost.\n",
    "\n",
    "\n",
    "Learning rate (shrinkage) ν: Why is it crucial?\n",
    "\n",
    "Subsampling (stochastic gradient boosting): Benefits and implementation.\n",
    "\n",
    "Tree constraints: max_depth, min_samples_leaf, etc.\n",
    "\n",
    "Number of estimators: How many boosting rounds?\n",
    "\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting)\n",
    "What are XGBoost's key innovations?\n",
    "\n",
    "Explain regularization in XGBoost (L1/L2 on leaf weights).\n",
    "\n",
    "What is the approximate greedy algorithm for split finding?\n",
    "\n",
    "How does sparsity-aware split finding work?\n",
    "\n",
    "What are column (feature) subsampling and block structure?\n",
    "\n",
    "LightGBM\n",
    "What is Gradient-based One-Side Sampling (GOSS)?\n",
    "\n",
    "Explain Exclusive Feature Bundling (EFB).\n",
    "\n",
    "Why is LightGBM faster than XGBoost?\n",
    "\n",
    "Leaf-wise growth vs. Level-wise growth.\n",
    "\n",
    "CatBoost\n",
    "How does it handle categorical features automatically?\n",
    "\n",
    "Explain ordered boosting (permutation-driven).\n",
    "\n",
    "What is the oblivious trees concept?\n",
    "\n",
    "How does CatBoost prevent target leakage?\n",
    "\n",
    "\n",
    "Critical parameters:\n",
    "\n",
    "learning rate (η) and n_estimators (trade-off)\n",
    "\n",
    "max_depth (tree complexity)\n",
    "\n",
    "subsample (row sampling)\n",
    "\n",
    "colsample_bytree/colsample_bylevel (feature sampling)\n",
    "\n",
    "reg_alpha, reg_lambda (L1/L2 regularization)\n",
    "\n",
    "Tuning strategies: Grid search, random search, Bayesian optimization\n",
    "\n",
    "Early stopping: How to implement and when to use?\n",
    "\n",
    "Advantages & Disadvantages\n",
    "Advantages:\n",
    "State-of-the-art performance on structured data\n",
    "\n",
    "Handles mixed data types well\n",
    "\n",
    "Built-in regularization prevents overfitting\n",
    "\n",
    "Feature importance scores\n",
    "\n",
    "Disadvantages:\n",
    "Computationally expensive\n",
    "\n",
    "More hyperparameters to tune\n",
    "\n",
    "Sequential training (harder to parallelize)\n",
    "\n",
    "Less interpretable than single trees\n",
    "\n",
    "\n",
    "Training: Sequential vs. Parallel\n",
    "\n",
    "Bias-Variance: Boosting reduces bias, bagging reduces variance\n",
    "\n",
    "Overfitting: GBM more prone if not regularized\n",
    "\n",
    "Performance: GBM often better but requires careful tuning\n",
    "\n",
    "Speed: Random Forest faster to train\n",
    "\n",
    "Gain: Average contribution of features across trees\n",
    "\n",
    "Cover: Number of observations affected by feature\n",
    "\n",
    "Frequency: How often feature is used in splits\n",
    "\n",
    "Which importance measure is most reliable?\n",
    "\n",
    "\n",
    "Shrinkage (learning rate)\n",
    "\n",
    "Tree constraints (depth, samples)\n",
    "\n",
    "Subsampling (rows and columns)\n",
    "\n",
    "L1/L2 regularization on leaf weights\n",
    "\n",
    "Dropout in boosting (DART in XGBoost)\n",
    "\n",
    "How to handle missing values in gradient boosting?\n",
    "\n",
    "Categorical encoding: One-hot vs. ordinal vs. target encoding\n",
    "\n",
    "Monotonic constraints: Ensuring directional relationships\n",
    "\n",
    "Interaction constraints: Limiting feature interactions\n",
    "\n",
    "GPU acceleration: Which libraries support it?\n",
    "\n",
    "SHAP values for gradient boosting models\n",
    "\n",
    "Partial dependence plots (PDP)\n",
    "\n",
    "Individual conditional expectation (ICE) plots\n",
    "\n",
    "Tree SHAP: Efficient SHAP for tree ensembles\n",
    "\n",
    "Local surrogate models (LIME)\n",
    "\n",
    "\n",
    "Quantile regression with gradient boosting\n",
    "\n",
    "Custom loss functions: How to implement them\n",
    "\n",
    "Multi-output regression/classification\n",
    "\n",
    "Transfer learning with pre-trained boosting models\n",
    "\n",
    "Online gradient boosting for streaming data\n",
    "\n",
    "\n",
    "Overfitting: Signs and prevention strategies\n",
    "\n",
    "Target leakage: Especially in time series\n",
    "\n",
    "Hyperparameter sensitivity: Finding robust configurations\n",
    "\n",
    "Memory issues with large datasets\n",
    "\n",
    "Categorical cardinality: High cardinality problems\n",
    "\n",
    "Why does gradient boosting often outperform random forest?\n",
    "\n",
    "How does boosting reduce bias? (Answer: By focusing on difficult samples)\n",
    "\n",
    "What is the bias-variance decomposition for boosting?\n",
    "\n",
    "Explain the universal approximation capability of boosting.\n",
    "\n",
    "What happens if you set learning rate = 1 in gradient boosting?\n",
    "\n",
    "Why do we use shallow trees (max_depth = 3-6) in gradient boosting?\n",
    "\n",
    "How does XGBoost handle missing values internally?\n",
    "\n",
    "What is the difference between gblinear and gbtree in XGBoost?\n",
    "\n",
    "How would you explain gradient boosting to a 10-year-old?\n",
    "\n",
    "Why is gradient boosting called \"gradient\" boosting?\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Show that with squared loss, gradient boosting reduces to fitting residuals.\n",
    "\n",
    "Prove that gradient boosting can be seen as functional gradient descent.\n",
    "\n",
    "Histogram-based algorithms for split finding\n",
    "\n",
    "Parallelization strategies: Feature parallel, data parallel\n",
    "\n",
    "Distributed training for large datasets\n",
    "\n",
    "Incremental learning for new data\n",
    "\n",
    "Time-series cross-validation for temporal data\n",
    "\n",
    "Group K-fold for correlated samples\n",
    "\n",
    "Calibration of predicted probabilities\n",
    "\n",
    "\n",
    "Competitions: Kaggle dominance of gradient boosting\n",
    "\n",
    "Finance: Credit scoring, fraud detection\n",
    "\n",
    "E-commerce: Click-through rate prediction\n",
    "\n",
    "Healthcare: Disease prediction, risk scoring\n",
    "\n",
    "Manufacturing: Predictive maintenance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
