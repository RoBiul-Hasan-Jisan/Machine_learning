{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adfdef3",
   "metadata": {},
   "source": [
    "\n",
    "What is Random Forest, and how does it work?\n",
    "\n",
    "Explain the bagging (bootstrap aggregating) concept.\n",
    "\n",
    "Why is it called a \"forest\"?\n",
    "\n",
    "What is the ensemble learning principle behind Random Forest?\n",
    "\n",
    "\n",
    "How does bootstrap sampling work in Random Forest?\n",
    "\n",
    "What is out-of-bag (OOB) error? How is it calculated?\n",
    "\n",
    "Explain feature randomness (random subset of features at each split).\n",
    "\n",
    "Why does Random Forest use both row sampling and column sampling?\n",
    "\n",
    "\n",
    "Write the pseudocode for Random Forest algorithm.\n",
    "\n",
    "How are individual decision trees grown in Random Forest?\n",
    "\n",
    "What are the differences between trees in a Random Forest vs. single decision trees?\n",
    "\n",
    "How does Random Forest make predictions?\n",
    "\n",
    "Classification: Majority voting\n",
    "\n",
    "Regression: Averaging\n",
    "\n",
    "\n",
    "n_estimators: Number of trees - how to choose optimal value?\n",
    "\n",
    "max_features: What are typical values (sqrt, log2, or percentage)?\n",
    "\n",
    "max_depth: Should trees be fully grown or limited?\n",
    "\n",
    "min_samples_split and min_samples_leaf\n",
    "\n",
    "bootstrap: With or without replacement?\n",
    "\n",
    "How to tune Random Forest hyperparameters effectively?\n",
    "\n",
    "\n",
    "Advantages:\n",
    "Reduces overfitting compared to single decision trees\n",
    "\n",
    "Handles high-dimensional data well\n",
    "\n",
    "Provides feature importance\n",
    "\n",
    "Handles missing values internally\n",
    "\n",
    "Robust to outliers\n",
    "\n",
    "Parallelizable\n",
    "\n",
    "Disadvantages:\n",
    "Less interpretable than single trees\n",
    "\n",
    "Can be slow for large datasets\n",
    "\n",
    "Memory intensive\n",
    "\n",
    "May overfit noisy datasets\n",
    "\n",
    "\n",
    "How is feature importance calculated in Random Forest?\n",
    "\n",
    "Gini importance (mean decrease impurity)\n",
    "\n",
    "Permutation importance (mean decrease accuracy)\n",
    "\n",
    "Which method is more reliable? Why?\n",
    "\n",
    "Can feature importance be misleading? When?\n",
    "\n",
    "How to interpret feature importance scores?\n",
    "\n",
    "\n",
    "What is OOB score? How is it calculated?\n",
    "\n",
    "Why can OOB error be used as an unbiased estimate of generalization error?\n",
    "\n",
    "Compare OOB error vs. cross-validation for Random Forest.\n",
    "\n",
    "When should you use OOB vs. cross-validation?\n",
    "\n",
    "\n",
    "Extra-Trees (Extremely Randomized Trees): How are they different?\n",
    "\n",
    "Random Subspaces method\n",
    "\n",
    "Random Patches method\n",
    "\n",
    "Balanced Random Forest for imbalanced data\n",
    "\n",
    "Quantile Random Forest for prediction intervals\n",
    "\n",
    "\n",
    "How to implement Random Forest from scratch (conceptual)?\n",
    "\n",
    "Using scikit-learn: RandomForestClassifier and RandomForestRegressor\n",
    "\n",
    "How to handle categorical variables in Random Forest?\n",
    "\n",
    "Best practices for preprocessing for Random Forest.\n",
    "\n",
    "\n",
    "Why does Random Forest reduce variance without increasing bias too much?\n",
    "\n",
    "What is the bias-variance tradeoff in Random Forest?\n",
    "\n",
    "How does the Law of Large Numbers apply to Random Forest?\n",
    "\n",
    "Explain why Random Forest doesn't overfit as you add more trees.\n",
    "\n",
    "\n",
    "Random Forest vs. single Decision Tree\n",
    "\n",
    "Random Forest vs. Gradient Boosting\n",
    "\n",
    "Random Forest vs. Neural Networks\n",
    "\n",
    "Random Forest vs. SVM for high-dimensional data\n",
    "\n",
    "Bagging vs. Boosting vs. Stacking\n",
    "\n",
    "\n",
    "Proximity matrix: What is it and how is it used?\n",
    "\n",
    "Missing value imputation using Random Forest\n",
    "\n",
    "Anomaly detection with Random Forest\n",
    "\n",
    "Random Forest for unsupervised learning\n",
    "\n",
    "Incremental Random Forest for streaming data\n",
    "\n",
    "How many trees are enough? (diminishing returns)\n",
    "\n",
    "How to deal with class imbalance in Random Forest?\n",
    "\n",
    "Class weighting\n",
    "\n",
    "Balanced Random Forest\n",
    "\n",
    "Synthetic sampling\n",
    "\n",
    "How to handle correlated features?\n",
    "\n",
    "Memory management for large Random Forests.\n",
    "\n",
    "\n",
    "Why are the trees in Random Forest grown deep (fully grown)?\n",
    "\n",
    "What happens if you set max_features = 1 in Random Forest?\n",
    "\n",
    "Why does Random Forest work well with default parameters?\n",
    "\n",
    "Can Random Forest overfit? When?\n",
    "\n",
    "How would you explain Random Forest to a non-technical person?\n",
    "\n",
    "What is the out-of-bag error estimate and why is it useful?\n",
    "\n",
    "How does Random Forest handle multicollinearity?\n",
    "\n",
    "s\n",
    "Prove that the expected error of an ensemble is less than or equal to the average error of individual classifiers.\n",
    "\n",
    "How is the variance reduction achieved in bagging?\n",
    "\n",
    "\n",
    "\n",
    "How does decorrelation of trees help in reducing variance?\n",
    "\n",
    "\n",
    "Parallelization strategies for Random Forest\n",
    "\n",
    "GPU-accelerated Random Forest implementations\n",
    "\n",
    "Sampling strategies for large datasets\n",
    "\n",
    "Feature selection before Random Forest - necessary or not?\n",
    "\n",
    "\n",
    "Credit risk modeling\n",
    "\n",
    "Medical diagnosis\n",
    "\n",
    "Customer churn prediction\n",
    "\n",
    "Image classification\n",
    "\n",
    "Anomaly detection in cybersecurity\n",
    "\n",
    "Recommendation systems\n",
    "\n",
    "\n",
    "Partial dependence plots (PDP)\n",
    "\n",
    "Individual conditional expectation (ICE) plots\n",
    "\n",
    "SHAP values for Random Forest\n",
    "\n",
    "Local interpretable model-agnostic explanations (LIME)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
