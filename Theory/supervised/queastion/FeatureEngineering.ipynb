{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0c0c26",
   "metadata": {},
   "source": [
    "\n",
    "What is feature engineering? Why is it called the \"secret sauce\" of ML?\n",
    "\n",
    "Explain the difference between feature selection, feature extraction, and feature creation.\n",
    "\n",
    "Why is feature engineering more critical than algorithm choice in many cases?\n",
    "\n",
    "What's the 80/20 rule in ML? (80% data preparation, 20% modeling)\n",
    "\n",
    "\n",
    "Numerical Features\n",
    "How do you handle outliers in numerical features? (Winsorizing, trimming, transformation)\n",
    "\n",
    "What are scaling methods and when to use each?\n",
    "\n",
    "Min-Max Scaling: When to use? (Neural networks, values in specific range)\n",
    "\n",
    "Standardization (Z-score): When to use? (Models assuming Gaussian)\n",
    "\n",
    "Robust Scaling: When to use? (With outliers)\n",
    "\n",
    "MaxAbs Scaling: When to use?\n",
    "\n",
    "Categorical Features\n",
    "Compare one-hot encoding vs label encoding vs target encoding\n",
    "\n",
    "When would you use each?\n",
    "\n",
    "What is the dummy variable trap and how to avoid it?\n",
    "\n",
    "How to handle high-cardinality categorical features? (Target encoding, frequency encoding, embedding)\n",
    "\n",
    "DateTime Features\n",
    "What features can you extract from timestamps?\n",
    "\n",
    "Examples: hour, day, month, year, dayofweek, weekend flag, holiday flag\n",
    "\n",
    "How to handle cyclical features? (sin/cos transformation)\n",
    "\n",
    "Time since specific event features\n",
    "\n",
    "Text Features\n",
    "Bag of Words vs TF-IDF vs Word Embeddings\n",
    "\n",
    "When to use n-grams?\n",
    "\n",
    "How to handle text length, word count, readability scores?\n",
    "\n",
    "\n",
    "Polynomial Features\n",
    "When to create interaction terms and polynomial features?\n",
    "\n",
    "What's the risk of polynomial features? (Curse of dimensionality, overfitting)\n",
    "\n",
    "How to select meaningful interactions?\n",
    "\n",
    "Binning/Discretization\n",
    "Why bin continuous features? (Handle non-linear relationships, outliers)\n",
    "\n",
    "Methods: Equal-width, equal-frequency, k-means, decision tree based\n",
    "\n",
    "How to choose number of bins?\n",
    "\n",
    "Aggregation Features\n",
    "Creating features by aggregating across:\n",
    "\n",
    "Time windows (rolling means, expanding stats)\n",
    "\n",
    "Groups (customer average, product median)\n",
    "\n",
    "Related entities (user's average purchase)\n",
    "\n",
    "Domain-Specific Features\n",
    "Finance: Moving averages, volatility measures, returns\n",
    "\n",
    "E-commerce: Time since last purchase, purchase frequency, monetary value\n",
    "\n",
    "Healthcare: BMI, combined lab ratios, vital sign trends\n",
    "\n",
    "Log Transformation\n",
    "When to use log transform? (Right-skewed data, multiplicative relationships)\n",
    "\n",
    "What about zero values? (log(x+1))\n",
    "\n",
    "Interpretability of log-transformed features\n",
    "\n",
    "Box-Cox & Yeo-Johnson\n",
    "What are power transformations and when to use them?\n",
    "\n",
    "Difference between Box-Cox (positive values) and Yeo-Johnson (any values)\n",
    "\n",
    "Rank Transformation\n",
    "When to use rank features? (Outliers, non-linear but monotonic relationships)\n",
    "\n",
    "\n",
    "Numerical Missing Data\n",
    "Mean/Median/Mode imputation\n",
    "\n",
    "KNN imputation\n",
    "\n",
    "Regression imputation\n",
    "\n",
    "Add \"missing\" flag\n",
    "\n",
    "Categorical Missing Data\n",
    "Mode imputation\n",
    "\n",
    "\"Missing\" as a new category\n",
    "\n",
    "Predictive imputation\n",
    "\n",
    "Advanced Methods\n",
    "MICE (Multiple Imputation by Chained Equations)\n",
    "\n",
    "Deep learning imputation\n",
    "\n",
    "When to drop vs impute?\n",
    "\n",
    "\n",
    "Filter Methods\n",
    "Correlation analysis (Pearson, Spearman)\n",
    "\n",
    "Chi-square test (categorical-categorical)\n",
    "\n",
    "ANOVA F-test (numerical-categorical)\n",
    "\n",
    "Mutual information\n",
    "\n",
    "Wrapper Methods\n",
    "Forward selection\n",
    "\n",
    "Backward elimination\n",
    "\n",
    "Recursive feature elimination (RFE)\n",
    "\n",
    "Pros/cons of wrapper methods\n",
    "\n",
    "Embedded Methods\n",
    "L1 regularization (Lasso)\n",
    "\n",
    "Tree-based feature importance\n",
    "\n",
    "Permutation importance\n",
    "\n",
    "Interview Questions on Selection\n",
    "\"How many features are too many?\" (Rule of thumb: 10x samples per feature)\n",
    "\n",
    "\"What would you do if you have 1000 features but only 100 samples?\"\n",
    "\n",
    "\"How to handle correlated features in selection?\"\n",
    "\n",
    "\n",
    "PCA vs t-SNE vs UMAP: When to use each?\n",
    "\n",
    "Autoencoders for feature extraction\n",
    "\n",
    "When to use dimensionality reduction vs feature selection?\n",
    "\n",
    "\n",
    "Lag features (t-1, t-2, ...)\n",
    "\n",
    "Rolling statistics (mean, std, min, max over window)\n",
    "\n",
    "Expanding window features\n",
    "\n",
    "Difference features (t - t-1)\n",
    "\n",
    "Seasonal decomposition features\n",
    "\n",
    "\n",
    "Feature Crossing\n",
    "Manual feature crossing vs automated (FM, Deep&Wide)\n",
    "\n",
    "Example: Age × Income, Location × Time\n",
    "\n",
    "Cluster Features\n",
    "Using clustering results as features (k-means labels)\n",
    "\n",
    "Distance to cluster centroids as features\n",
    "\n",
    "Text-Specific\n",
    "Sentiment scores\n",
    "\n",
    "Named Entity Recognition features\n",
    "\n",
    "Readability scores\n",
    "\n",
    "Embedding averages\n",
    "\n",
    "\"What is the curse of dimensionality and how does feature engineering help?\"\n",
    "\n",
    "\"How does feature scaling affect different algorithms?\"\n",
    "\n",
    "\"Explain bias-variance tradeoff in context of feature engineering\"\n",
    "\n",
    "\"What is target leakage and how to avoid it in feature engineering?\"\n",
    "\n",
    "\n",
    "\"Tell me about a time when feature engineering significantly improved your model\"\n",
    "\n",
    "\"Describe your process for feature engineering under time constraints\"\n",
    "\n",
    "\"How do you collaborate with domain experts for feature engineering?\"\n",
    "\n",
    "\"How do you document and version your features?\"\n",
    "\n",
    "\n",
    "\"You have 5000 features and 1000 samples. What do you do?\"\n",
    "\n",
    "\"How would you engineer features for a dataset with 80% missing values?\"\n",
    "\n",
    "\"What if your test data has categories not seen in training?\"\n",
    "\n",
    "\"How to handle features with different scales and distributions together?\"\n",
    "\n",
    "\n",
    "\"How to make feature engineering efficient for large datasets?\"\n",
    "\n",
    "\"Batch vs real-time feature engineering\"\n",
    "\n",
    "\"Feature stores: What are they and why use them?\"\n",
    "\n",
    "\"Monitoring feature distributions in production\"\n",
    "\n",
    "\n",
    "Automated Feature Engineering (FeatureTools, AutoFeat)\n",
    "\n",
    "Deep Feature Synthesis\n",
    "\n",
    "Neural Network embeddings as features\n",
    "\n",
    "Transfer learning features (using pre-trained models)\n",
    "\n",
    "s\n",
    "Pitfalls to Avoid\n",
    "Target leakage (using future information)\n",
    "\n",
    "Overfitting during feature engineering\n",
    "\n",
    "Not considering computational cost\n",
    "\n",
    "Ignoring feature interpretability\n",
    "\n",
    "Not validating on holdout set\n",
    "\n",
    "Best Practices\n",
    "Always split data before any feature engineering\n",
    "\n",
    "Use cross-validation for feature selection\n",
    "\n",
    "Document every feature's source and logic\n",
    "\n",
    "Monitor feature drift in production\n",
    "\n",
    "Start simple, then add complexity\n",
    "\n",
    "\n",
    "\"How do you use scikit-learn pipelines for feature engineering?\"\n",
    "\n",
    "\"Feature engineering in TensorFlow vs PyTorch\"\n",
    "\n",
    "\"Spark for large-scale feature engineering\"\n",
    "\n",
    "Handle missing values\n",
    "\n",
    "Encode categorical variables\n",
    "\n",
    "Scale/normalize numerical features\n",
    "\n",
    "Handle outliers\n",
    "\n",
    "Create interaction features\n",
    "\n",
    "Generate polynomial features\n",
    "\n",
    "Extract datetime components\n",
    "\n",
    "Create aggregate features\n",
    "\n",
    "Apply transformations (log, sqrt)\n",
    "\n",
    "Reduce dimensionality if needed\n",
    "\n",
    "Split data before any engineering\n",
    "\n",
    "Validate feature importance\n",
    "\n",
    "Document all features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
