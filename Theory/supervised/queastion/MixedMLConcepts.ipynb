{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07a2f9f",
   "metadata": {},
   "source": [
    "One-Hot Encoding\n",
    "What: Converting categorical variables to binary vectors\n",
    "\n",
    "Why: Most ML algorithms can't handle categorical data directly\n",
    "\n",
    "Example: Colors [Red, Green, Blue] → Red: [1,0,0], Green: [0,1,0], Blue: [0,0,1]\n",
    "\n",
    "Dummy Variable Trap: Why remove one column? (Multi-collinearity)\n",
    "\n",
    "Alternatives: Label Encoding, Target Encoding, Binary Encoding\n",
    "\n",
    "2. ROC Curve & AUC\n",
    "ROC (Receiver Operating Characteristic): Plot of TPR vs FPR at various thresholds\n",
    "\n",
    "AUC (Area Under Curve): Probability classifier ranks positive higher than negative\n",
    "\n",
    "Perfect classifier: AUC = 1.0\n",
    "\n",
    "Random classifier: AUC = 0.5 (diagonal line)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "AUC = 0.9: Excellent\n",
    "\n",
    "AUC = 0.8: Good\n",
    "\n",
    "AUC = 0.7: Fair\n",
    "\n",
    "AUC = 0.6: Poor\n",
    "\n",
    "3. ADALINE (Adaptive Linear Neuron)\n",
    "Invented: By Bernard Widrow & Ted Hoff (1960)\n",
    "\n",
    "Difference from Perceptron:\n",
    "\n",
    "Perceptron: Step activation, binary output\n",
    "\n",
    "ADALINE: Linear activation, continuous output\n",
    "\n",
    "Cost Function: Mean Squared Error (MSE)\n",
    "\n",
    "Learning Rule: Widrow-Hoff rule (delta rule, LMS algorithm)\n",
    "\n",
    "Update: w←w+η(y−y')x\n",
    "\n",
    "4. Bias in Machine Learning\n",
    "Three Types of Bias:\n",
    "Statistical Bias: E[θ']−θ (Difference between expected estimate and true value)\n",
    "\n",
    "Algorithmic Bias: Systematic errors from model assumptions\n",
    "\n",
    "Social Bias: Unfair discrimination in predictions\n",
    "\n",
    "Bias-Variance Tradeoff:\n",
    "High Bias: Underfitting (oversimplified model)\n",
    "\n",
    "High Variance: Overfitting (too complex, sensitive to noise)\n",
    "\n",
    "Total Error = Bias² + Variance + Irreducible Error\n",
    "\n",
    "5. MADALINE (Multiple ADALINE)\n",
    "Extension: Network of ADALINE units\n",
    "\n",
    "Architecture: Multiple adaptive linear neurons\n",
    "\n",
    "Training: MRII (Multiple-Input, Multiple-Output) algorithm\n",
    "\n",
    "Application: Early telephone echo cancellation\n",
    "\n",
    "6. NCC (Nearest Centroid Classifier)\n",
    "Also called: Rocchio classifier, simplified k-NN\n",
    "\n",
    "How it works: Classify based on nearest class mean (centroid)\n",
    "\n",
    "Distance metric: Usually Euclidean\n",
    "\n",
    "Advantages: Simple, fast training\n",
    "\n",
    "Disadvantages: Sensitive to feature scaling, assumes spherical clusters\n",
    "\n",
    "7. F1 Score\n",
    "Formula: F1=2× Precision*Recall/Precision×Recall\n",
    "Harmonic mean of precision and recall\n",
    "\n",
    "Use case: When balance between precision and recall is important\n",
    "\n",
    "Best: 1.0, Worst: 0.0\n",
    "\n",
    "Fβ score: Generalization where β controls recall importance\n",
    "\n",
    "8. Overfitting\n",
    "Definition: Model learns training data noise/patterns that don't generalize\n",
    "\n",
    "Symptoms:\n",
    "\n",
    "Training accuracy >> Test accuracy\n",
    "\n",
    "Complex decision boundaries\n",
    "\n",
    "High variance\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Regularization (L1/L2)\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "Early stopping\n",
    "\n",
    "Dropout (neural networks)\n",
    "\n",
    "Pruning (decision trees)\n",
    "\n",
    "More training data\n",
    "\n",
    "Recall (Sensitivity)\n",
    "Formula: \n",
    "Recall= TP/TP+FN\n",
    "Interpretation: What proportion of actual positives did we catch?\n",
    "\n",
    "Also called: Sensitivity, True Positive Rate (TPR)\n",
    "\n",
    "Use case: Important in medical diagnosis, fraud detection\n",
    "\n",
    "Trade-off: With precision (precision-recall curve)\n",
    "\n",
    "10. Underfitting\n",
    "Definition: Model too simple to capture data patterns\n",
    "\n",
    "Symptoms:\n",
    "\n",
    "Poor performance on both training and test\n",
    "\n",
    "High bias\n",
    "\n",
    "Simple decision boundaries\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Add more features\n",
    "\n",
    "Use more complex model\n",
    "\n",
    "Reduce regularization\n",
    "\n",
    "Train longer (for neural networks)\n",
    "\n",
    "Feature engineering\n",
    "\n",
    "11. Perceptron\n",
    "Invented: By Frank Rosenblatt (1957)\n",
    "\n",
    "First artificial neuron\n",
    "\n",
    "Architecture: Input layer → weights → summation → step function\n",
    "\n",
    "Learning Rule: Perceptron Learning Algorithm (PLA)\n",
    "\n",
    "Update:  w←w+η(y−y')x (only when wrong)\n",
    "\n",
    "Limitation: Can't learn XOR (linear separability required)\n",
    "\n",
    "Perceptron Convergence Theorem: Guaranteed convergence for linearly separable data\n",
    "\n",
    "\n",
    "When would you use one-hot encoding vs. label encoding?\"\n",
    "\n",
    "\"Explain the bias-variance tradeoff using a practical example.\"\n",
    "\n",
    "\"How does ADALINE differ from Perceptron mathematically?\"\n",
    "\n",
    "\"What does an AUC of 0.8 mean in practical terms?\"\n",
    "\n",
    "\"How would you detect and fix overfitting in a neural network?\"\n",
    "\n",
    "\"When is recall more important than precision?\"\n",
    "\n",
    "\"Why can't a single-layer perceptron solve XOR?\"\n",
    "\n",
    "Historical Context Timeline\n",
    "1957: Perceptron (Rosenblatt)\n",
    "\n",
    "1960: ADALINE/MADALINE (Widrow & Hoff)\n",
    "\n",
    "1967: Nearest Centroid (Rocchio)\n",
    "\n",
    "1970s: ROC analysis in signal detection\n",
    "\n",
    "1990s: F1 Score becomes popular in IR\n",
    "\n",
    "2000s: One-Hot Encoding standard in ML\n",
    "\n",
    " Modern Alternatives\n",
    "Instead of One-Hot: Embeddings, Feature Hashing\n",
    "\n",
    "Beyond ROC: Precision-Recall curves (better for imbalance)\n",
    "\n",
    "Beyond Perceptron: Multilayer perceptrons, deep learning\n",
    "\n",
    "Beyond NCC: k-NN with k>1, SVMs\n",
    "\n",
    "\n",
    "One-Hot Encoding high-cardinality features (curse of dimensionality)\n",
    "\n",
    "Using accuracy with imbalanced data (use F1/AUC instead)\n",
    "\n",
    "Confusing statistical bias with social bias\n",
    "\n",
    "Thinking ADALINE solves XOR (it doesn't - still linear)\n",
    "\n",
    "Using F1 without understanding precision-recall tradeoff\n",
    "\n",
    "\n",
    "​\n",
    "\n",
    "\n",
    "\n",
    "​\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
