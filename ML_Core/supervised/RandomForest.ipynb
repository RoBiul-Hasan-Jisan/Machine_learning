{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d30a548",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "A **Random Forest Regressor** is an ensemble learning method that combines multiple Decision Trees to produce more accurate and stable predictions for regression tasks. It operates on the principle of \"wisdom of crowds\" - many weak learners (trees) together form a strong learner.\n",
    "\n",
    "\n",
    "\n",
    "### Core Components\n",
    "* **Multiple Decision Trees:** Base estimators that make individual predictions.\n",
    "* **Bootstrap Aggregating (Bagging):** Training each tree on a random subset of data.\n",
    "* **Feature Randomness:** Considering only a random subset of features at each split.\n",
    "* **Averaging Predictions:** The final output is the **average** of all individual tree predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## How Random Forest Regression Works\n",
    "\n",
    "### Training Process\n",
    "1.  **Create Bootstrap Samples:** Draw random samples *with replacement* from the original training data.\n",
    "2.  **Build Decision Trees:** For each bootstrap sample, grow a decision tree. Crucially, at each split, the tree considers only a random subset of features (not all features).\n",
    "3.  **Parallel Training:** All trees are trained independently and usually in parallel.\n",
    "4.  **Aggregate Results:** Combine predictions from all trees by calculating the mean.\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize\n",
    "# n_estimators: Number of trees (default is 100)\n",
    "# random_state: Ensures reproducibility\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train\n",
    "# rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict (Returns continuous values)\n",
    "# predictions = rf_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest prediction\n",
    "def predict_random_forest(X):\n",
    "    predictions = []\n",
    "    for tree in forest:\n",
    "        pred = tree.predict(X)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # For regression: average all predictions\n",
    "    final_prediction = np.mean(predictions)\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6ea10",
   "metadata": {},
   "source": [
    "### 1. Bootstrap Aggregating (Bagging)\n",
    "* **Sampling:** Creates multiple datasets by sampling *with replacement*.\n",
    "* **Data Distribution:** Each tree sees $\\approx 63.2\\%$ of the original data (some samples are repeated).\n",
    "* **OOB:** The remaining $36.8\\%$ form **\"Out-of-Bag\" (OOB)** samples, which are used for internal validation.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Feature Randomness\n",
    "* **The Logic:** At each split, the algorithm considers only a random subset (`max_features`) of the total features.\n",
    "* **Typical Values:** $\\sqrt{\\text{n\\_features}}$ (common for classification) or $\\log_2(\\text{n\\_features})$ (common for regression).\n",
    "* **Goal:** Reduces the **correlation** between trees, ensuring they don't all look the same.\n",
    "\n",
    "### 3. Variance Reduction Formula\n",
    "The overall variance of the Random Forest is reduced compared to single trees according to this approximation:\n",
    "\n",
    "$$Var(RF) = \\rho \\cdot \\sigma^2 + \\frac{(1 - \\rho) \\cdot \\sigma^2}{M}$$\n",
    "\n",
    "* **$\\rho$ (rho):** Correlation between trees (we want this low).\n",
    "* **$\\sigma^2$ (sigma squared):** Variance of individual trees.\n",
    "* **$M$:** Number of trees in the forest (as $M$ increases, the second term vanishes).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Hyperparameters\n",
    "\n",
    "```python\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    max_depth=None,         # Maximum tree depth\n",
    "    min_samples_split=2,    # Minimum samples to split a node\n",
    "    min_samples_leaf=1,     # Minimum samples at a leaf node\n",
    "    max_features='auto',    # Features to consider for split\n",
    "    bootstrap=True,         # Use bootstrap samples\n",
    "    oob_score=False,        # Use out-of-bag samples for scoring\n",
    "    random_state=42         # Seed for reproducibility\n",
    ")\n",
    "```\n",
    "### Types of Feature Importance\n",
    "\n",
    "1.  **Gini Importance:** Based on the total **impurity reduction** brought by a feature.\n",
    "2.  **Permutation Importance:** Measures the drop in model performance (accuracy or $R^2$) when a specific feature is randomly shuffled (breaking its relationship with the target).\n",
    "3.  **Mean Decrease Impurity:** The average impurity decrease calculated across all trees in the forest.\n",
    "\n",
    "\n",
    "\n",
    "### When to Use Random Forest Regressor\n",
    "\n",
    "| Criteria | Description |\n",
    "| :--- | :--- |\n",
    "| **Good for** | Tabular data, mixed data types, non-linear relationships. |\n",
    "| **Use when** | Interpretability is somewhat important (via feature importance) but you need higher accuracy than a single tree. |\n",
    "| **Ideal for** | Medium-sized datasets ($1,000$ - $100,000$ samples). |\n",
    "\n",
    "---\n",
    "\n",
    "### FAQ: Random Forest Regressor\n",
    "\n",
    "#### 1. How does Random Forest reduce overfitting compared to a single Decision Tree?\n",
    "It uses two main mechanisms to lower variance:\n",
    "* **Bagging:** Each tree trains on a different random subset of data, so no single tree sees the whole picture.\n",
    "* **Feature Randomness:** Trees are forced to de-correlate by considering only a random subset of features at each split, preventing them from all relying on the same dominant features.\n",
    "\n",
    "#### 2. What is the Out-of-Bag (OOB) score and why is it useful?\n",
    "The OOB score is computed using the $\\approx 37\\%$ of data that is **not** included in the bootstrap sample for a specific tree.\n",
    "* **Built-in Cross-Validation:** It acts as a validation set without needing to manually split the data.\n",
    "* **Unbiased Estimate:** It provides a reliable estimate of the generalization error.\n",
    "* **Tuning:** It is extremely useful for tuning hyperparameters efficiently.\n",
    "\n",
    "#### 3. How do you interpret feature importance in Random Forest?\n",
    "* It represents the **average decrease in impurity** contributed by that feature across all trees.\n",
    "* **Higher importance** = Greater contribution to the model's predictive power.\n",
    "* *Note:* It should be interpreted relatively (Feature A is 2x more important than Feature B), not absolutely.\n",
    "\n",
    "#### 4. When would you choose Random Forest over Gradient Boosting?\n",
    "Choose Random Forest when:\n",
    "* **Speed:** You need faster training times (RF trains in parallel; Boosting is sequential).\n",
    "* **Simplicity:** You want a model that works well \"out of the box\" with less hyperparameter tuning.\n",
    "* **Noise:** You are dealing with very noisy data (RF is generally more robust to outliers).\n",
    "* **Validation:** You want to use OOB scores for quick validation.\n",
    "\n",
    "#### 5. What are the limitations of Random Forest?\n",
    "* **Computational Cost:** Can be slow and memory-intensive for very large datasets.\n",
    "* **Extrapolation:** Poor at predicting values outside the range of the training data (a limitation of all tree-based models).\n",
    "* **Black Box:** While feature importance helps, it is harder to interpret the specific decision path compared to a single Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856082c",
   "metadata": {},
   "source": [
    "# Random Forest: Classifier vs. Regressor\n",
    "\n",
    "Just like the single Decision Tree, the Random Forest ensemble adapts its logic based on whether your target variable is a **Category** or a **Number**.\n",
    "\n",
    "### 1. Random Forest Classifier\n",
    "**Use this when:** Your target variable is **Categorical** (Classes/Labels).\n",
    "\n",
    "\n",
    "\n",
    "* **The Logic:** \"Majority Vote.\"\n",
    "    * Every tree in the forest makes a prediction (e.g., Tree 1: \"Red\", Tree 2: \"Blue\", Tree 3: \"Red\").\n",
    "    * The forest counts the votes.\n",
    "    * The class with the most votes wins.\n",
    "* **Output:** A Class Label (or probability).\n",
    "* **Evaluation Metrics:** Accuracy, Confusion Matrix, ROC-AUC.\n",
    "\n",
    "**Example:**\n",
    "* **Fraud Detection:** 100 trees look at a transaction. 90 say \"Legit\", 10 say \"Fraud\". The model predicts **Legit**.\n",
    "\n",
    "### 2. Random Forest Regressor\n",
    "**Use this when:** Your target variable is **Continuous** (Numerical Values).\n",
    "\n",
    "* **The Logic:** \"Averaging.\"\n",
    "    * Every tree in the forest predicts a specific value (e.g., Tree 1: 100, Tree 2: 105, Tree 3: 95).\n",
    "    * The forest calculates the average (mean) of all these predictions.\n",
    "* **Output:** A continuous number.\n",
    "* **Evaluation Metrics:** MSE, MAE, RMSE, $R^2$ Score.\n",
    "\n",
    "**Example:**\n",
    "* **House Price:** 100 trees predict the price. The average of all 100 predictions is taken as the final price.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | Random Forest **Classifier** | Random Forest **Regressor** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Target Type** | **Categories** (Discrete) | **Numbers** (Continuous) |\n",
    "| **Prediction Method** | Majority Voting (Mode) | Averaging (Mean) |\n",
    "| **Criterion** | Gini Impurity, Entropy | Squared Error (MSE), Absolute Error |\n",
    "| **Python Class** | `RandomForestClassifier` | `RandomForestRegressor` |\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SCENARIO 1: CLASSIFICATION (Predicting Churn)\n",
    "# -------------------------------------------------------\n",
    "# y target: [0, 1, 0, 0, 1] (Classes)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "# clf.fit(X_train, y_train)\n",
    "# prediction = clf.predict(X_test)  # Returns 0 or 1\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SCENARIO 2: REGRESSION (Predicting Temperature)\n",
    "# -------------------------------------------------------\n",
    "# y target: [23.5, 24.1, 19.8] (Continuous)\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=100, criterion='squared_error')\n",
    "# reg.fit(X_train, y_train)\n",
    "# prediction = reg.predict(X_test)  # Returns e.g., 22.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Create and train the model\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.4f}, R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Example\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c27c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.25395595827057227\n",
      "R2 Score: 0.8062009933635497\n",
      "Feature Importances: [0.52588625 0.05435475 0.04445027 0.02960746 0.03069355 0.13805542\n",
      " 0.08864622 0.08830608]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"MSE:\", mean_squared_error(Y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(Y_test, y_pred))\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80db20d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Housing Dataset Overview:\n",
      "Shape: (1000, 22)\n",
      "\n",
      "First 5 rows:\n",
      "   area_sqft  bedrooms  bathrooms  stories  year_built   latitude   longitude  \\\n",
      "0       2098         2        1.5        2        1965  37.627162 -122.278365   \n",
      "1       1717         2        1.0        1        1996  37.503511 -122.404276   \n",
      "2       2188         3        1.0        1        2008  37.652853 -121.968664   \n",
      "3       2713         3        1.5        2        1980  37.620724 -122.493994   \n",
      "4       1659         1        2.5        1        1953  37.514959 -121.970554   \n",
      "\n",
      "   distance_city_center  school_rating  has_garage  ...  has_garden  \\\n",
      "0              8.244515              3           1  ...           1   \n",
      "1              2.203316              4           1  ...           1   \n",
      "2              0.532070              5           1  ...           0   \n",
      "3             16.336162              5           1  ...           1   \n",
      "4              5.286421              2           1  ...           1   \n",
      "\n",
      "   has_fireplace  crime_rate  population_density  median_income_neighborhood  \\\n",
      "0              1    7.799807         6085.419862                56205.813997   \n",
      "1              1    9.768496         6086.249631                71013.716052   \n",
      "2              0    2.241657         6800.325467                67927.887517   \n",
      "3              0    1.493919         3461.695991                85943.464301   \n",
      "4              1    5.254967         8654.437451                76551.571617   \n",
      "\n",
      "   lot_size_sqft  condition  energy_efficiency  property_type   price  \n",
      "0           3440          5                  C          Condo  684922  \n",
      "1           2329          5                  C          Condo  936348  \n",
      "2           6965          2                  B  Single Family  869265  \n",
      "3           3077          4                  E          Condo  850866  \n",
      "4           4152          1                  B  Single Family  861775  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 22 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   area_sqft                   1000 non-null   int64  \n",
      " 1   bedrooms                    1000 non-null   int64  \n",
      " 2   bathrooms                   1000 non-null   float64\n",
      " 3   stories                     1000 non-null   int64  \n",
      " 4   year_built                  1000 non-null   int32  \n",
      " 5   latitude                    1000 non-null   float64\n",
      " 6   longitude                   1000 non-null   float64\n",
      " 7   distance_city_center        1000 non-null   float64\n",
      " 8   school_rating               1000 non-null   int64  \n",
      " 9   has_garage                  1000 non-null   int64  \n",
      " 10  garage_cars                 1000 non-null   int64  \n",
      " 11  has_pool                    1000 non-null   int64  \n",
      " 12  has_garden                  1000 non-null   int64  \n",
      " 13  has_fireplace               1000 non-null   int64  \n",
      " 14  crime_rate                  1000 non-null   float64\n",
      " 15  population_density          1000 non-null   float64\n",
      " 16  median_income_neighborhood  1000 non-null   float64\n",
      " 17  lot_size_sqft               1000 non-null   int64  \n",
      " 18  condition                   1000 non-null   int64  \n",
      " 19  energy_efficiency           1000 non-null   object \n",
      " 20  property_type               1000 non-null   object \n",
      " 21  price                       1000 non-null   int64  \n",
      "dtypes: float64(7), int32(1), int64(12), object(2)\n",
      "memory usage: 168.1+ KB\n",
      "None\n",
      "\n",
      "Basic Statistics:\n",
      "         area_sqft     bedrooms    bathrooms      stories  year_built  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.00000   \n",
      "mean   1811.391000     2.857000     2.021500     1.435000  1985.53000   \n",
      "std     586.640433     1.023523     0.693375     0.578022    20.98937   \n",
      "min     144.000000     1.000000     1.000000     1.000000  1950.00000   \n",
      "25%    1411.250000     2.000000     1.500000     1.000000  1967.00000   \n",
      "50%    1814.500000     3.000000     2.000000     1.000000  1986.00000   \n",
      "75%    2188.250000     3.000000     2.500000     2.000000  2004.00000   \n",
      "max    4111.000000     5.000000     4.000000     3.000000  2022.00000   \n",
      "\n",
      "          latitude    longitude  distance_city_center  school_rating  \\\n",
      "count  1000.000000  1000.000000           1000.000000    1000.000000   \n",
      "mean     37.694454  -122.199329              5.188091       3.375000   \n",
      "std       0.117728     0.170568              4.998183       1.079602   \n",
      "min      37.500251  -122.499239              0.027066       1.000000   \n",
      "25%      37.590113  -122.339337              1.568644       3.000000   \n",
      "50%      37.694081  -122.195224              3.721620       4.000000   \n",
      "75%      37.793781  -122.057739              7.383789       4.000000   \n",
      "max      37.899620  -121.900090             41.298671       5.000000   \n",
      "\n",
      "        has_garage  garage_cars     has_pool   has_garden  has_fireplace  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000     1000.00000   \n",
      "mean      0.806000     1.016000     0.310000     0.707000        0.58400   \n",
      "std       0.395627     0.945853     0.462725     0.455366        0.49314   \n",
      "min       0.000000     0.000000     0.000000     0.000000        0.00000   \n",
      "25%       1.000000     0.000000     0.000000     0.000000        0.00000   \n",
      "50%       1.000000     1.000000     0.000000     1.000000        1.00000   \n",
      "75%       1.000000     2.000000     1.000000     1.000000        1.00000   \n",
      "max       1.000000     3.000000     1.000000     1.000000        1.00000   \n",
      "\n",
      "        crime_rate  population_density  median_income_neighborhood  \\\n",
      "count  1000.000000         1000.000000                 1000.000000   \n",
      "mean      5.121668         4972.017698                74892.032932   \n",
      "std       2.831634         1986.286544                25179.832441   \n",
      "min       0.187344         -221.186006                  181.090031   \n",
      "25%       2.664404         3573.370482                57455.277699   \n",
      "50%       5.132761         4932.686017                75360.691953   \n",
      "75%       7.541839         6325.410371                91000.284622   \n",
      "max       9.998937        11058.185983               157939.574770   \n",
      "\n",
      "       lot_size_sqft    condition         price  \n",
      "count    1000.000000  1000.000000  1.000000e+03  \n",
      "mean     5034.478000     3.510000  8.677543e+05  \n",
      "std      1911.588297     1.008423  1.439037e+05  \n",
      "min        40.000000     1.000000  3.744210e+05  \n",
      "25%      3750.000000     3.000000  7.695272e+05  \n",
      "50%      5012.000000     4.000000  8.628400e+05  \n",
      "75%      6356.250000     4.000000  9.654608e+05  \n",
      "max     11218.000000     5.000000  1.301920e+06  \n",
      "\n",
      "Dataset saved as 'housing_data.csv'\n",
      "\n",
      "Top 10 features correlated with price:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic housing data\n",
    "data = {\n",
    "    # Basic property features\n",
    "    'area_sqft': np.random.normal(1800, 600, n_samples).astype(int),\n",
    "    'bedrooms': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.1, 0.25, 0.4, 0.2, 0.05]),\n",
    "    'bathrooms': np.random.choice([1, 1.5, 2, 2.5, 3, 3.5, 4], n_samples, p=[0.15, 0.2, 0.3, 0.2, 0.1, 0.04, 0.01]),\n",
    "    'stories': np.random.choice([1, 2, 3], n_samples, p=[0.6, 0.35, 0.05]),\n",
    "    'year_built': np.random.randint(1950, 2023, n_samples),\n",
    "    \n",
    "    # Location features\n",
    "    'latitude': np.random.uniform(37.5, 37.9, n_samples),\n",
    "    'longitude': np.random.uniform(-122.5, -121.9, n_samples),\n",
    "    'distance_city_center': np.random.exponential(5, n_samples),\n",
    "    'school_rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.15, 0.3, 0.35, 0.15]),\n",
    "    \n",
    "    # Property amenities\n",
    "    'has_garage': np.random.choice([0, 1], n_samples, p=[0.2, 0.8]),\n",
    "    'garage_cars': np.random.choice([0, 1, 2, 3], n_samples, p=[0.2, 0.4, 0.3, 0.1]),\n",
    "    'has_pool': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
    "    'has_garden': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "    'has_fireplace': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
    "    \n",
    "    # Neighborhood features\n",
    "    'crime_rate': np.random.uniform(0.1, 10.0, n_samples),\n",
    "    'population_density': np.random.normal(5000, 2000, n_samples),\n",
    "    'median_income_neighborhood': np.random.normal(75000, 25000, n_samples),\n",
    "    \n",
    "    # Additional features\n",
    "    'lot_size_sqft': np.random.normal(5000, 2000, n_samples).astype(int),\n",
    "    'condition': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.1, 0.3, 0.4, 0.15]),\n",
    "    'energy_efficiency': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_samples, p=[0.1, 0.3, 0.4, 0.15, 0.05]),\n",
    "    'property_type': np.random.choice(['Apartment', 'Townhouse', 'Single Family', 'Condo'], \n",
    "                                    n_samples, p=[0.3, 0.2, 0.4, 0.1])\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure some logical constraints\n",
    "df['area_sqft'] = np.abs(df['area_sqft'])  # No negative areas\n",
    "df['bedrooms'] = np.maximum(1, df['bedrooms'])  # At least 1 bedroom\n",
    "df['bathrooms'] = np.maximum(1, df['bathrooms'])  # At least 1 bathroom\n",
    "df['year_built'] = np.minimum(2023, df['year_built'])  # No future years\n",
    "df['lot_size_sqft'] = np.abs(df['lot_size_sqft'])  # No negative lot sizes\n",
    "\n",
    "# If no garage, set garage_cars to 0\n",
    "df.loc[df['has_garage'] == 0, 'garage_cars'] = 0\n",
    "\n",
    "# Generate realistic housing prices based on features with some noise\n",
    "base_price = 300000  # Base price in dollars\n",
    "\n",
    "# Calculate price based on features\n",
    "price = (\n",
    "    base_price +\n",
    "    df['area_sqft'] * 150 +  # $150 per sqft\n",
    "    df['bedrooms'] * 25000 +  # $25k per bedroom\n",
    "    df['bathrooms'] * 30000 +  # $30k per bathroom\n",
    "    df['stories'] * 20000 +  # $20k per story\n",
    "    (2023 - df['year_built']) * -500 +  # Newer houses more expensive\n",
    "    df['school_rating'] * 15000 +  # $15k per school rating point\n",
    "    df['has_garage'] * 25000 +  # $25k for garage\n",
    "    df['garage_cars'] * 10000 +  # $10k per garage car space\n",
    "    df['has_pool'] * 35000 +  # $35k for pool\n",
    "    df['has_garden'] * 15000 +  # $15k for garden\n",
    "    df['has_fireplace'] * 8000 +  # $8k for fireplace\n",
    "    df['condition'] * 12000 +  # $12k per condition point\n",
    "    -df['crime_rate'] * 3000 +  # Lower crime = higher price\n",
    "    -df['distance_city_center'] * 8000 +  # Closer to city = higher price\n",
    "    df['median_income_neighborhood'] * 0.5  # Neighborhood wealth effect\n",
    ")\n",
    "\n",
    "# Add property type premium\n",
    "property_type_premium = {\n",
    "    'Apartment': 0,\n",
    "    'Townhouse': 20000,\n",
    "    'Single Family': 50000,\n",
    "    'Condo': -10000\n",
    "}\n",
    "df['property_type_premium'] = df['property_type'].map(property_type_premium)\n",
    "price += df['property_type_premium']\n",
    "\n",
    "# Add energy efficiency premium\n",
    "energy_efficiency_premium = {\n",
    "    'A': 15000,\n",
    "    'B': 8000,\n",
    "    'C': 0,\n",
    "    'D': -5000,\n",
    "    'E': -10000\n",
    "}\n",
    "df['energy_efficiency_premium'] = df['energy_efficiency'].map(energy_efficiency_premium)\n",
    "price += df['energy_efficiency_premium']\n",
    "\n",
    "# Add some random noise (10% of price)\n",
    "noise = np.random.normal(0, 0.1 * price.mean(), n_samples)\n",
    "price += noise\n",
    "\n",
    "# Ensure prices are realistic (no negative prices)\n",
    "price = np.maximum(150000, price)\n",
    "\n",
    "# Add price to DataFrame\n",
    "df['price'] = price.astype(int)\n",
    "\n",
    "# Drop temporary columns\n",
    "df = df.drop(['property_type_premium', 'energy_efficiency_premium'], axis=1)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Housing Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('housing_data.csv', index=False)\n",
    "print(f\"\\nDataset saved as 'housing_data.csv'\")\n",
    "\n",
    "# Display correlation with price\n",
    "print(\"\\nTop 10 features correlated with price:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677dcfa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
