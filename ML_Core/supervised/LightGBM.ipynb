{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29140e91",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "**LightGBM** stands for **Light Gradient Boosting Machine**. It is a gradient boosting framework developed by Microsoft that is designed to be:\n",
    "\n",
    "* **Faster** than other GBM implementations\n",
    "* **More memory efficient**\n",
    "* **Able to handle large-scale data**\n",
    "\n",
    "Like XGBoost, it is an ensemble method that uses decision trees, but it employs several novel techniques to achieve its performance advantages.\n",
    "\n",
    "### Key Philosophy: Why be \"Light\"?\n",
    "Traditional boosting algorithms (including XGBoost's default) grow trees **level-wise** (horizontally). LightGBM grows trees **leaf-wise** (vertically), which is more efficient but can lead to overfitting on small datasets if not properly regularized.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## How it Works\n",
    "\n",
    "LightGBM shares the same fundamental gradient boosting framework with XGBoost. The objective function is similar:\n",
    "\n",
    "$$\n",
    "Obj(\\theta) = \\sum_{i} L(y_i, \\hat{y}_i) + \\sum_{k} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Where the regularization term $\\Omega(f_k)$ also penalizes the number of leaves and the L2 norm of leaf weights. However, its key differentiators lie in **how** it builds the trees.\n",
    "\n",
    "### 1. Gradient-Based One-Side Sampling (GOSS)\n",
    "**Idea:** Not all data points are equally important for boosting. Data points with larger gradients (i.e., those that are poorly predicted) contribute more to the information gain.\n",
    "\n",
    "\n",
    "\n",
    "**How it works:**\n",
    "1.  **Sort** the training instances by the absolute value of their gradients.\n",
    "2.  **Keep** the top $a\\%$ of instances with the largest gradients.\n",
    "3.  **Randomly sample** $b\\%$ from the remaining instances (those with small gradients).\n",
    "4.  When computing the gain for splits, GOSS **amplifies the weight** of the sampled data with small gradients by a constant factor $\\frac{1-a}{b}$.\n",
    "\n",
    "> **Result:** This allows LightGBM to focus computational resources on the \"difficult\" examples while still maintaining the original data distribution, leading to faster training with minimal accuracy loss.\n",
    "\n",
    "### 2. Exclusive Feature Bundling (EFB)\n",
    "**Idea:** In high-dimensional data, many features are sparse and often mutually exclusive (they never take non-zero values simultaneously). EFB bundles these features together to reduce dimensionality.\n",
    "\n",
    "**How it works:**\n",
    "1.  **Identify Exclusive Features:** Find features that rarely take non-zero values simultaneously.\n",
    "2.  **Bundle Them:** Merge these features into a single \"bundled\" feature.\n",
    "\n",
    "> **Result:** The number of features is significantly reduced, speeding up the training process without losing much information. This is particularly effective for **one-hot encoded categorical features**.\n",
    "\n",
    "### 3. Leaf-Wise (Best-First) Tree Growth\n",
    "This is the most significant difference from XGBoost's default approach.\n",
    "\n",
    "* **XGBoost (Level-Wise):** Grows the tree level by level. At each level, all leaves are split simultaneously. This can be inefficient as it may split even leaves that contribute little to the loss reduction.\n",
    "* **LightGBM (Leaf-Wise):** At each step, it identifies the leaf that will yield the **largest reduction in the loss** and splits only that leaf. This results in much deeper trees for the same number of leaves and often achieves lower loss.\n",
    "\n",
    "> **Trade-off:** Leaf-wise growth is more prone to overfitting, especially on small datasets. This is why LightGBM has important regularization parameters like `num_leaves` and `min_data_in_leaf`.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: LightGBM vs. XGBoost\n",
    "\n",
    "| Feature | LightGBM | XGBoost |\n",
    "| :--- | :--- | :--- |\n",
    "| **Tree Growth** | **Leaf-wise** (Best-first) | **Level-wise** (Depth-wise) |\n",
    "| **Speed** | **Faster** | Slower (relatively) |\n",
    "| **Memory Usage** | **Lower** | Higher |\n",
    "| **Large Data** | Excellent | Good |\n",
    "| **Categorical Features** | **Native support** (no one-hot needed) | Requires one-hot encoding or preprocessing |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Choose Which?\n",
    "\n",
    "### Choose LightGBM when:\n",
    "* You have **very large datasets** (millions of rows).\n",
    "* **Training time** is critical.\n",
    "* You have limited computational resources (memory).\n",
    "* Your data has **many categorical features**.\n",
    "* You're dealing with high-dimensional data.\n",
    "\n",
    "### Choose XGBoost when:\n",
    "* Your dataset is **small to medium-sized**.\n",
    "* You want maximum performance and are willing to wait.\n",
    "* You need extremely robust results (less prone to overfitting on small data).\n",
    "* You want fine-grained control over the training process.\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Concept: Leaf-Wise Growth\n",
    "\n",
    "```text\n",
    "       Root\n",
    "      /    \\\n",
    "  Leaf A   Leaf B  <- LightGBM splits here if gain is max\n",
    "          /     \\\n",
    "      Leaf C   Leaf D\n",
    "\n",
    "\n",
    "```\n",
    "- CatBoost Regressor → numeric output\n",
    "\n",
    "- CatBoost Classifier → class probability output\n",
    "\n",
    "- Core boosting logic is almost identical\n",
    "\n",
    "- Only loss function and output mapping changes\n",
    "\n",
    "## Key Difference\n",
    "\n",
    "In regression, CatBoost predicts numbers.\n",
    "\n",
    "In classification, CatBoost predicts probabilities (0 to 1 for binary, or softmax for multi-class).\n",
    "\n",
    "Internally, the tree structure, ordered boosting, and categorical handling are almost identical.\n",
    "\n",
    "Only the loss function and gradient calculation change between Regressor and Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202cc0c",
   "metadata": {},
   "source": [
    "# LightGBM Regressor vs. Classifier\n",
    "\n",
    "Just like CatBoost, LightGBM shares the same underlying engine (leaf-wise growth, histogram binning) for both tasks, but optimizes for different targets.\n",
    "\n",
    "## 1. Problem Type & Output\n",
    "\n",
    "| Feature | LightGBM Classifier | LightGBM Regressor |\n",
    "| :--- | :--- | :--- |\n",
    "| **Problem Domain** | Classification problems | Regression problems |\n",
    "| **Goal** | Predicts class labels or probabilities | Predicts continuous numerical values |\n",
    "| **Output Example** | `0`/`1` or `[0.3, 0.7]` | `125.7`, `-2.45` |\n",
    "\n",
    "## 2. Objectives & Metrics\n",
    "\n",
    "These are the specific parameters (`objective` and `metric`) used in LightGBM configuration.\n",
    "\n",
    "| Feature | Classifier | Regressor |\n",
    "| :--- | :--- | :--- |\n",
    "| **Objective Functions** | `binary` (Binary classification)<br>`multiclass` (Multi-class)<br>`cross_entropy` | `regression` (L2 loss/MSE)<br>`regression_l1` (MAE)<br>`huber` (Huber loss)<br>`fair` (Fair loss) |\n",
    "| **Evaluation Metrics** | `auc`<br>`binary_logloss`<br>`multi_logloss`<br>`error` (Classification error) | `l2` (MSE), `l1` (MAE)<br>`rmse`<br>`mape` |\n",
    "\n",
    "## 3. When to Choose Which?\n",
    "\n",
    "### Use LightGBM Classifier when:\n",
    "* Predicting categories (e.g., Yes/No, Spam/Not Spam, Customer Churn).\n",
    "* You need **probability outputs** to assess risk or confidence.\n",
    "* You are optimizing for classification metrics like **Accuracy, Precision, Recall, or AUC**.\n",
    "\n",
    "### Use LightGBM Regressor when:\n",
    "* Predicting **continuous numerical values**.\n",
    "* Forecasting quantities (e.g., Sales volume, Stock prices, Temperature).\n",
    "* You are optimizing for regression metrics like **RMSE, MAE, or R²**.\n",
    "\n",
    "## 4. Key Differences\n",
    "\n",
    "* **Prediction Output:**\n",
    "    * **Regression:** Predicts numbers directly.\n",
    "    * **Classification:** Predicts probabilities using **Sigmoid** (binary) or **Softmax** (multi-class), often mapped to a class label via a threshold.\n",
    "* **Internal Mechanics:**\n",
    "    * The core algorithms—**Tree-building, Leaf-wise growth, Histogram binning, and Categorical handling**—remain **identical**.\n",
    "    * Only the **Loss Function** (how error is calculated) and **Output Mapping** differ.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **LightGBM Regressor** $\\rightarrow$ Numeric output.\n",
    "* **LightGBM Classifier** $\\rightarrow$ Probability output.\n",
    "* **Core Logic** $\\rightarrow$ Identical tree-building.\n",
    "* **Difference** $\\rightarrow$ Loss function & Output transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046df962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class LightGBMTree:\n",
    "    def __init__(self, max_depth=3, min_samples_leaf=20, num_bins=16):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.num_bins = num_bins\n",
    "        self.tree = None\n",
    "\n",
    "    def _bin_data(self, X):\n",
    "        bins = []\n",
    "        X_binned = np.zeros_like(X, dtype=int)\n",
    "\n",
    "        for j in range(X.shape[1]):\n",
    "            col = X[:, j]\n",
    "            edges = np.linspace(col.min(), col.max(), self.num_bins + 1)\n",
    "            X_binned[:, j] = np.digitize(col, edges) - 1\n",
    "            bins.append(edges)\n",
    "\n",
    "        return X_binned, bins\n",
    "\n",
    "    def _best_split(self, X, grad, hess):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_bin = None\n",
    "\n",
    "        for j in range(X.shape[1]):\n",
    "            for b in range(self.num_bins):\n",
    "                left = (X[:, j] <= b)\n",
    "                right = ~left\n",
    "\n",
    "                if left.sum() < self.min_samples_leaf or right.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                G_left, H_left = grad[left].sum(), hess[left].sum()\n",
    "                G_right, H_right = grad[right].sum(), hess[right].sum()\n",
    "\n",
    "                gain = (G_left**2 / (H_left + 1e-6)) + (G_right**2 / (H_right + 1e-6))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = j\n",
    "                    best_bin = b\n",
    "\n",
    "        return best_feature, best_bin\n",
    "\n",
    "    def _build(self, X, grad, hess, depth):\n",
    "        if depth == self.max_depth:\n",
    "            return {\"leaf\": -grad.sum() / (hess.sum() + 1e-6)}\n",
    "\n",
    "        feature, split_bin = self._best_split(X, grad, hess)\n",
    "\n",
    "        if feature is None:\n",
    "            return {\"leaf\": -grad.sum() / (hess.sum() + 1e-6)}\n",
    "\n",
    "        left = (X[:, feature] <= split_bin)\n",
    "        right = ~left\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"bin\": split_bin,\n",
    "            \"left\": self._build(X[left], grad[left], hess[left], depth + 1),\n",
    "            \"right\": self._build(X[right], grad[right], hess[right], depth + 1),\n",
    "        }\n",
    "\n",
    "    def fit(self, X, grad, hess):\n",
    "        X_binned, self.bins = self._bin_data(X)\n",
    "        self.tree = self._build(X_binned, grad, hess, depth=0)\n",
    "\n",
    "    def _predict_row(self, row, node):\n",
    "        if \"leaf\" in node:\n",
    "            return node[\"leaf\"]\n",
    "\n",
    "        feature = node[\"feature\"]\n",
    "        bin_value = np.digitize(row[feature], self.bins[feature]) - 1\n",
    "\n",
    "        if bin_value <= node[\"bin\"]:\n",
    "            return self._predict_row(row, node[\"left\"])\n",
    "        else:\n",
    "            return self._predict_row(row, node[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(x, self.tree) for x in X])\n",
    "\n",
    "\n",
    "\n",
    "# Gradient Boosting using the custom LightGBM tree\n",
    "\n",
    "\n",
    "class LightGBMRegressorCore:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pred = np.zeros_like(y, dtype=float)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            grad = pred - y\n",
    "            hess = np.ones_like(y)\n",
    "\n",
    "            tree = LightGBMTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, grad, hess)\n",
    "\n",
    "            update = tree.predict(X)\n",
    "            pred -= self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for tree in self.trees:\n",
    "            pred -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we can do this (Classifier):\n",
    "from lightgbm import LGBMClassifier\n",
    "model_clf = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    categorical_feature=['category_col'],\n",
    "    metric='auc'\n",
    ")\n",
    "\n",
    "# Then you automatically know this (Regressor):\n",
    "from lightgbm import LGBMRegressor\n",
    "model_reg = LGBMRegressor(\n",
    "    n_estimators=1000,           # Same\n",
    "    num_leaves=31,               # Same  \n",
    "    learning_rate=0.05,          # Same\n",
    "    categorical_feature=['category_col'],  # Same\n",
    "    metric='rmse'                # Only this changes!\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
