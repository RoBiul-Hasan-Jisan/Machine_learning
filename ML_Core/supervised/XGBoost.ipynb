{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84be5ba",
   "metadata": {},
   "source": [
    "# XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "XGBoost stands for **eXtreme Gradient Boosting**.\n",
    "* **Gradient Boosting:** It belongs to the family of ensemble learning methods, specifically using the Gradient Boosting framework.\n",
    "* **Ensemble Learning:** The core idea is to combine multiple \"weak learners\" (typically decision trees) to create a single \"strong learner.\" The model \"learns\" from its previous mistakes.\n",
    "\n",
    "\n",
    "\n",
    "### Analogy: The Student Learning from Mistakes\n",
    "Imagine a student preparing for an exam:\n",
    "1.  **First Try (Tree 1):** The student takes a practice test and gets a score. They analyze their mistakes.\n",
    "2.  **Second Try (Tree 2):** They focus *specifically* on the topics they got wrong in the first test.\n",
    "3.  **Repeat (Subsequent Trees):** They keep taking new tests, each time paying extra attention to the errors made in the previous test.\n",
    "4.  **Final Exam (Prediction):** For the final exam, the student's knowledge is the sum of all the incremental learning from each practice test.\n",
    "\n",
    "---\n",
    "\n",
    "## How it Works: Step-by-Step\n",
    "\n",
    "### 1. The High-Level Idea: Sequential Ensemble\n",
    "Unlike Random Forest which builds trees in parallel, XGBoost builds trees **sequentially**. Each new tree is trained to correct the residual errors made by the combination of all previous trees.\n",
    "\n",
    "### 2. The Mathematical Formulation\n",
    "\n",
    "#### a) The Model\n",
    "The final prediction for a data point $i$ after $K$ trees is:\n",
    "\n",
    "$$\\hat{y}_i = \\phi(x_i) = \\sum_{k=1}^{K} f_k(x_i)$$\n",
    "\n",
    "* $\\hat{y}_i$: Final predicted value.\n",
    "* $x_i$: Feature vector for data point $i$.\n",
    "* $f_k$: The prediction from the $k$-th decision tree.\n",
    "\n",
    "#### b) The Objective Function\n",
    "This is the heart of XGBoost. The goal is to minimize:\n",
    "\n",
    "$$Obj(\\theta) = \\sum_i L(y_i, \\hat{y}_i) + \\sum_k \\Omega(f_k)$$\n",
    "\n",
    "This function has two critical parts:\n",
    "1.  **Training Loss ($L$):** Measures how well our model fits the training data (e.g., Squared Error $(y_i - \\hat{y}_i)^2$).\n",
    "2.  **Regularization Term ($\\Omega$):** Penalizes model complexity to prevent overfitting.\n",
    "    $$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda ||w||^2$$\n",
    "    * $T$: Number of leaves in the tree.\n",
    "    * $w$: The score (prediction value) on each leaf.\n",
    "    * $\\gamma$ (gamma): Minimum loss reduction required to make a split.\n",
    "    * $\\lambda$ (lambda): L2 regularization term on leaf weights.\n",
    "\n",
    "### 3. The Learning Process: Additive Training\n",
    "We start with an initial prediction (e.g., mean value).\n",
    "\n",
    "* **Step 0:** $\\hat{y}_i^{(0)} = 0$\n",
    "* **Step 1:** Build tree $f_1$ to minimize the objective. $\\hat{y}_i^{(1)} = \\hat{y}_i^{(0)} + f_1(x_i)$\n",
    "* **Step t:** Add tree $f_t$ that improves the model the most.\n",
    "    $$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)$$\n",
    "\n",
    "---\n",
    "\n",
    "## How is the Next Tree Built? (Gradient Boosting)\n",
    "\n",
    "We use **Gradient Descent** in function space. We don't have parameters like weights in a neural network; our \"parameters\" are the trees themselves.\n",
    "\n",
    "**1. Compute Gradients:**\n",
    "For a given loss function (e.g., Squared Loss), we calculate the gradient ($g_i$) and Hessian ($h_i$) for each data point with respect to the previous prediction.\n",
    "* $g_i = \\partial L / \\partial \\hat{y} = -2(y_i - \\hat{y}_i)$ (First derivative)\n",
    "* $h_i = \\partial^2 L / \\partial \\hat{y}^2 = 2$ (Second derivative)\n",
    "\n",
    "**2. Find Optimal Leaf Weights:**\n",
    "Once the tree structure is fixed, the optimal weight $w_j^*$ for leaf $j$ is calculated analytically:\n",
    "\n",
    "$$w_j^* = - \\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$$\n",
    "\n",
    "**3. Calculate the Objective Reduction (The \"Gain\"):**\n",
    "The algorithm decides where to split the tree by maximizing this **Gain** formula:\n",
    "\n",
    "$$Gain = \\frac{1}{2} \\left[ \\frac{(\\sum_{L} g_i)^2}{\\sum_{L} h_i + \\lambda} + \\frac{(\\sum_{R} g_i)^2}{\\sum_{R} h_i + \\lambda} - \\frac{(\\sum_{P} g_i)^2}{\\sum_{P} h_i + \\lambda} \\right] - \\gamma$$\n",
    "\n",
    "> **Key Insight:** Notice the $-\\gamma$ at the end. This means the split must improve the loss by at least $\\gamma$ to be considered valid. This is XGBoost's built-in pruning mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Why XGBoost Wins\n",
    "\n",
    "| Feature | Meaning |\n",
    "| :--- | :--- |\n",
    "| **Regularization (L1 & L2)** | Avoids overfitting (unlike standard Gradient Boosting). |\n",
    "| **Parallel Tree Building** | Uses all CPU cores during the split-finding phase. |\n",
    "| **Missing Values** | Automatically learns the best direction for missing data. |\n",
    "| **Tree Pruning** | Uses \"max_depth\" parameter and prunes backwards. |\n",
    "| **Shrinkage (`eta`)** | Scales down the weights of new trees to prevent overfitting. |\n",
    "\n",
    "### When to Use\n",
    "* **Target ($y$):** Continuous (Regression) or Categories (Classification).\n",
    "* **Features ($X$):** Numeric (mostly), but handles sparse data well.\n",
    "* **Performance:** When accuracy is the #1 priority (Kaggle competitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192df720",
   "metadata": {},
   "source": [
    "# XGBoost: Regressor vs. Classifier\n",
    "\n",
    "Just like Random Forest and Decision Trees, XGBoost uses the same underlying tree-boosting framework but adapts its **Objective Function**, **Loss Function**, and **Output** depending on the task.\n",
    "\n",
    "### 1. Problem Type & Output\n",
    "\n",
    "| Feature | XGBoost **Classifier** | XGBoost **Regressor** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Problem Type** | Classification (Categories) | Regression (Continuous Numbers) |\n",
    "| **Prediction Goal** | Class labels or Probabilities | Continuous quantities |\n",
    "| **Example Output** | `0` or `1`, `[0.2, 0.8]` | `250000`, `125.7`, `-2.45` |\n",
    "| **Python Class** | `XGBClassifier` | `XGBRegressor` |\n",
    "\n",
    "### 2. Objectives & Loss Functions\n",
    "\n",
    "| Feature | XGBoost **Classifier** | XGBoost **Regressor** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Default Objective** | `binary:logistic` (Binary)<br>`multi:softmax` (Multiclass) | `reg:squarederror` (Default)<br>`reg:absoluteerror` |\n",
    "| **Loss Function** | Log Loss (Binary Cross-Entropy) | MSE (Mean Squared Error), MAE |\n",
    "| **Mathematical Goal** | Maximize Likelihood | Minimize Error Distance |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. When to Use Which?\n",
    "\n",
    "#### Use XGBoost **Classifier** when:\n",
    "* Predicting categories (e.g., Spam/Ham, Fraud/Not Fraud, Customer Churn).\n",
    "* You need probability outputs (e.g., `predict_proba()` to get \"80% chance of rain\").\n",
    "* You are evaluating with classification metrics: **Accuracy, Precision, Recall, F1-Score, AUC-ROC**.\n",
    "\n",
    "#### Use XGBoost **Regressor** when:\n",
    "* Predicting continuous numerical values (e.g., House Prices, Stock Value, Temperature).\n",
    "* Forecasting quantities (e.g., \"How many units will we sell next month?\").\n",
    "* You are evaluating with regression metrics: **RMSE (Root Mean Squared Error), MAE, $R^2$ Score**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Deep Dive: Key Technical Differences\n",
    "\n",
    "| Component | Regressor | Classifier |\n",
    "| :--- | :--- | :--- |\n",
    "| **Target Variable ($y$)** | Numeric (Continuous) | Categorical (Discrete) |\n",
    "| **Output Transformation** | Raw Number (Identity) | **Sigmoid** (Binary) or **Softmax** (Multiclass) |\n",
    "| **Gradient Calculation** | Simple difference: $\\text{pred} - \\text{target}$ | Computed using derivative of Log Loss |\n",
    "| **Hessian Calculation** | Constant ($2$ for MSE) | $p(1-p)$ (where $p$ is probability) |\n",
    "\n",
    "> **Summary:**\n",
    "> * **XGBoost Regressor** $\\rightarrow$ Numeric prediction, Regression Loss.\n",
    "> * **XGBoost Classifier** $\\rightarrow$ Probability prediction, Classification Loss.\n",
    "> * **The Core:** The tree-building and boosting framework is **identical**. To switch from Regressor to Classifier, XGBoost simply changes the **Loss Function**, the **Gradient/Hessian** formulas, and the final **Output Mapping**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e19179",
   "metadata": {},
   "source": [
    "# The Math: How XGBoost Switches Modes\n",
    "\n",
    "XGBoost uses a **Second-Order Taylor Expansion** to approximate the loss function. This means for *every* split decision, it needs two numbers for every data point:\n",
    "1.  **Gradient ($g_i$):** The \"slope\" (First Derivative of Loss).\n",
    "2.  **Hessian ($h_i$):** The \"curvature\" (Second Derivative of Loss).\n",
    "\n",
    "$$Obj \\approx \\sum_{i=1}^{n} [L(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\Omega(f_t)$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1. XGBoost Regressor (Linear Regression)\n",
    "**Goal:** Minimize the distance between prediction and target.\n",
    "\n",
    "* **Loss Function:** Mean Squared Error (MSE)\n",
    "    $$L(\\theta) = \\frac{1}{2} (y_i - \\hat{y}_i)^2$$\n",
    "    *(Note: The 1/2 is added to make the derivative cleaner)*\n",
    "\n",
    "* **Step 1: The Gradient ($g_i$)**\n",
    "    Take the first derivative with respect to the prediction $\\hat{y}$:\n",
    "    $$g_i = \\frac{\\partial L}{\\partial \\hat{y}} = (\\hat{y}_i - y_i)$$\n",
    "    *Interpretation:* The error (Residual).\n",
    "\n",
    "* **Step 2: The Hessian ($h_i$)**\n",
    "    Take the derivative of the gradient (Second derivative):\n",
    "    $$h_i = \\frac{\\partial^2 L}{\\partial \\hat{y}^2} = 1$$\n",
    "    *Interpretation:* The curvature is constant for squared error.\n",
    "\n",
    "* **Final Output:**\n",
    "    The leaf weights are calculated directly using these values. The output is a raw number (e.g., $250,000$).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. XGBoost Classifier (Binary Logistic)\n",
    "**Goal:** Minimize the probability error (Log Loss).\n",
    "\n",
    "* **Transformation:** The model outputs a \"log-odds\" score ($z$), which is converted to a probability ($p$) using the Sigmoid function:\n",
    "    $$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "* **Loss Function:** Binary Cross-Entropy (Log Loss)\n",
    "    $$L(\\theta) = -[y_i \\ln(p_i) + (1 - y_i) \\ln(1 - p_i)]$$\n",
    "\n",
    "* **Step 1: The Gradient ($g_i$)**\n",
    "    This derivation is complex, but the result is elegant:\n",
    "    $$g_i = \\frac{\\partial L}{\\partial z} = p_i - y_i$$\n",
    "    *Interpretation:* The difference between the predicted probability and the actual class (0 or 1).\n",
    "\n",
    "* **Step 2: The Hessian ($h_i$)**\n",
    "    The second derivative depends on the probability itself:\n",
    "    $$h_i = \\frac{\\partial^2 L}{\\partial z^2} = p_i (1 - p_i)$$\n",
    "    *Interpretation:* The curvature is highest when $p \\approx 0.5$ (uncertainty is high) and lowest when $p$ is near 0 or 1.\n",
    "\n",
    "* **Final Output:**\n",
    "    The tree outputs a \"log-odds\" value. To get the final probability, we apply the Sigmoid function to the sum of all tree outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Comparison Table\n",
    "\n",
    "| Component | **Regressor** (MSE) | **Classifier** (Log Loss) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Loss Function ($L$)** | $\\frac{1}{2}(y - \\hat{y})^2$ | $-[y \\ln(p) + (1-y) \\ln(1-p)]$ |\n",
    "| **Gradient ($g_i$)** | $\\hat{y} - y$ | $p - y$ |\n",
    "| **Hessian ($h_i$)** | $1$ (Constant) | $p(1-p)$ (Variable) |\n",
    "| **Leaf Weight Formula** | $w^* = \\frac{-\\sum (\\hat{y}-y)}{\\sum 1 + \\lambda}$ | $w^* = \\frac{-\\sum (p-y)}{\\sum p(1-p) + \\lambda}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11cec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function (Squared Error)\n",
    "def squared_error_grad_hess(y_true,y_pred):\n",
    "    # gradient: d/dy_pred (1/2 * (y - y_pred)^2) = y_pred - y\n",
    "    g = y_pred - y_true\n",
    "    # hessian: second derivative = 1\n",
    "    h = 1.0 \n",
    "    return g,h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38478963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of a Tree Node\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  # leaf weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b09b65",
   "metadata": {},
   "source": [
    "Compute Leaf Weight (XGBoost Formula)\n",
    "ùë§  = ‚àí ùê∫/ùêª+ùúÜ\n",
    "‚Äã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6360dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_leaf_weight(G, H, lambda_):\n",
    "    return - G / (H + lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8026b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gain of a Split\n",
    "def compute_gain(GL, HL, GR, HR, lambda_, gamma):\n",
    "    parent_gain = (GL + GR)**2 / (HL + HR + lambda_)\n",
    "    left_gain = GL**2 / (HL + lambda_)\n",
    "    right_gain = GR**2 / (HR + lambda_)\n",
    "    return 0.5 * (left_gain + right_gain - parent_gain) - gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fa120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#  OBJECTIVE (Squared Error)\n",
    "\n",
    "def squared_error_grad_hess(y_true, y_pred):\n",
    "    g = y_pred - y_true  # gradient\n",
    "    h = np.ones_like(y_true)  # hessian (constant)\n",
    "    return g, h\n",
    "\n",
    "\n",
    "#  TREE NODE\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  # leaf weight\n",
    "\n",
    "\n",
    "#  LEAF WEIGHT FORMULA\n",
    "#  w = -G / (H + Œª)\n",
    "\n",
    "def compute_leaf_weight(G, H, lambda_):\n",
    "    return -G / (H + lambda_)\n",
    "\n",
    "\n",
    "#  XGBOOST GAIN FORMULA\n",
    "\n",
    "def compute_gain(GL, HL, GR, HR, lambda_, gamma):\n",
    "    parent = (GL + GR)**2 / (HL + HR + lambda_)\n",
    "    left = GL**2 / (HL + lambda_)\n",
    "    right = GR**2 / (HR + lambda_)\n",
    "    return 0.5 * (left + right - parent) - gamma\n",
    "\n",
    "\n",
    "#  FIND BEST SPLIT\n",
    "\n",
    "def find_best_split(X, g, h, lambda_, gamma):\n",
    "    best_gain = -float(\"inf\")\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    for feature in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "\n",
    "        for t in thresholds:\n",
    "            left_idx = X[:, feature] <= t\n",
    "            right_idx = ~left_idx\n",
    "\n",
    "            if left_idx.sum() == 0 or right_idx.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            GL, HL = g[left_idx].sum(), h[left_idx].sum()\n",
    "            GR, HR = g[right_idx].sum(), h[right_idx].sum()\n",
    "\n",
    "            gain = compute_gain(GL, HL, GR, HR, lambda_, gamma)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = t\n",
    "                \n",
    "    return best_feature, best_threshold, best_gain\n",
    "\n",
    "\n",
    "#  BUILD TREE (Recursive)\n",
    "\n",
    "def build_tree(X, g, h, depth, max_depth, lambda_, gamma):\n",
    "    if depth == max_depth:\n",
    "        return Node(value=compute_leaf_weight(g.sum(), h.sum(), lambda_))\n",
    "\n",
    "    feature, threshold, gain = find_best_split(X, g, h, lambda_, gamma)\n",
    "\n",
    "    if (feature is None) or (gain < 0):\n",
    "        return Node(value=compute_leaf_weight(g.sum(), h.sum(), lambda_))\n",
    "\n",
    "    left_idx = X[:, feature] <= threshold\n",
    "    right_idx = ~left_idx\n",
    "\n",
    "    left_child = build_tree(X[left_idx], g[left_idx], h[left_idx], depth+1, max_depth, lambda_, gamma)\n",
    "    right_child = build_tree(X[right_idx], g[right_idx], h[right_idx], depth+1, max_depth, lambda_, gamma)\n",
    "\n",
    "    return Node(feature, threshold, left_child, right_child)\n",
    "\n",
    "\n",
    "#  PREDICT WITH ONE TREE\n",
    "\n",
    "def predict_tree(node, x):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return predict_tree(node.left, x)\n",
    "    else:\n",
    "        return predict_tree(node.right, x)\n",
    "\n",
    "\n",
    "#  XGBOOST REGRESSOR CORE\n",
    "\n",
    "class XGBoostRegressorCore:\n",
    "    def __init__(self, n_estimators=50, learning_rate=0.1, max_depth=3, lambda_=1, gamma=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n = len(y)\n",
    "        y_pred = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            g = y_pred - y  # gradient\n",
    "            h = np.ones(n)  # hessian\n",
    "\n",
    "            tree = build_tree(X, g, h, 0, self.max_depth, self.lambda_, self.gamma)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            update = np.array([predict_tree(tree, x) for x in X])\n",
    "            y_pred += self.learning_rate * update\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "\n",
    "        for tree in self.trees:\n",
    "            preds += self.learning_rate * np.array([predict_tree(tree, x) for x in X])\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537c112",
   "metadata": {},
   "source": [
    "XGBoost is a gradient boosting algorithm:\n",
    "\n",
    "Start with all predictions = 0\n",
    "\n",
    "Compute gradient (how wrong the model is)\n",
    "\n",
    "Build a decision tree that predicts the gradient\n",
    "\n",
    "Add this tree to the model\n",
    "\n",
    "Repeat for many trees\n",
    "\n",
    "Each tree fixes the errors of the previous trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we can do this (Classifier):\n",
    "from xgboost import XGBClassifier\n",
    "model_clf = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Then you automatically know this (Regressor):\n",
    "from xgboost import XGBRegressor\n",
    "model_reg = XGBRegressor(\n",
    "    n_estimators=1000,           # Same\n",
    "    learning_rate=0.05,          # Same  \n",
    "    max_depth=6,                 # Same\n",
    "    objective='reg:squarederror', # Only this changes!\n",
    "    eval_metric='rmse'           # And this changes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
