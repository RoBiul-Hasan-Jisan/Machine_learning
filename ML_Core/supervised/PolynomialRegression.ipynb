{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22afc6e",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "## 1. Definition & Goal\n",
    "\n",
    "A form of Linear Regression used when the relationship between the independent variable ($X$) and dependent variable ($Y$) is **non-linear** (curved).\n",
    "\n",
    "### The Hypothesis\n",
    "Although the curve is non-linear, the equation is **linear in parameters** (the weights $\\beta$ are still linear). We essentially \"trick\" the linear model by adding powers of the original features.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + ... + \\beta_nx^n + \\epsilon$$\n",
    "\n",
    "* **$n$ (Degree):** The power to which we raise the input.\n",
    "    * $n=1$: Linear (Straight line)\n",
    "    * $n=2$: Quadratic (Parabola)\n",
    "    * $n=3$: Cubic (S-shape)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How It Works \n",
    "\n",
    "The algorithm treats $x^2$ and $x^3$ as if they were distinct features ($x_2, x_3$) in a Multiple Linear Regression.\n",
    "\n",
    "**Example Transformation:**\n",
    "If Input $X = [2]$:\n",
    "1.  **Linear:** Features = $[2]$\n",
    "2.  **Polynomial (Degree 2):** Features = $[2, 4]$  *(since $2^2=4$)*\n",
    "3.  **Polynomial (Degree 3):** Features = $[2, 4, 8]$ *(since $2^3=8$)*\n",
    "\n",
    "Then, we solve using the standard Normal Equation or Gradient Descent:\n",
    "$$y = \\beta_0 + \\beta_1(2) + \\beta_2(4) + \\beta_3(8)$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Bias-Variance Tradeoff \n",
    "\n",
    "Choosing the correct \"Degree\" ($n$) is the hardest part of Polynomial Regression.\n",
    "\n",
    "| Degree | Result | Problem | Type |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Low ($n=1$)** | Straight line through curve | **Underfitting** (High Bias) | Too Simple |\n",
    "| **Optimal** | Follows the trend nicely | **Good Fit** | Just Right |\n",
    "| **High ($n=15$)** | Wiggles to hit every dot | **Overfitting** (High Variance) | Too Complex |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why is it still called \"Linear\" Regression?\n",
    "\n",
    "This is a common interview trick question.\n",
    "\n",
    "* **Answer:** Because the model calculates the weights ($\\beta$) linearly.\n",
    "* The equation $y = \\beta_0 + \\beta_1x + \\beta_2x^2$ is linear regarding $\\beta$.\n",
    "* We are just finding the best weighted sum of the inputs (even if the inputs are squared).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Challenges & Solutions\n",
    "\n",
    "### A. Extrapolation\n",
    "Polynomials are terrible at predicting values outside the training data range.\n",
    "* **Issue:** A high-degree polynomial will shoot up to positive or negative infinity rapidly as soon as $x$ moves slightly beyond the known data.\n",
    "\n",
    "### B. Overfitting (The Runge Phenomenon)\n",
    "As you increase the degree, the curve starts oscillating wildly at the edges.\n",
    "* **Solution 1: Cross-Validation:** Test different degrees ($n=2, 3, 4...$) and pick the one with the lowest validation error.\n",
    "* **Solution 2: Regularization:** Apply **Ridge (L2)** or **Lasso (L1)** regression to penalize large coefficients, keeping the curve smoother.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb564da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 6. Python Implementation (Scikit-Learn)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. Create a pipeline (Transform -> Fit)\n",
    "degree = 2\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "# 2. Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "predictions = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
