{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " logistic regression multi-dimensional data\n",
    " \n",
    " \n",
    " $$ F(X)=X \\times W $$\n",
    " $$ H(x)= \\frac{1}{1+ e ^{-F(x)}} $$\n",
    " $$ C= -\\frac{1}{n} \\sum_{i,j} (Y \\odot log(H(x)) + (1-Y) \\odot log(1-H(x)) ) $$\n",
    "\n",
    "$X_{n \\times k}$\n",
    "\n",
    "$W_{k \\times p}$\n",
    "\n",
    "$Y_{n \\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model used for **binary classification** (can be extended to multi-class). It predicts the **probability** that an instance belongs to a particular class.\n",
    "\n",
    "Despite its name, it is a **classification algorithm**, not regression. The \"regression\" part refers to using a regression-like approach to estimate probabilities.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Core Mathematical Concepts\n",
    "\n",
    "#### Logit Function (Log-Odds)\n",
    "The \"Logit\" is the natural log of the odds ratio. It maps probability values from $(0, 1)$ to $(-\\infty, +\\infty)$.\n",
    "\n",
    "$$\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$$\n",
    "\n",
    "* **$p$:** Probability of success (class = 1)\n",
    "* **$1-p$:** Probability of failure (class = 0)\n",
    "* **$\\ln(\\frac{p}{1-p})$:** Log-odds\n",
    "\n",
    "#### Sigmoid Function\n",
    "The Sigmoid function maps the output of the linear equation ($z$) back to a probability range $(0, 1)$.\n",
    "\n",
    "$$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "* **Where:** $z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$\n",
    "* **Properties:**\n",
    "    * **Output range:** $(0, 1)$\n",
    "    * **Shape:** S-shaped curve\n",
    "    * **Derivative:** $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Assumptions of Logistic Regression\n",
    "* **Binary Outcome:** The target variable is binary (for standard logistic regression).\n",
    "* **Linearity of Log-Odds:** There is a linear relationship between the independent variables and the log-odds of the target.\n",
    "* **No Multicollinearity:** Independent variables should not be highly correlated with each other.\n",
    "* **Independence:** Observations must be independent of each other.\n",
    "* **Large Sample Size:** Rule of thumb is at least 10 events per predictor variable.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Cost Function & Optimization\n",
    "\n",
    "#### Cost Function: Cross-Entropy Loss (Log Loss)\n",
    "We cannot use Mean Squared Error (MSE) because it creates a non-convex loss surface (many local minima). Instead, we use **Cross-Entropy**, which is convex.\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right]$$\n",
    "\n",
    "* $h_\\theta(x)$: Predicted probability\n",
    "* $y$: Actual label (0 or 1)\n",
    "* $m$: Number of samples\n",
    "\n",
    "#### Optimization: Gradient Descent\n",
    "We minimize the Cost Function using Gradient Descent or solvers like `liblinear`/`lbfgs`.\n",
    "\n",
    "**Update Rule:**\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "**Gradient Calculation:**\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Model Evaluation Metrics\n",
    "\n",
    "\n",
    "\n",
    "[Image of ROC curve classification]\n",
    "\n",
    "\n",
    "#### Classification Metrics\n",
    "* **Accuracy:** $\\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "* **Precision:** $\\frac{TP}{TP+FP}$ (Focus on minimizing False Positives)\n",
    "* **Recall (Sensitivity):** $\\frac{TP}{TP+FN}$ (Focus on minimizing False Negatives)\n",
    "* **F1-Score:** $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "* **Specificity:** $\\frac{TN}{TN+FP}$\n",
    "\n",
    "#### Probability & Calibration Metrics\n",
    "* **Log Loss:** Measures the uncertainty/quality of predicted probabilities.\n",
    "* **ROC-AUC:** Area Under the Receiver Operating Characteristic curve. Measures the model's ability to distinguish between classes.\n",
    "* **Precision-Recall AUC:** Better for highly imbalanced datasets.\n",
    "\n",
    "#### Threshold Selection\n",
    "Default threshold is **0.5**, but it should be tuned based on business needs:\n",
    "* Use **ROC Curve** to find the optimal balance.\n",
    "* Use **Youden's J statistic:** $J = \\text{Sensitivity} + \\text{Specificity} - 1$.\n",
    "* Consider the **Business Cost** of False Positives vs. False Negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Regularization (Preventing Overfitting)\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "Adds the absolute value of magnitude of coefficients as a penalty term.\n",
    "$$J(\\theta) = \\text{CrossEntropy} + \\lambda \\sum |\\theta_j|$$\n",
    "* **Effect:** Creates **sparse models** (Feature Selection). Can zero out coefficients entirely.\n",
    "\n",
    "#### L2 Regularization (Ridge)\n",
    "Adds the squared magnitude of coefficients as a penalty term.\n",
    "$$J(\\theta) = \\text{CrossEntropy} + \\lambda \\sum \\theta_j^2$$\n",
    "* **Effect:** Shrinks coefficients toward zero (but not exactly zero). Handles multicollinearity better.\n",
    "\n",
    "#### Elastic Net\n",
    "* A linear combination of **L1** and **L2** regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multiclass Logistic Regression\n",
    "\n",
    "1.  **One-vs-Rest (OvR):** Trains $k$ binary classifiers (e.g., Red vs. Not Red, Blue vs. Not Blue).\n",
    "2.  **Multinomial (Softmax) Regression:** Direct generalization using the Softmax function.\n",
    "    $$P(y=k|x) = \\frac{e^{\\theta_k \\cdot x}}{\\sum_{j} e^{\\theta_j \\cdot x}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Feature Importance & Interpretation\n",
    "\n",
    "#### Odds Ratio\n",
    "$$\\text{OR} = e^{\\beta}$$\n",
    "* **Interpretation:** For a 1-unit increase in $x$, the odds of the event happening increase by a factor of $e^{\\beta}$.\n",
    "\n",
    "#### Statistical Significance\n",
    "* **Wald Test:** $z = \\frac{\\beta}{SE(\\beta)}$ (Tests if coefficient $\\ne 0$).\n",
    "* **P-values:** Used to determine statistical significance of features.\n",
    "* **Confidence Intervals:** Defines the range for odds ratios.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Practical Considerations\n",
    "\n",
    "#### Handling Categorical Variables\n",
    "* Use **One-Hot Encoding** (Dummy Variables).\n",
    "* **Avoid Dummy Variable Trap:** Drop one category (k-1 dummies) to prevent perfect multicollinearity.\n",
    "* Use **Target/Mean Encoding** for high-cardinality features.\n",
    "\n",
    "#### Feature Scaling\n",
    "* **Not strictly required** for the core logic (unlike KNN).\n",
    "* **Highly Recommended** because:\n",
    "    1.  It helps Gradient Descent converge much faster.\n",
    "    2.  It is **Mandatory** if you are using Regularization (L1/L2), otherwise larger features will be penalized unfairly.\n",
    "\n",
    "#### Checking Assumptions\n",
    "* **Linearity:** Use Box-Tidwell test or residual plots.\n",
    "* **Influential Points:** Check Cook's Distance.\n",
    "* **Multicollinearity:** Check VIF (Variance Inflation Factor).\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Advantages vs. Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "| :--- | :--- |\n",
    "| Outputs probabilities (not just classes). | Assumes a linear decision boundary. |\n",
    "| Highly interpretable (Odds Ratios). | Sensitive to outliers. |\n",
    "| Efficient to train. | Requires feature engineering for non-linear relationships. |\n",
    "| Works well with small datasets. | Can underperform on complex patterns compared to Trees/NNs. |\n",
    "| Less prone to overfitting (with Regularization). | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Despite its name, **Logistic Regression** is a **Classification** algorithm, not a regression algorithm. It is used to predict the probability of a target variable belonging to a certain class (e.g., 0 or 1, Yes or No).\n",
    "\n",
    "\n",
    "\n",
    "### Core Concept: The Sigmoid Function\n",
    "Linear Regression fits a straight line that can go from $-\\infty$ to $+\\infty$. This doesn't work for probability (which must be between 0 and 1).\n",
    "\n",
    "Logistic Regression solves this by squashing the output of a linear equation into an **S-shaped curve** using the **Sigmoid Function**.\n",
    "\n",
    "* **Linear Equation:** $z = w \\cdot x + b$\n",
    "* **Sigmoid Transformation:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "**Result:** The output is always between **0 and 1**, representing the **probability** ($P$) that the event will happen.\n",
    "\n",
    "### How it Decides (Decision Boundary)\n",
    "Once the model calculates the probability, it applies a **threshold** (usually 0.5) to classify the data.\n",
    "\n",
    "* If $P \\ge 0.5 \\rightarrow$ Class 1 (e.g., \"Spam\")\n",
    "* If $P < 0.5 \\rightarrow$ Class 0 (e.g., \"Not Spam\")\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences: Linear vs. Logistic Regression\n",
    "\n",
    "| Feature | Linear Regression | Logistic Regression |\n",
    "| :--- | :--- | :--- |\n",
    "| **Purpose** | Predict continuous values (Price, Temp). | Predict categorical outcomes (Yes/No). |\n",
    "| **Output** | Any number ($-\\infty$ to $+\\infty$). | Probability ($0$ to $1$). |\n",
    "| **Curve** | Straight Line. | S-Shaped Curve (Sigmoid). |\n",
    "| **Cost Function** | Mean Squared Error (MSE). | **Log Loss** (Binary Cross Entropy). |\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Logistic Regression\n",
    "1.  **Binary:** Target has two possible outcomes (e.g., Pass/Fail).\n",
    "2.  **Multinomial:** Target has three or more nominal categories (e.g., Cat/Dog/Bird) â€“ uses the **Softmax** function.\n",
    "3.  **Ordinal:** Target has three or more ordinal categories (e.g., Low/Medium/High).\n",
    "\n",
    "### Assumptions\n",
    "* **Linearity of Log-Odds:** The independent variables are linearly related to the log-odds of the target.\n",
    "* **No Multicollinearity:** Independent variables should not be highly correlated with each other.\n",
    "* **Large Sample Size:** Generally requires a larger dataset than linear regression to achieve stable results.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, k, p=100, 8, 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.random([n,k])\n",
    "W=np.random.random([k,p])\n",
    "\n",
    "y=np.random.randint(p, size=(1,n))\n",
    "Y=np.zeros((n,p))\n",
    "Y[np.arange(n), y]=1\n",
    "\n",
    "max_itr=5000\n",
    "alpha=0.01\n",
    "Lambda=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient is as follows:\n",
    "$$ X^T (H(x)-Y) + \\lambda 2 W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x)= w[0]*x + w[1]\n",
    "def F(X, W):\n",
    "    return np.matmul(X,W)\n",
    "\n",
    "def H(F):\n",
    "    return 1/(1+np.exp(-F))\n",
    "\n",
    "def cost(Y_est, Y):\n",
    "    E= - (1/n) * (np.sum(Y*np.log(Y_est) + (1-Y)*np.log(1-Y_est)))  + np.linalg.norm(W,2)\n",
    "    return E, np.sum(np.argmax(Y_est,1)==y)/n\n",
    "\n",
    "def gradient(Y_est, Y, X):\n",
    "    return (1/n) * np.matmul(X.T, (Y_est - Y) ) + Lambda* 2* W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(W, X, Y, alpha, max_itr):\n",
    "    for i in range(max_itr):\n",
    "        \n",
    "        F_x=F(X,W)\n",
    "        Y_est=H(F_x)\n",
    "        E, c= cost(Y_est, Y)\n",
    "        Wg=gradient(Y_est, Y, X)\n",
    "        W=W - alpha * Wg\n",
    "        if i%1000==0:\n",
    "            print(E, c)\n",
    "        \n",
    "    return W, Y_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take into account for the biases, we concatenate X by a 1 column, and increase the number of rows in W by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.368653735228364 0.31\n",
      "4.994251188297815 0.43\n",
      "4.951873226767272 0.48\n",
      "4.922370610237865 0.47\n",
      "4.901694423284286 0.48\n"
     ]
    }
   ],
   "source": [
    "X=np.concatenate( (X, np.ones((n,1))), axis=1 ) \n",
    "W=np.concatenate( (W, np.random.random((1,p)) ), axis=0 )\n",
    "\n",
    "W, Y_est = fit(W, X, Y, alpha, max_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Create model with regularization\n",
    "model = LogisticRegression(\n",
    "    penalty='l2',           # Regularization type\n",
    "    C=1.0,                  # Inverse of regularization strength\n",
    "    solver='lbfgs',         # Optimization algorithm\n",
    "    max_iter=1000,          # Maximum iterations\n",
    "    class_weight='balanced' # Handle imbalanced data\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Interpret coefficients\n",
    "for feature, coef in zip(feature_names, model.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.4f} (OR: {np.exp(coef):.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
