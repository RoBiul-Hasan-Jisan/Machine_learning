{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " logistic regression multi-dimensional data\n",
    " \n",
    " \n",
    " $$ F(X)=X \\times W $$\n",
    " $$ H(x)= \\frac{1}{1+ e ^{-F(x)}} $$\n",
    " $$ C= -\\frac{1}{n} \\sum_{i,j} (Y \\odot log(H(x)) + (1-Y) \\odot log(1-H(x)) ) $$\n",
    "\n",
    "$X_{n \\times k}$\n",
    "\n",
    "$W_{k \\times p}$\n",
    "\n",
    "$Y_{n \\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical model used for binary classification (can be extended to multi-class). It predicts the probability that an instance belongs to a particular class.\n",
    "\n",
    "Despite its name, it's a classification algorithm, not regression. The \"regression\" part refers to using a regression-like approach to estimate probabilities.\n",
    "\n",
    "### Logit Function (Log-Odds)\n",
    "``` logit(p) = ln(p/(1-p)) = β₀ + β₁x₁ + ... + βₙxₙ  ```\n",
    "\n",
    "Where:\n",
    "\n",
    "p = probability of success (class = 1)\n",
    "\n",
    "1-p = probability of failure (class = 0)\n",
    "\n",
    "ln(p/(1-p)) = log-odds\n",
    "\n",
    "\n",
    "### Sigmoid Function\n",
    "```bash\n",
    "p = σ(z) = 1 / (1 + e^(-z))\n",
    "where z = β₀ + β₁x₁ + ... + βₙxₙ\n",
    "\n",
    "```\n",
    "\n",
    "Properties of Sigmoid:\n",
    "\n",
    "Output range: (0, 1)\n",
    "\n",
    "S-shaped curve\n",
    "\n",
    "Derivative: σ'(z) = σ(z)(1-σ(z))\n",
    "\n",
    "### Assumptions of Logistic Regression\n",
    "\n",
    "- Binary outcome variable (for standard logistic regression)\n",
    "\n",
    "- Linearity of independent variables and log odds (logit linearity)\n",
    "\n",
    "- No multicollinearity among independent variables\n",
    "\n",
    "- Independence of observations\n",
    "\n",
    "- Large sample size (rule of thumb: at least 10 events per predictor variable)\n",
    "\n",
    "###  Cost Function: Cross-Entropy Loss\n",
    "``` J(θ) = -1/m ∑ [y⁽ⁱ⁾ log(hθ(x⁽ⁱ⁾)) + (1-y⁽ⁱ⁾) log(1-hθ(x⁽ⁱ⁾))]  ```\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "hθ(x) = predicted probability\n",
    "\n",
    "y = actual label (0 or 1)\n",
    "\n",
    "m = number of samples\n",
    "\n",
    "Why not MSE? MSE would give non-convex loss surface, making optimization difficult.\n",
    "\n",
    "### Model Training: Optimization\n",
    "\n",
    "Maximum Likelihood Estimation (MLE)\n",
    "Find parameters that maximize likelihood of observing the data:\n",
    "``` L(θ) = ∏ hθ(x⁽ⁱ⁾)^y⁽ⁱ⁾ (1-hθ(x⁽ⁱ⁾))^(1-y⁽ⁱ⁾)  ```\n",
    "Maximizing likelihood = minimizing negative log-likelihood (cross-entropy)\n",
    "\n",
    "### Optimization Algorithms\n",
    "  Gradient Descent\n",
    "  ```bash  \n",
    "   θ_j := θ_j - α ∂J(θ)/∂θ_j\n",
    "   ∂J(θ)/∂θ_j = 1/m ∑ (hθ(x⁽ⁱ⁾) - y⁽ⁱ⁾) x_j⁽ⁱ⁾\n",
    "   ```\n",
    "\n",
    "\n",
    "### Model Evaluation Metrics\n",
    "\n",
    "Classification Metrics:\n",
    "Accuracy: (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "Precision: TP/(TP+FP)\n",
    "\n",
    "Recall/Sensitivity: TP/(TP+FN)\n",
    "\n",
    "F1-Score: 2(PrecisionRecall)/(Precision+Recall)\n",
    "\n",
    "Specificity: TN/(TN+FP)\n",
    "\n",
    "Probability Calibration Metrics:\n",
    "Log Loss: Measures quality of predicted probabilities\n",
    "\n",
    "ROC-AUC: Area under ROC curve (measures separability)\n",
    "\n",
    "Precision-Recall AUC: Better for imbalanced data\n",
    "\n",
    "\n",
    "### Threshold Selection\n",
    "Default threshold = 0.5, but can be tuned:\n",
    "\n",
    "Use ROC curve to find optimal threshold\n",
    "Consider business costs of FP vs FN\n",
    "Use Youden's J statistic: J = Sensitivity + Specificity - 1\n",
    "\n",
    "### Regularization in Logistic Regression\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "``` J(θ) = CrossEntropy + λ∑|θ_j| ```\n",
    "Creates sparse models (feature selection)\n",
    "\n",
    "Can zero out coefficients\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "``` J(θ) = CrossEntropy + λ∑θ_j² ```\n",
    "\n",
    "Shrinks coefficients toward zero\n",
    "\n",
    "Handles multicollinearity better\n",
    "\n",
    "### Elastic Net: Combination of L1 and L2\n",
    "\n",
    "\n",
    "## Multiclass Logistic Regression\n",
    "Two Approaches:\n",
    "One-vs-Rest (OvR): Train k binary classifiers\n",
    "\n",
    "Multinomial/Softmax Regression: Direct generalization\n",
    "``` P(y=k|x) = e^(θ_k·x) / ∑ e^(θ_j·x)  ```\n",
    "\n",
    "##  Feature Importance & Interpretation\n",
    "  ### Odds Ratio\n",
    "  ``` OR = e^β ```\n",
    "  Interpretation: For 1-unit increase in x, odds increase by factor of e^β\n",
    "\n",
    "### Statistical Significance\n",
    "Wald Test: z = β/SE(β) ~ N(0,1)\n",
    "\n",
    "p-values: Test if coefficient ≠ 0\n",
    "\n",
    "Confidence Intervals: CI for odds ratios\n",
    "\n",
    "- Handling Categorical Variables\n",
    "Use one-hot encoding (dummy variables)\n",
    "\n",
    "Avoid dummy variable trap (drop one category)\n",
    "\n",
    "Consider target/mean encoding for high-cardinality features\n",
    "\n",
    "- Feature Scaling\n",
    "Not required for logistic regression (unlike k-NN or SVM)\n",
    "\n",
    "But helps gradient descent converge faster\n",
    "\n",
    "Required for regularization to work properly\n",
    "\n",
    "- Checking Model Assumptions\n",
    "Linearity: Box-Tidwell test or residual plots\n",
    "\n",
    "Influential points: Cook's distance\n",
    "\n",
    "Multicollinearity: VIF (Variance Inflation Factor)\n",
    "\n",
    "##### Advantages:\n",
    "Outputs probabilities\n",
    "\n",
    "Interpretable (odds ratios)\n",
    "\n",
    "Efficient to train\n",
    "\n",
    "Works well with small datasets\n",
    "\n",
    "Less prone to overfitting with regularization\n",
    "\n",
    "##### Disadvantages:\n",
    "Assumes linear decision boundary\n",
    "\n",
    "Sensitive to outliers\n",
    "\n",
    "Requires feature engineering for non-linear relationships\n",
    "\n",
    "Can underperform with complex patterns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, k, p=100, 8, 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.random([n,k])\n",
    "W=np.random.random([k,p])\n",
    "\n",
    "y=np.random.randint(p, size=(1,n))\n",
    "Y=np.zeros((n,p))\n",
    "Y[np.arange(n), y]=1\n",
    "\n",
    "max_itr=5000\n",
    "alpha=0.01\n",
    "Lambda=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient is as follows:\n",
    "$$ X^T (H(x)-Y) + \\lambda 2 W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x)= w[0]*x + w[1]\n",
    "def F(X, W):\n",
    "    return np.matmul(X,W)\n",
    "\n",
    "def H(F):\n",
    "    return 1/(1+np.exp(-F))\n",
    "\n",
    "def cost(Y_est, Y):\n",
    "    E= - (1/n) * (np.sum(Y*np.log(Y_est) + (1-Y)*np.log(1-Y_est)))  + np.linalg.norm(W,2)\n",
    "    return E, np.sum(np.argmax(Y_est,1)==y)/n\n",
    "\n",
    "def gradient(Y_est, Y, X):\n",
    "    return (1/n) * np.matmul(X.T, (Y_est - Y) ) + Lambda* 2* W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(W, X, Y, alpha, max_itr):\n",
    "    for i in range(max_itr):\n",
    "        \n",
    "        F_x=F(X,W)\n",
    "        Y_est=H(F_x)\n",
    "        E, c= cost(Y_est, Y)\n",
    "        Wg=gradient(Y_est, Y, X)\n",
    "        W=W - alpha * Wg\n",
    "        if i%1000==0:\n",
    "            print(E, c)\n",
    "        \n",
    "    return W, Y_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take into account for the biases, we concatenate X by a 1 column, and increase the number of rows in W by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.368653735228364 0.31\n",
      "4.994251188297815 0.43\n",
      "4.951873226767272 0.48\n",
      "4.922370610237865 0.47\n",
      "4.901694423284286 0.48\n"
     ]
    }
   ],
   "source": [
    "X=np.concatenate( (X, np.ones((n,1))), axis=1 ) \n",
    "W=np.concatenate( (W, np.random.random((1,p)) ), axis=0 )\n",
    "\n",
    "W, Y_est = fit(W, X, Y, alpha, max_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Create model with regularization\n",
    "model = LogisticRegression(\n",
    "    penalty='l2',           # Regularization type\n",
    "    C=1.0,                  # Inverse of regularization strength\n",
    "    solver='lbfgs',         # Optimization algorithm\n",
    "    max_iter=1000,          # Maximum iterations\n",
    "    class_weight='balanced' # Handle imbalanced data\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# Interpret coefficients\n",
    "for feature, coef in zip(feature_names, model.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.4f} (OR: {np.exp(coef):.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
