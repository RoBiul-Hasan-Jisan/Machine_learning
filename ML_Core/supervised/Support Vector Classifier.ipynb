{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a0a2ef",
   "metadata": {},
   "source": [
    "# SVC (Support Vector Classifier)\n",
    "\n",
    "SVC is like drawing the widest possible street between different classes. It doesn't just separate points — it creates the maximum possible **margin** between classes.\n",
    "\n",
    "\n",
    "\n",
    "### Core Idea: The Widest Street Analogy\n",
    "Imagine you have:\n",
    "* **Red balls (Class A)** on the left\n",
    "* **Blue balls (Class B)** on the right\n",
    "\n",
    "**SVC's goal:** Draw a street (decision boundary) that:\n",
    "1.  Separates red from blue perfectly (or as well as possible).\n",
    "2.  Makes the street **as wide as possible**.\n",
    "3.  Puts \"guard rails\" (support vectors) at the edges.\n",
    "\n",
    "> **Support Vectors:** The data points closest to the boundary that \"support\" or define the street width. If you remove other points, the street stays the same. If you remove a support vector, the street moves.\n",
    "\n",
    "---\n",
    "\n",
    "### The Math Behind It\n",
    "The goal is to find the hyperplane that maximizes the margin.\n",
    "\n",
    "**Objective Function:**\n",
    "$$\\text{Minimize: } \\frac{1}{2} ||w||^2$$\n",
    "*(Minimizing the weight vector norm $||w||$ creates a wider margin)*\n",
    "\n",
    "**Subject to Constraints:**\n",
    "$$y_i (w \\cdot x_i + b) \\ge 1$$\n",
    "*(Ensures all points are correctly classified outside the street)*\n",
    "\n",
    "* **$w$:** Vector perpendicular to the decision boundary.\n",
    "* **$b$:** Bias term (offset).\n",
    "* **Support Vectors:** Points where $y_i (w \\cdot x_i + b) = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Linearly Separable Data\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create some separable data\n",
    "# Class 0: Lower left, Class 1: Upper right\n",
    "X = np.array([[1, 2], [2, 3], [2, 1], [3, 2],\n",
    "              [6, 5], [7, 6], [7, 4], [8, 5]])\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "```\n",
    "# Train SVC with a Linear Kernel\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "# The \"street\" is defined by the support vectors\n",
    "\n",
    "print(f\"Support vectors:\\n{model.support_vectors_}\")\n",
    "\n",
    "print(f\"Number of support vectors: {len(model.support_vectors_)}\")\n",
    "\n",
    "The Math Behind It:\n",
    "\n",
    "$$\\text{Minimize: } \\frac{1}{2} ||w||^2$$\n",
    "\n",
    "*(Minimizing the weight vector norm $||w||$ creates a wider margin)*\n",
    "\n",
    "Subject to: yᵢ(w·xᵢ + b) ≥ 1  (all points correctly classified)\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "w = perpendicular to the decision boundary\n",
    "\n",
    "||w|| = width of the margin (smaller w = wider street)\n",
    "\n",
    "Points with yᵢ(w·xᵢ + b) = 1 are support vectors\n",
    "\n",
    "\n",
    "\n",
    "The Magic Trick: Kernel Method\n",
    "Problem: Real data isn't always linearly separable in 2D.\n",
    "\n",
    "SVC's solution: \"Let's look at it from a different angle!\"\n",
    "```bash\n",
    " Before kernel trick (can't separate circles):\n",
    " • • •   • • •\n",
    " • • •   • • •\n",
    " • • •   • • •\n",
    "\n",
    " With kernel trick (project to 3D, now separable):\n",
    "    • • •           (higher up)\n",
    "   • • •           (middle height)\n",
    "  • • •           (lower down)\n",
    "\n",
    "\n",
    "```\n",
    "Common Kernels:\n",
    "```bash\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 1. Linear Kernel (for linearly separable data)\n",
    "svc_linear = SVC(kernel='linear')\n",
    "\n",
    "# 2. Polynomial Kernel (for curved boundaries)\n",
    "svc_poly = SVC(kernel='poly', degree=3)  # Degree controls curvature\n",
    "\n",
    "# 3. RBF (Radial Basis Function) Kernel - MOST COMMON\n",
    "#    Creates circular/oval boundaries, flexible\n",
    "svc_rbf = SVC(kernel='rbf', gamma='scale')\n",
    "\n",
    "# 4. Sigmoid Kernel (similar to neural network)\n",
    "svc_sigmoid = SVC(kernel='sigmoid')\n",
    "```\n",
    "\n",
    "\n",
    "### When to Use SVC:\n",
    "**GOOD For:**\n",
    "\n",
    "Small to medium datasets (scales poorly with huge data)\n",
    "\n",
    "High-dimensional data (text, images, genes - where features > samples)\n",
    "\n",
    "Clear margin of separation expected\n",
    "\n",
    "Binary classification problems\n",
    "\n",
    "When you need a robust boundary (less affected by outliers than other methods)\n",
    "\n",
    "**BAD For:**\n",
    "\n",
    "Very large datasets (slow training time, O(n²) to O(n³))\n",
    "\n",
    "Noisy data with overlapping classes (performs poorly)\n",
    "\n",
    "Multi-class problems (though sklearn handles it with \"one-vs-one\")\n",
    "\n",
    "When you need probability estimates (SVC doesn't naturally provide them)\n",
    "\n",
    "When interpretability is crucial (hard to explain kernel transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be961412",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
