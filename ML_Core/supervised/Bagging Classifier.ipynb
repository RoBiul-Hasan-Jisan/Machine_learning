{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafd1422",
   "metadata": {},
   "source": [
    "# Bagging Classifier\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)** is an ensemble learning technique that improves model stability and accuracy by combining multiple base estimators trained on random subsets of the data.\n",
    "\n",
    "**Core Concept:** Train multiple instances of the same algorithm on different data samples, then combine their predictions.\n",
    "\n",
    "\n",
    "\n",
    "### 1. How it Works\n",
    "\n",
    "#### A. Bootstrap Sampling\n",
    "* Creates multiple datasets by sampling **with replacement** from the original training data.\n",
    "* Each bootstrap sample contains roughly **63.2% unique instances** (on average).\n",
    "* The remaining **~36.8%** are \"out-of-bag\" (OOB) samples that can be used for internal validation.\n",
    "\n",
    "#### B. Prediction Aggregation\n",
    "* **Classification:** Majority voting (\"hard voting\") or average probabilities (\"soft voting\").\n",
    "* **Regression:** Average of all predictions.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For classification with $n$ estimators:\n",
    "\n",
    "$$\\hat{y} = \\text{mode}\\{h_1(x), h_2(x), ..., h_n(x)\\}$$\n",
    "\n",
    "For regression:\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} h_i(x)$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Types of Bagging Classifiers in Scikit-Learn\n",
    "\n",
    "#### 1. Standard `BaggingClassifier`\n",
    "Allows you to bag *any* base estimator (e.g., Logistic Regression, SVM, Decision Trees).\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Bagging classifier\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,        # Train on 80% of samples\n",
    "    max_features=0.8,       # Use 80% of features\n",
    "    bootstrap=True,         # Sampling with replacement\n",
    "    bootstrap_features=False,\n",
    "    oob_score=True,         # Use OOB for validation\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7f420",
   "metadata": {},
   "source": [
    "RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest is essentially bagging with decision trees\n",
    "# PLUS feature randomization at each split\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',    # Feature bagging\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ExtraTreesClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Similar to Random Forest but with random splits\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cd99d",
   "metadata": {},
   "source": [
    "## How does Bagging reduce overfitting?\n",
    "\n",
    "Bagging reduces overfitting by averaging multiple models trained on different data subsets. This decreases variance without increasing bias. The randomness in sampling ensures models learn different patterns, and their aggregation cancels out individual errors.\n",
    "\n",
    "\n",
    "\n",
    "# Bagging / Ensemble Model Parameters\n",
    "\n",
    "Below are some key parameters often used in ensemble methods like **Bagging** or **Random Forest**:\n",
    "\n",
    "| Parameter           | Description                                                                                     |\n",
    "|--------------------|-------------------------------------------------------------------------------------------------|\n",
    "| `n_estimators`     | Number of base estimators (more estimators usually reduce variance, but improvements diminish after a point). |\n",
    "| `max_samples`      | Fraction or number of samples to draw for each estimator. Default is `1.0` (use all samples).  |\n",
    "| `max_features`     | Fraction or number of features to draw for each estimator (controls diversity among estimators). |\n",
    "| `bootstrap`        | Whether to sample **with replacement** (`True` for standard bagging, `False` for no replacement). |\n",
    "| `bootstrap_features` | Whether to sample features with replacement. Useful in Random Forests for decorrelating trees. |\n",
    "| `oob_score`        | Whether to use **out-of-bag samples** for validation. Helps estimate model performance without a separate validation set. |\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Using Bagging with Scikit-Learn\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Bagging Regressor\n",
    "bag_model = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "bag_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"OOB Score:\", bag_model.oob_score_)\n",
    "print(\"Test Score:\", bag_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c99aea",
   "metadata": {},
   "source": [
    "### Strengths & Weaknesses of Bagging\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)** is primarily designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Advantages\n",
    "* **Reduces Variance:** Especially effective for high-variance estimators (like deep decision trees). By averaging multiple models, the variance of the final prediction is reduced.\n",
    "* **Improves Stability:** The ensemble is less sensitive to noise and specific outliers in the training data.\n",
    "* **Parallelizable:** Since each base estimator is independent of the others, they can be trained simultaneously across multiple cores or machines.\n",
    "* **OOB (Out-Of-Bag) Validation:** Provides built-in validation. The data left out during the bootstrap sampling (approx. 37%) can be used to evaluate the model without needing a separate validation set.\n",
    "* **Handles Overfitting:** It can regularize complex base estimators that would otherwise overfit the data.\n",
    "\n",
    "#### 2. Disadvantages\n",
    "* **Computationally Expensive:** Requires training multiple models (often hundreds), which increases training time compared to a single model.\n",
    "* **Less Interpretable:** It is much harder to explain the logic of an ensemble of 100 trees than a single Decision Tree.\n",
    "* **Memory Intensive:** Storing multiple models requires significantly more RAM.\n",
    "* **May Not Reduce Bias:** If the base estimator has high bias (underfitting), bagging will generally not fix it. It is designed to fix variance, not bias.\n",
    "* **Can Underfit:** If the base estimator is too simple (e.g., a shallow tree), the aggregate model may also underfit.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Guide\n",
    "\n",
    "| When to **Use** Bagging | When to **Avoid** Bagging |\n",
    "| :--- | :--- |\n",
    "| **Base estimator has high variance**<br>(e.g., deep Decision Trees, k-NN with low k) | **Base estimator is low-variance**<br>(e.g., Linear Regression, Logistic Regression) |\n",
    "| **Sufficient computational resources** are available (RAM/CPU) | **Computational resources are limited** |\n",
    "| **Model stability** is more important than interpretability | **Interpretability** is crucial (need to explain \"why\") |\n",
    "| **Parallel computing** is available to speed up training | **Training data is very small** (bootstrapping might reduce unique data per model too much) |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
