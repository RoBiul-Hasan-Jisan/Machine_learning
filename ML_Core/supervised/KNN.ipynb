{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad8cca9",
   "metadata": {},
   "source": [
    "# KNN (K-Nearest Neighbors)\n",
    "\n",
    "KNN is a **supervised**, **instance-based** (lazy learner), and **non-parametric** algorithm used for classification and regression. It makes predictions by finding the $k$ closest data points (neighbors) in the training set to a new input and using a majority vote (classification) or an average (regression) of their values.\n",
    "\n",
    "\n",
    "\n",
    "### Core Steps\n",
    "The core steps for making a prediction on a new data point are:\n",
    "\n",
    "1.  **Calculate Distance:** Measure the distance (e.g., Euclidean, Manhattan) from the new point to all points in the training set.\n",
    "2.  **Find Neighbors:** Identify the $k$ points with the smallest distances.\n",
    "3.  **Aggregate for Output:**\n",
    "    * **For Classification:** Assign the most common class among the $k$ neighbors.\n",
    "    * **For Regression:** Assign the average value of the $k$ neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "## Critical Parameters and Hyperparameter Tuning\n",
    "The performance of KNN hinges on several key decisions.\n",
    "\n",
    "### 1. Choosing 'k' (Number of Neighbors)\n",
    "This is the most critical parameter.\n",
    "* **Small $k$ (e.g., 1):** Makes the model sensitive to noise (overfitting).\n",
    "* **Large $k$:** Oversimplifies the model (underfitting).\n",
    "* **Tip:** An **odd** $k$ value is preferred for classification to avoid ties. The optimal $k$ is data-dependent and is typically found using cross-validation or the Elbow Method.\n",
    "\n",
    "### 2. Selecting a Distance Metric\n",
    "The choice defines \"closeness.\" Common metrics include:\n",
    "* **Euclidean Distance:** The straight-line distance (default for continuous features).\n",
    "* **Manhattan Distance:** Sum of absolute differences (useful for grid-like data).\n",
    "* **Minkowski Distance:** A generalized formula; setting $p=2$ gives Euclidean, $p=1$ gives Manhattan.\n",
    "\n",
    "### 3. Other Tuning Parameters\n",
    "* **Weights:** Neighbors can be weighted uniformly or by the inverse of their distance (`weights='distance'`), giving closer points more influence.\n",
    "* **Algorithm:** Methods like `ball_tree` or `kd_tree` can be faster than `brute` force for larger datasets.\n",
    "\n",
    "> **Best Practice:** Use `GridSearchCV` or `RandomizedSearchCV` from scikit-learn to systematically test combinations of these parameters (e.g., `n_neighbors`, `weights`, `metric`) and find the best set via cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use (and When to Avoid)\n",
    "\n",
    "| Use KNN when... | Avoid KNN when... |\n",
    "| :--- | :--- |\n",
    "| You have a small to moderately sized dataset. | The dataset is very large (slow prediction). |\n",
    "| Interpretability is important (explaining \"nearest neighbors\" is intuitive). | The dataset has many features (high-dimensional). |\n",
    "| Data has a non-linear pattern and you need a simple baseline model. | Prediction speed is a critical requirement. |\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Applications\n",
    "Due to its intuitive logic, KNN is widely used in:\n",
    "\n",
    "* **Recommendation Systems:** Finding users with similar tastes to suggest products or content.\n",
    "* **Pattern Recognition & Security:** Detecting fraudulent credit card transactions by identifying anomalous patterns.\n",
    "* **Healthcare:** Classifying medical diagnoses, such as predicting the risk of a disease based on similar patient records.\n",
    "* **Finance:** Credit scoring, stock market forecasting, and customer profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0d779",
   "metadata": {},
   "source": [
    "# KNN: Classifier vs. Regressor\n",
    "\n",
    "Just like Decision Trees, the KNN algorithm adapts based on whether your target variable is a **Category** (Label) or a **Number** (Value).\n",
    "\n",
    "### 1. KNN Classifier\n",
    "**Use this when:** Your target variable is **Categorical**.\n",
    "\n",
    "* **The Logic:** \"Majority Vote.\" The algorithm looks at the $k$ nearest neighbors and assigns the class that appears most frequently.\n",
    "* **Tie-Breaking:** If there is a tie (e.g., 2 neighbors are Red, 2 are Blue), it often selects the class of the *closest* neighbor (if weights are used) or the first one found.\n",
    "* **Output:** A Class Label (e.g., \"Malignant\", \"Spam\", \"Blue\").\n",
    "\n",
    "**Example:**\n",
    "* If $k=3$ and the neighbors are [Red, Red, Blue], the prediction is **Red**.\n",
    "\n",
    "### 2. KNN Regressor\n",
    "**Use this when:** Your target variable is **Continuous**.\n",
    "\n",
    "* **The Logic:** \"Average.\" The algorithm looks at the $k$ nearest neighbors and calculates the mean (average) of their values.\n",
    "* **Output:** A Numerical Value (e.g., 150.5, $250k, 32Â°C).\n",
    "\n",
    "**Example:**\n",
    "* If $k=3$ and the neighbor values are [10, 20, 30], the prediction is **20** (since $(10+20+30)/3 = 20$).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | KNN **Classifier** | KNN **Regressor** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Method** | Majority Voting | Averaging (Mean) |\n",
    "| **Target** | Categories (Classes) | Continuous Numbers |\n",
    "| **Evaluation** | Accuracy, F1-Score | MSE, MAE, $R^2$ |\n",
    "| **Python Class** | `KNeighborsClassifier` | `KNeighborsRegressor` |\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "The setup is nearly identical, just a different class import.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SCENARIO 1: CLASSIFICATION (Predicting Fruit Type)\n",
    "# -------------------------------------------------------\n",
    "# X: [Weight, Color_Score]\n",
    "# y: ['Apple', 'Orange', 'Apple']\n",
    "\n",
    "# k=3, weights='distance' (closer neighbors have more say)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "# knn_clf.fit(X, y)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SCENARIO 2: REGRESSION (Predicting House Price)\n",
    "# -------------------------------------------------------\n",
    "# X: [Sq_Ft, Num_Rooms]\n",
    "# y: [250000, 400000, 320000]\n",
    "\n",
    "# k=3, weights='uniform' (simple average)\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=3, weights='uniform')\n",
    "# knn_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16345e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries & Load Data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# X = df.drop('target_column', axis=1)\n",
    "# y = df['target_column']\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Feature Scaling (CRITICAL for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Hyperparameter Tuning with Grid Search\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Evaluate Best Model\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Best Params: {grid_search.best_params_}, Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01837b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bc108bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
