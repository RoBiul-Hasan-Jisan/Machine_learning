{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbb70d7",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A **Decision Tree** is a versatile, non-parametric supervised learning algorithm used for both classification and regression tasks. Its model forms a tree-like structure, mimicking human decision-making processes.\n",
    "\n",
    "\n",
    "\n",
    "### Core Components\n",
    "\n",
    "* **Root Node:** The topmost node, representing the entire dataset. It gets split into two or more homogeneous sets.\n",
    "* **Internal Nodes (Decision Nodes):** Nodes that represent a decision (or test) on a specific feature, splitting into further nodes.\n",
    "* **Leaf Nodes (Terminal Nodes):** The final output nodes. They represent a class label (in classification) or a continuous value (in regression) and do not split further.\n",
    "* **Branches (Sub-Trees):** Sections of the tree that are not the root or a leaf.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does a Decision Tree Learn?\n",
    "\n",
    "The algorithm learns by recursively splitting the data into purer subsets based on the features. At each node, it selects the feature and split point that best separates the data according to the target variable. This process continues until a stopping criterion is met (e.g., maximum depth is reached, or a node is 100% pure).\n",
    "\n",
    "### Key Splitting Criteria\n",
    "The \"best\" split is determined by a metric that quantifies the impurity or disorder of a node.\n",
    "\n",
    "#### 1. Gini Impurity\n",
    "Measures the probability of misclassifying a randomly chosen element from the node if it were randomly labeled according to the class distribution in the node.\n",
    "\n",
    "* **Formula:** $Gini = 1 - \\sum p_i^2$\n",
    "    *(where $p_i$ is the proportion of class $i$ in the node)*\n",
    "* **Range:** $0$ (perfectly pure) to $0.5$ (for a binary class with equal distribution).\n",
    "* **Characteristics:** Computationally efficient as it doesn't involve logarithms.\n",
    "\n",
    "#### 2. Entropy & Information Gain\n",
    "**Entropy** measures the amount of uncertainty or disorder in a node.\n",
    "\n",
    "* **Formula:** $Entropy = -\\sum (p_i \\times \\log_2(p_i))$\n",
    "* **Range:** $0$ (perfectly pure) to $1$ (maximally impure for binary classification).\n",
    "\n",
    "**Information Gain (IG)** is the reduction in entropy after a dataset is split on an attribute. The split with the highest IG is chosen.\n",
    "\n",
    "* **Formula:** $IG = Entropy(parent) - \\text{Weighted Average} \\times Entropy(children)$\n",
    "\n",
    "---\n",
    "\n",
    "### Gini vs. Entropy: Which to Use?\n",
    "\n",
    "1.  **Gini Impurity** is slightly faster to compute and is the default in many libraries (like `scikit-learn`).\n",
    "2.  **Entropy** may lead to more balanced trees as it tends to create slightly more granular splits.\n",
    "\n",
    "> **Note:** In practice, the difference is often negligible; the resulting trees are usually very similar.\n",
    "\n",
    "---\n",
    "#### Where Are Decision Trees Used?\n",
    "* **Finance:** Credit scoring, loan approval.\n",
    "* **Healthcare:** Disease diagnosis, patient risk stratification.\n",
    "* **E-commerce:** Customer segmentation, product recommendation engines.\n",
    "* **Manufacturing:** Quality control and fault detection.\n",
    "\n",
    "#### Practical Challenges & Solutions\n",
    "**Handling Missing Values:**\n",
    "* **Imputation:** Fill with mean, median, or mode.\n",
    "* **Surrogate Splits:** (C4.5, CART) Use other features that mimic the original split to handle data points with missing values.\n",
    "\n",
    "**Imbalanced Data:**\n",
    "* Use `class_weight` parameter to assign higher weights to minority classes.\n",
    "* Employ sampling techniques (SMOTE, undersampling) before training.\n",
    "\n",
    "**Feature Types:**\n",
    "* **Continuous:** Find an optimal threshold (e.g., Age $\\le 30$).\n",
    "* **Categorical:** Can be handled natively by some algorithms (C4.5) or require encoding (like Ordinal/Label Encoding) for others (CART).\n",
    "\n",
    "---\n",
    "\n",
    "### System Design Angle: Production Considerations\n",
    "\n",
    "#### When to Use Decision Trees?\n",
    "**Advantages:**\n",
    "* **Highly interpretable and visualizable** (White Box model).\n",
    "* Can handle both numerical and categorical data without scaling.\n",
    "* Mirrors human decision-making, which is great for business logic.\n",
    "\n",
    "**Disadvantages:**\n",
    "* **Prone to Overfitting:** Requires careful tuning and pruning.\n",
    "* **Unstable:** Small changes in data can lead to a completely different tree (high variance).\n",
    "* **Biased:** Tend to favor features with more levels (e.g., continuous features).\n",
    "\n",
    "#### Challenges in Production\n",
    "* **Model Drift:** The tree's rules can become obsolete as data distributions change, requiring periodic retraining.\n",
    "* **Scalability:** While fast to predict, training a very deep tree on large datasets can be memory-intensive.\n",
    "* **Instability:** A single tree in production is risky. This is why ensemble methods (Random Forests, Gradient Boosting) are almost always preferred for production systems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1. How do Decision Trees handle categorical vs. continuous features?\n",
    "\n",
    "* **Continuous Features:** The algorithm sorts the feature values and evaluates all possible split points (e.g., $X > 5.2$). It chooses the threshold that minimizes impurity (maximizes information gain).\n",
    "* **Categorical Features:**\n",
    "    * **Binary trees (like CART):** It tries all possible binary partitions of the categories (e.g., Color in {Red, Blue} vs. Color in {Green}).\n",
    "    * **Multi-way trees (like ID3):** It can create a branch for each category.\n",
    "    * *Note:* In libraries like `scikit-learn`, categorical features often need to be numerically encoded (e.g., LabelEncoder) beforehand.\n",
    "\n",
    "### 2. What is the role of entropy and information gain?\n",
    "\n",
    "* **Entropy:** Quantifies the disorder within a node. A pure node has an entropy of $0$.\n",
    "* **Information Gain:** Measures how much a split reduces this entropy. The algorithm greedily selects the feature and split point that results in the highest information gain at each step, effectively creating the most homogeneous child nodes.\n",
    "\n",
    "### 3. Why might a Decision Tree overfit, and how can you prevent it?\n",
    "\n",
    "**Why?** The tree can keep splitting until every leaf node is perfectly pure, effectively memorizing the training data, including noise and outliers.\n",
    "\n",
    "**Prevention:**\n",
    "1.  **Pruning:** Use cost-complexity pruning (`ccp_alpha`).\n",
    "2.  **Limit Tree Size:** Set a `max_depth`, `min_samples_split`, or `min_samples_leaf`.\n",
    "3.  **Use Ensemble Methods:** Train multiple trees (e.g., Random Forest) to average out the instability.\n",
    "4.  **Use More Data:** Helps the model learn general patterns instead of noise.\n",
    "\n",
    "### 4. Advantages and disadvantages of Gini Impurity vs. Entropy?\n",
    "\n",
    "| Criterion | Advantages | Disadvantages |\n",
    "| :--- | :--- | :--- |\n",
    "| **Gini Impurity** | Faster to compute (no logarithms). | Tends to isolate the most frequent class in its own branch. |\n",
    "| **Entropy** | More theoretically grounded. May create more balanced trees. | Slightly slower computation due to logarithms. |\n",
    "\n",
    "> **Note:** In practice, they produce very similar results, and the choice is often a matter of preference.\n",
    "\n",
    "### 5. How do Decision Trees handle missing values?\n",
    "\n",
    "* **Imputation:** Before training, fill missing values with a statistic like the mean or mode.\n",
    "* **Surrogate Splits (CART):** A powerful technique where the tree finds a \"backup\" feature that produces a split most similar to the primary split. If the primary feature is missing for a data point, the surrogate feature is used.\n",
    "* **Ignore:** Some implementations simply skip data points with missing values in the feature being considered for a split.\n",
    "\n",
    "### 6. Why are Decision Trees prone to instability?\n",
    "They are **high-variance estimators**. Because of their hierarchical nature, a small change in the training data can lead to a completely different choice at the root node, which then propagates down and changes the entire structure of the tree. This instability is why they are rarely used alone in practice.\n",
    "\n",
    "### 7. A Decision Tree overfits: performs well on training data but poorly on test data. What do you do?\n",
    "\n",
    "1.  **Apply Pruning:** This is the most direct method. Use `ccp_alpha` for post-pruning.\n",
    "2.  **Increase Regularization:** Increase `min_samples_leaf` or `min_samples_split`, or reduce `max_depth`.\n",
    "3.  **Gather More Training Data.**\n",
    "4.  **Perform Feature Selection:** Remove irrelevant features that may be adding noise.\n",
    "5.  **Switch to an Ensemble Method:** Use a Random Forest or Gradient Boosting machine, which are built to overcome the limitations of a single tree.\n",
    "\n",
    "### 8. How does pruning work, and what are the different types?\n",
    "\n",
    "* **Pre-Pruning (Early Stopping):** Stops the tree from growing based on predefined conditions (e.g., `max_depth=5`). It's simple but can be short-sighted.\n",
    "* **Post-Pruning (Cost-Complexity Pruning):**\n",
    "    1.  Grow the tree to its full depth.\n",
    "    2.  Systematically remove branches (sub-trees) from the fully grown tree.\n",
    "    3.  Replace a sub-tree with a leaf node.\n",
    "    4.  Evaluate the performance of the pruned tree on a validation set.\n",
    "    5.  Select the pruned tree that maximizes validation performance. It uses a hyperparameter `alpha` to balance tree complexity and accuracy.\n",
    "\n",
    "### 9. Do Decision Trees require feature scaling?\n",
    "**No.** Since the splitting logic is based on ordering feature values and calculating impurity, the scale of the features (e.g., Age vs. Salary) does not influence the model. This is a significant advantage over distance-based models like SVMs or K-Nearest Neighbors.\n",
    "\n",
    "### 10. How do Decision Trees determine feature importance?\n",
    "Feature importance is calculated as the (normalized) total reduction of the impurity criterion (Gini/Entropy/MSE) brought by that feature.\n",
    "\n",
    "* **Calculation:** For each feature, sum the impurity decrease for every node that splits on that feature, weighted by the fraction of samples it handles.\n",
    "* **Limitations:**\n",
    "    * **Bias towards high-cardinality features:** Continuous features or categorical features with many levels have more split options and can appear more important.\n",
    "    * **Correlation:** If two features are highly correlated, the importance may be arbitrarily assigned to one, making the other seem less important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da81d3",
   "metadata": {},
   "source": [
    "# Decision Tree: Classifier vs. Regressor\n",
    "\n",
    "The decision on whether to use a **Classifier** or a **Regressor** depends entirely on your **Target Variable** (the thing you are trying to predict).\n",
    "\n",
    "### 1. Decision Tree Classifier\n",
    "**Use this when:** Your target variable is **Categorical** (distinct classes or labels).\n",
    "\n",
    "* **The Goal:** Split the data to separate different classes (e.g., Yes vs. No).\n",
    "* **Prediction Output:** The **majority class** (mode) of the samples in the leaf node.\n",
    "* **Evaluation Metrics:** Accuracy, Precision, Recall, F1-Score, AUC-ROC.\n",
    "* **Splitting Criteria:** Gini Impurity, Information Gain (Entropy).\n",
    "\n",
    "**Real-World Examples:**\n",
    "* **Email Filter:** Is this email `Spam` or `Not Spam`?\n",
    "* **Medical Diagnosis:** Does the patient have `Diabetes`, `Heart Disease`, or `No Disease`?\n",
    "* **Loan Approval:** Should the loan be `Approved` or `Denied`?\n",
    "\n",
    "### 2. Decision Tree Regressor\n",
    "**Use this when:** Your target variable is **Continuous** (numerical values).\n",
    "\n",
    "\n",
    "\n",
    "* **The Goal:** Split the data to group similar numerical values together.\n",
    "* **Prediction Output:** The **average value** (mean) of the samples in the leaf node.\n",
    "* **Evaluation Metrics:** MSE (Mean Squared Error), RMSE, MAE, R-Squared ($R^2$).\n",
    "* **Splitting Criteria:** MSE (Mean Squared Error), Friedman MSE, MAE.\n",
    "\n",
    "**Real-World Examples:**\n",
    "* **House Pricing:** Predicting the exact **price** of a house (e.g., $350,000) based on square footage and location.\n",
    "* **Sales Forecasting:** Predicting **how many units** of a product will sell next month.\n",
    "* **Temperature Prediction:** Predicting the specific **temperature** (e.g., 24.5°C) for tomorrow.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Comparison Table\n",
    "\n",
    "| Feature | Decision Tree **Classifier** | Decision Tree **Regressor** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Target Type** | **Categories** (Discrete) | **Numbers** (Continuous) |\n",
    "| **Prediction** | Majority Vote (Mode) | Average (Mean) |\n",
    "| **Splitting Metric** | Gini Impurity / Entropy | Variance / Mean Squared Error (MSE) |\n",
    "| **Example Output** | \"Dog\", \"Cat\", \"Bird\" | 10.5, 342.1, 0.95 |\n",
    "| **Python Class** | `sklearn.tree.DecisionTreeClassifier` | `sklearn.tree.DecisionTreeRegressor` |\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation Difference\n",
    "\n",
    "Notice that the setup is almost identical, but you import a different class from `sklearn`.\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SCENARIO 1: CLASSIFICATION (Predicting Colors)\n",
    "# -------------------------------------------------------\n",
    "# X features: [Weight, Texture]\n",
    "# y target:   ['Apple', 'Orange', 'Apple'] (Categories)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SCENARIO 2: REGRESSION (Predicting Price)\n",
    "# -------------------------------------------------------\n",
    "# X features: [Rooms, Location_Score]\n",
    "# y target:   [200000, 150000, 300000] (Continuous Numbers)\n",
    "\n",
    "reg = DecisionTreeRegressor(criterion='squared_error', max_depth=3)\n",
    "# reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0496, R²: 0.8916\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model\n",
    "tree_reg = DecisionTreeRegressor(\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.4f}, R²: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
