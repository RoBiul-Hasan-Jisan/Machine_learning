{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11da07a6",
   "metadata": {},
   "source": [
    "# LASSO Regression (L1 Regularization)\n",
    "\n",
    "## 1. Definition\n",
    "**LASSO** = **L**east **A**bsolute **S**hrinkage and **S**election **O**perator.\n",
    "\n",
    "It is a regression analysis method that performs both **variable selection** and **regularization** in order to enhance the prediction accuracy and interpretability of the statistical model it produces.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Cost Function (Math)\n",
    "\n",
    "Lasso modifies the standard Mean Squared Error (MSE) by adding an **L1 Penalty** term.\n",
    "\n",
    "$$J(\\beta) = \\underbrace{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}_{\\text{MSE}} + \\underbrace{\\lambda \\sum_{j=1}^{k} |\\beta_j|}_{\\text{L1 Penalty}}$$\n",
    "\n",
    "* **$\\lambda$ (Lambda):** The tuning parameter.\n",
    "* **$|\\beta_j|$:** The absolute value of the coefficients.\n",
    "* **Note:** We do **not** regularize the intercept ($\\beta_0$).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why Coefficients Hit Zero (Geometric Intuition)\n",
    "\n",
    "\n",
    "\n",
    "* **Ridge (L2):** The constraint region is a **Circle** ($\\beta^2$). The error contours usually hit the circle *not* on the axis, shrinking coefficients close to 0 but never exactly 0.\n",
    "* **Lasso (L1):** The constraint region is a **Diamond** ($|\\beta|$). The corners of the diamond lie on the axes. The error contours are statistically likely to hit these \"sharp corners\" first.\n",
    "    * **Result:** When the solution hits a corner, one of the coefficients is exactly 0.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Optimization (How it solves)\n",
    "\n",
    "Unlike Linear or Ridge Regression, Lasso **does not** have a closed-form solution (you can't just use matrix algebra to solve it in one step) because the absolute value function $|\\beta|$ is not differentiable at zero.\n",
    "\n",
    "**Algorithms used:**\n",
    "1.  **Coordinate Descent:** (Most common) Optimizes one coefficient at a time while holding others fixed.\n",
    "2.  **LARS (Least Angle Regression):** An efficient algorithm for high-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Lasso vs. Ridge (Comparison)\n",
    "\n",
    "| Topic                 | **Lasso (L1)**                                    | **Ridge (L2)**                                 |\n",
    "| --------------------- | ------------------------------------------------- | ---------------------------------------------- |\n",
    "| **Penalty type**      | Uses **absolute value** of coefficients           | Uses **square** of coefficients                |\n",
    "| **Feature selection** |  **Yes** – makes some coefficients exactly **0**  |  **No** – makes coefficients **small**, not 0 |\n",
    "| **Multicollinearity** | Keeps **one** feature and removes others          | Keeps **all** features but shrinks them        |\n",
    "| **Solution method**   | Solved using **iterative algorithms**             | Has a **direct (closed-form)** solution        |\n",
    "| **Best use case**     | When **few features are important**               | When **all features matter a little**          |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Critical Requirement: Feature Scaling\n",
    "\n",
    "**You must standardize your data** before using Lasso.\n",
    "* **Why?** Lasso penalizes the *magnitude* of coefficients.\n",
    "* If \"Salary\" is in thousands (100,000) and \"Age\" is in double digits (40), Lasso will unfairly punish \"Salary\" just because the number is bigger, not because it's less important.\n",
    "* **Fix:** Use `StandardScaler`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. The Weakness & The Fix (Elastic Net)\n",
    "\n",
    "**The Problem:**\n",
    "If two features are highly correlated ( *Height* and *Leg Length*), Lasso effectively flips a coin and keeps one while dropping the other to zero. This can be unstable.\n",
    "\n",
    "**The Solution: Elastic Net**\n",
    "Combines L1 (Lasso) and L2 (Ridge).\n",
    "$$Cost = MSE + \\lambda_1 \\sum |\\beta| + \\lambda_2 \\sum \\beta^2$$\n",
    "* It groups correlated features together (Ridge effect) and selects them or drops them as a group (Lasso effect).\n",
    "\n",
    "---\n",
    "\n",
    "# Applications of L1 Regularization (Lasso)\n",
    "\n",
    "## 1. Lasso Regression (Linear Models)\n",
    "This is the most common application of L1 regularization.\n",
    "\n",
    "* **Goal:** To prevent overfitting in linear models while simultaneously performing **Feature Selection**.\n",
    "* **Mechanism:** It adds the absolute value of coefficients ($|\\beta|$) to the cost function.\n",
    "* **Result:** It forces the coefficients of weak or irrelevant features to become **exactly zero**, effectively removing them from the equation.\n",
    "* **Best For:** Creating **Sparse Models** (models where most weights are zero) which are easier to interpret and faster to run.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logistic Regression with L1\n",
    "L1 regularization is not limited to regression; it is widely used in classification tasks.\n",
    "\n",
    "* **Goal:** To identify which features most strongly discriminate between classes (e.g., \"Spam\" vs. \"Not Spam\").\n",
    "* **How it works:** The solver (like `liblinear` in Scikit-Learn) minimizes the Log Loss + L1 Penalty.\n",
    "* **Use Case:** High-dimensional classification, such as:\n",
    "    * **Text Classification:** Determining which specific words (features) predict the sentiment of a review.\n",
    "    * **Genomics:** Identifying which specific genes (out of thousands) predict a disease.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Neural Networks \n",
    "In Deep Learning, L1 regularization is applied to the weight matrices of the layers.\n",
    "\n",
    "* **Goal:** Reduce model complexity and memory footprint.\n",
    "* **Input Layer Pruning:** Applying L1 to the **first hidden layer** is a powerful trick. It forces the network to ignore irrelevant input features entirely, acting as an automatic feature selector for raw data.\n",
    "* **Network Sparsity:**\n",
    "    * **L2 (Weight Decay):** Makes weights small (diffuse).\n",
    "    * **L1 Regularization:** Makes weights zero (sparse). This can allow for \"Network Pruning\" (compressing the model size by removing zero-weight connections).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary: When to Apply L1?\n",
    "\n",
    "| Domain | Application | Benefit |\n",
    "| :--- | :--- | :--- |\n",
    "| **Finance** | Predicting credit risk | Removes irrelevant financial indicators to explain *why* a loan was rejected. |\n",
    "| **Bioinformatics** | DNA Analysis | Selects the 5 genes responsible for a trait out of 20,000 candidates. |\n",
    "| **Computer Vision** | Compressed Sensing | Reconstructs images with fewer pixels by assuming the image signal is sparse. |\n",
    "| **NLP** | Sentiment Analysis | Isolates key keywords (e.g., \"Terrible\", \"Amazing\") and ignores filler words (e.g., \"the\", \"is\"). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5734a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 3.92693362e-01  1.50810624e-02 -0.00000000e+00  0.00000000e+00\n",
      "  1.64168387e-05 -3.14918929e-03 -1.14291203e-01 -9.93076483e-02]\n",
      "Intercept: -7.698845419807455\n",
      "MSE: 0.6135115198058131\n",
      "R2 Score: 0.5318167610318159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Lasso model\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"MSE:\", mean_squared_error(Y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87b679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [   0.         -152.66477923  552.69777529  303.36515791  -81.36500664\n",
      "   -0.         -229.25577639    0.          447.91952518   29.64261704]\n",
      "Intercept: 151.57485282893947\n",
      "MSE: 2798.193485169719\n",
      "R2 Score: 0.4718547867276227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"MSE:\", mean_squared_error(Y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f112eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [41.82204111 64.12395241 18.73201774 46.72243239 23.9565675  16.39228576\n",
      " 82.69535426 64.17889918  7.4871428  28.65860581]\n",
      "Intercept: -0.2927999172299187\n",
      "MSE: 97.1743302138308\n",
      "R2 Score: 0.9950667951885857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Synthetic regression dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"MSE:\", mean_squared_error(Y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(Y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
