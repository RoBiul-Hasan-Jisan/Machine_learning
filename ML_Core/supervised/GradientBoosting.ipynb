{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6caecb9",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Gradient Boosting Regressor is an ensemble method that builds many weak decision tree models **sequentially**, where each new tree tries to fix the errors made by the previous trees.\n",
    "\n",
    "\n",
    "\n",
    "### How it Works\n",
    "1.  **Sequential Learning:** Instead of training all trees independently (like Random Forest), Gradient Boosting trains them one after another.\n",
    "2.  **Error Correction:** Each new tree learns from the **\"residuals\"** (errors) of the previous tree.\n",
    "\n",
    "### Key Libraries (The Big Three)\n",
    "* **XGBoost:** Famous for performance and speed; the industry standard for competitions.\n",
    "* **LightGBM:** Developed by Microsoft; faster and uses less memory, great for huge datasets.\n",
    "* **CatBoost:** Developed by Yandex; handles categorical data (text labels) exceptionally well without extensive preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls and Solutions\n",
    "\n",
    "#### 1. Overfitting\n",
    "**Symptoms:**\n",
    "* Training error keeps decreasing while validation error increases.\n",
    "* Model becomes too complex.\n",
    "\n",
    "**Solutions:**\n",
    "* Use smaller `learning_rate` with more `n_estimators`.\n",
    "* Apply stronger regularization (`max_depth`, `min_samples_split`).\n",
    "* Use `subsample < 1.0` (Stochastic Gradient Boosting).\n",
    "* Implement **Early Stopping**.\n",
    "\n",
    "#### 2. Underfitting\n",
    "**Symptoms:**\n",
    "* Both training and validation errors are high.\n",
    "* Model is too simple.\n",
    "\n",
    "**Solutions:**\n",
    "* Increase `max_depth`.\n",
    "* Increase `n_estimators`.\n",
    "* Increase `learning_rate` (with caution).\n",
    "* Reduce regularization parameters.\n",
    "\n",
    "#### 3. Computational Efficiency\n",
    "**Optimizations:**\n",
    "* Use **histogram-based boosting** (like LightGBM) for large datasets.\n",
    "* Reduce `n_estimators` while increasing `learning_rate`.\n",
    "* Use `max_features` to limit feature consideration at each split.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "* **Start Simple:** Begin with default parameters and iterate.\n",
    "* **Feature Scaling:** Gradient Boosting is generally robust to feature scales, but normalization can help convergence.\n",
    "* **Handle Missing Values:** Many implementations (like XGBoost) can handle `NaN`s naturally.\n",
    "* **Monitor Learning:** Use validation curves to detect overfitting early.\n",
    "* **Ensemble Diversity:** Consider blending with other models for maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2a081",
   "metadata": {},
   "source": [
    "# Gradient Boosting: Classifier vs. Regressor\n",
    "\n",
    "### 1. Gradient Boosting Classifier\n",
    "**Use this when:** Your target variable is **Categorical** (classes/labels).\n",
    "\n",
    "* **Goal:** Predict the probability of a sample belonging to a class.\n",
    "* **Loss Function:** Log Loss (Deviance) or Exponential Loss.\n",
    "* **Output:** Probability scores (which are then thresholded to get a class label).\n",
    "* **Python Class:** `sklearn.ensemble.GradientBoostingClassifier` or `XGBClassifier`.\n",
    "\n",
    "**Example:**\n",
    "* Will a user click on this ad? (Yes/No)\n",
    "* Is this transaction fraudulent? (Fraud/Not Fraud)\n",
    "\n",
    "### 2. Gradient Boosting Regressor\n",
    "**Use this when:** Your target variable is **Continuous** (numerical values).\n",
    "\n",
    "* **Goal:** Predict a specific quantity or amount.\n",
    "* **Loss Function:** Squared Error (MSE), Absolute Error (MAE), or Huber Loss (for robustness to outliers).\n",
    "* **Output:** A numerical value.\n",
    "* **Python Class:** `sklearn.ensemble.GradientBoostingRegressor` or `XGBRegressor`.\n",
    "\n",
    "**Example:**\n",
    "* Predicting the price of a used car.\n",
    "* Predicting the duration of a taxi ride.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | Gradient Boosting **Classifier** | Gradient Boosting **Regressor** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Target** | Categories (0 or 1, Red or Blue) | Numbers (10.5, 5000, -2.1) |\n",
    "| **Objective** | Minimize classification error (Log Loss) | Minimize prediction error (MSE/RMSE) |\n",
    "| **Prediction** | Class Label or Probability | Continuous Value |\n",
    "| **Key Hyperparameter** | `loss='log_loss'` (default) | `loss='squared_error'` (default) |\n",
    "\n",
    "### Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "# 1. Classification (Target: Yes/No)\n",
    "# Useful for: Fraud detection, Churn prediction\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\n",
    "# clf.fit(X_train, y_train_classes)\n",
    "\n",
    "# 2. Regression (Target: Numerical Price)\n",
    "# Useful for: House pricing, Stock trends\n",
    "reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "# reg.fit(X_train, y_train_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06546b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"MSE:\", mean_squared_error(Y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(Y_test, y_pred))\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
