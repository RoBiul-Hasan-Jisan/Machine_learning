{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d3ab41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f5de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb4475c",
   "metadata": {},
   "source": [
    "Data Preparation Guidelines\n",
    "Text data: Use TF-IDF or CountVectorizer\n",
    "\n",
    "Categorical data: One-hot encoding or label encoding\n",
    "\n",
    "Continuous data: Consider binning for MultinomialNB or use GaussianNB\n",
    "\n",
    "Missing values: Naive Bayes handles missing values naturally in some implementations\n",
    "\n",
    "\n",
    "Handling Common Issues\n",
    "Zero Frequency Problem (When a feature value doesn't appear in a class)\n",
    "\n",
    "Solution: Apply Laplace smoothing (Î± > 0)\n",
    "\n",
    "Numerical Underflow (Multiplying many small probabilities)\n",
    "\n",
    "Solution: Use log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4878b",
   "metadata": {},
   "source": [
    "## Strengths & Weaknesses\n",
    "Advantages\n",
    "Extremely fast training and prediction\n",
    "\n",
    "Works well with high-dimensional data (like text)\n",
    "\n",
    "Requires small amount of training data\n",
    "\n",
    "Handles both continuous and discrete data\n",
    "\n",
    "Simple to implement and interpret\n",
    "\n",
    "Performs well with categorical features\n",
    "\n",
    "Disadvantages\n",
    "Strong independence assumption (rarely true in practice)\n",
    "\n",
    "Zero frequency problem without smoothing\n",
    "\n",
    "Not ideal for regression tasks (primarily for classification)\n",
    "\n",
    "Can be outperformed by more complex models with large datasets\n",
    "\n",
    "Sensitive to irrelevant features (no feature selection built-in)\n",
    "\n",
    "## Real-World Applications\n",
    "1. Text Classification\n",
    "Spam detection (Gmail, Outlook)\n",
    "\n",
    "Sentiment analysis\n",
    "\n",
    "News categorization\n",
    "\n",
    "Language detection\n",
    "\n",
    "2. Medical Diagnosis\n",
    "Disease prediction based on symptoms\n",
    "\n",
    "Risk assessment\n",
    "\n",
    "3. Recommendation Systems\n",
    "Product recommendations\n",
    "\n",
    "Content filtering\n",
    "\n",
    "4. Fraud Detection\n",
    "Credit card fraud\n",
    "\n",
    "Insurance claim fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d3baa",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on **Bayes' Theorem** with the \"naive\" assumption that all features are **conditionally independent** given the class label.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Bayes' Theorem\n",
    "This is the mathematical foundation of the algorithm.\n",
    "\n",
    "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
    "\n",
    "* **$P(y|X)$ (Posterior):** The probability of class $y$ given features $X$ (what we want to calculate).\n",
    "* **$P(X|y)$ (Likelihood):** The probability of observing features $X$ given that the class is $y$.\n",
    "* **$P(y)$ (Prior):** The initial probability of class $y$ (before seeing any data).\n",
    "* **$P(X)$ (Evidence):** The total probability of observing features $X$ (normalization constant).\n",
    "\n",
    "### 2. The \"Naive\" Assumption\n",
    "The classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. This simplifies the computation significantly.\n",
    "\n",
    "$$P(x_1, x_2, \\dots, x_n | y) = P(x_1|y) \\cdot P(x_2|y) \\cdot \\dots \\cdot P(x_n|y)$$\n",
    "\n",
    "> **Note:** This assumption is rarely true in real-world data (e.g., in text, words *are* correlated), but the algorithm still performs surprisingly well.\n",
    "\n",
    "### 3. Prediction Formula\n",
    "For classification, we predict the class $\\hat{y}$ that has the highest posterior probability:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i|y)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Naive Bayes Classifiers\n",
    "\n",
    "### 1. Gaussian Naive Bayes\n",
    "**Use Case:** Continuous features that are assumed to follow a **Normal (Gaussian) Distribution** (e.g., Iris dataset features).\n",
    "\n",
    "**Probability Density Function:**\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)$$\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize and train\n",
    "gnb = GaussianNB()\n",
    "# gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79d0ce",
   "metadata": {},
   "source": [
    "### 2. Multinomial Naive Bayes\n",
    "**Use Case:** Discrete counts. This is the standard algorithm for **Text Classification** (e.g., word counts, spam detection).\n",
    "\n",
    "\n",
    "\n",
    "**Likelihood with Smoothing:**\n",
    "$$P(x_i|y) = \\frac{N_{yi} + \\alpha}{N_y + \\alpha \\cdot n}$$\n",
    "\n",
    "* **$N_{yi}$:** Count of feature $i$ in class $y$.\n",
    "* **$N_y$:** Total count of all features in class $y$.\n",
    "* **$\\alpha$ (alpha):** Smoothing parameter.\n",
    "    * If $\\alpha=1$, it is **Laplace Smoothing**.\n",
    "    * This prevents zero probabilities if a word is missing from the training data (zero frequency problem).\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# alpha=1.0 is default (Laplace smoothing)\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "# mnb.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac830fdc",
   "metadata": {},
   "source": [
    "### 3. Bernoulli Naive Bayes\n",
    "**Use Case:** Binary/Boolean features (e.g., Presence vs. Absence of a word, simple True/False features).\n",
    "\n",
    "**Formula:**\n",
    "It penalizes the non-occurrence of a feature $i$ that is an indicator for class $y$.\n",
    "\n",
    "$$P(x_i|y) = P(i|y) \\cdot x_i + (1 - P(i|y)) \\cdot (1 - x_i)$$\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# binarize=0.0 sets the threshold for converting continuous features to binary\n",
    "bnb = BernoulliNB(binarize=0.0)\n",
    "# bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e670b6",
   "metadata": {},
   "source": [
    "## Quick Reference Card\n",
    "\n",
    "### When to Use Which Variant\n",
    "\n",
    "| Variant | Input Data Type | Example Use Case |\n",
    "| :--- | :--- | :--- |\n",
    "| **GaussianNB** | Continuous features (assumed Normal Distribution). | Iris flower dimensions, physical measurements. |\n",
    "| **MultinomialNB** | Discrete counts. | Word counts in text, ratings (1-5). |\n",
    "| **BernoulliNB** | Binary features (True/False, 0/1). | Word presence vs. absence, boolean flags. |\n",
    "\n",
    "### Preprocessing Checklist\n",
    "* **Handle missing values** (Naive Bayes generally doesn't handle NaNs natively in sklearn).\n",
    "* **Encode categorical variables.**\n",
    "* **Scale features** (Critical for **GaussianNB** to ensure bell curves are comparable).\n",
    "* **Apply text vectorization** (CountVectorizer or TF-IDF) for text data.\n",
    "* **Balance classes** if needed (though Naive Bayes handles imbalances reasonably well, extreme cases need attention).\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "* **Forgetting feature scaling** for `GaussianNB`.\n",
    "* **Not applying smoothing** (alpha) for unseen feature combinations (Zero Frequency Problem).\n",
    "* **Using raw counts without normalization** for text data (long documents might dominate short ones).\n",
    "* **Ignoring feature correlations** when they are strong (remember, the \"Naive\" assumption assumes independence).\n",
    "* **Assuming probabilities are well-calibrated** (Naive Bayes is known for being a great classifier but a poor probability estimator; the `predict_proba` outputs are often too extreme)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
