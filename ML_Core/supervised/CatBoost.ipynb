{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae96d13",
   "metadata": {},
   "source": [
    "# CatBoost Regressor\n",
    "\n",
    "**CatBoost** stands for **Cat**egory **Boost**ing. It is a gradient boosting algorithm that excels particularly with datasets containing many categorical features. It handles categorical features natively and efficiently without extensive preprocessing, while maintaining high prediction quality.\n",
    "\n",
    "### Main Advantages\n",
    "* **Superior handling** of categorical features\n",
    "* **Reduced need** for hyperparameter tuning\n",
    "* **High performance** with default settings\n",
    "* **Robust** to overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## How it Works\n",
    "\n",
    "Like XGBoost and LightGBM, CatBoost uses the gradient boosting framework with the same fundamental objective function:\n",
    "\n",
    "$$\n",
    "Obj(\\theta) = \\sum_{i} L(y_i, \\hat{y}_i) + \\sum_{k} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "However, its key innovations lie in how it handles categorical features and how it computes gradients.\n",
    "\n",
    "### 1. Ordered Boosting (The Secret Sauce)\n",
    "**Problem:** Traditional gradient boosting suffers from target leakage or prediction shift. When calculating gradients for a data point, the model uses the current ensemble, but this ensemble was trained on the same data point, creating a bias.\n",
    "\n",
    "**CatBoost Solution: Ordered Boosting**\n",
    "* Uses a **permutation (ordering)** of the training data.\n",
    "* For each example, the model used to calculate gradients is trained **only on the examples that come before it** in the permutation.\n",
    "* This eliminates target leakage and makes the model more robust.\n",
    "\n",
    "> **Analogy:** It's like time-series cross-validationâ€”you don't use future data to predict the past.\n",
    "\n",
    "### 2. Innovative Categorical Feature Handling\n",
    "**Problem:** Common methods like one-hot encoding can lead to high dimensionality, while label encoding can create false relationships.\n",
    "\n",
    "**CatBoost Solution: Ordered Target Statistics**\n",
    "For each categorical feature, it calculates:\n",
    "\n",
    "$$\n",
    "\\text{avg\\_target} = \\frac{\\text{sum of target for category} + \\text{prior}}{\\text{count of category} + 1}\n",
    "$$\n",
    "\n",
    "But crucially, it uses the same **ordered principle**: when calculating the statistic for a training example, it *only uses the examples that come before it in the permutation*.\n",
    "\n",
    "**Other Methods Supported:**\n",
    "* **One-hot encoding:** For low-cardinality features.\n",
    "* **Feature combinations:** Automatically creates interactions between categorical features.\n",
    "\n",
    "### 3. Symmetric Tree Structure\n",
    "Unlike XGBoost and LightGBM which build asymmetric trees, CatBoost builds **symmetric (oblivious) trees**:\n",
    "* Same splitting condition across all nodes at the same level.\n",
    "* **Faster prediction time** (just index traversal).\n",
    "* More resistant to overfitting.\n",
    "* Easier to implement in production.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Choose CatBoost\n",
    "\n",
    "| Choose CatBoost when... | Be Cautious when... |\n",
    "| :--- | :--- |\n",
    "| Your dataset has **many categorical features** | You have very few categorical features |\n",
    "| You want good performance with **minimal tuning** | Training speed is the absolute priority (LightGBM is faster) |\n",
    "| You need robust results without overfitting | You need extremely fine-grained control over tree structure |\n",
    "| **Productization is important** (symmetric trees = faster prediction) | |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Why is CatBoost Special?\n",
    "\n",
    "CatBoost solves two major problems inherent in other boosting algorithms:\n",
    "\n",
    "**1. Target Leakage**\n",
    "* Other models (XGBoost, LightGBM) require Label Encoding or One-hot Encoding.\n",
    "* These can create target leakage and bias in gradients.\n",
    "\n",
    "**2. Prediction Shift**\n",
    "* Boosting suffers from predicting based on its own predictions, which leads to overfitting.\n",
    "\n",
    "**$\\rightarrow$ CatBoost solves both using Ordered Boosting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d435594",
   "metadata": {},
   "source": [
    "# CatBoost Classifier vs. Regressor\n",
    "\n",
    "While both algorithms share the same underlying engine (ordered boosting, symmetric trees), they are optimized for different types of predictive tasks.\n",
    "\n",
    "## 1. Problem Type & Output\n",
    "\n",
    "| Feature | CatBoost Classifier | CatBoost Regressor |\n",
    "| :--- | :--- | :--- |\n",
    "| **Problem Domain** | Classification problems | Regression problems |\n",
    "| **Goal** | Predicts class labels or probabilities | Predicts continuous numerical values |\n",
    "| **Example Output** | `0`/`1`, `\"Spam\"`/`\"Ham\"`, `[0.2, 0.8]` | `125.7`, `-2.45`, `0.893` |\n",
    "\n",
    "## 2. Loss Functions\n",
    "\n",
    "| **Classifier** (Optimizes Probability) | **Regressor** (Optimizes Error) |\n",
    "| :--- | :--- |\n",
    "| **Logloss** (Binary Classification) | **RMSE** (Root Mean Squared Error) |\n",
    "| **MultiClass** | **MAE** (Mean Absolute Error) |\n",
    "| **CrossEntropy** | **Quantile** |\n",
    "| *Output:* Probabilities $\\rightarrow$ Classes | *Output:* Direct numerical values |\n",
    "\n",
    "## 3. Evaluation Metrics\n",
    "\n",
    "| Classifier Metrics | Regressor Metrics |\n",
    "| :--- | :--- |\n",
    "| Accuracy, AUC, F1-Score | RMSE, MAE, $R^2$ |\n",
    "| Precision, Recall | MAPE (Mean Absolute Percentage Error) |\n",
    "| Logloss | MSLE (Mean Squared Log Error) |\n",
    "\n",
    "## 4. Key Differences\n",
    "\n",
    "### Target Type\n",
    "* **Regressor:** Numeric (continuous)\n",
    "* **Classifier:** Categorical (discrete classes)\n",
    "\n",
    "### Output Transformation\n",
    "* **Regressor:** Raw number\n",
    "* **Classifier:** Probability (via Sigmoid or Softmax) $\\rightarrow$ Optional threshold to get class\n",
    "\n",
    "### Gradient / Hessian Calculation\n",
    "* **Regressor:** Simple difference (Prediction - Target)\n",
    "* **Classifier:** Derived from classification loss (e.g., Logloss)\n",
    "\n",
    "### Loss Function\n",
    "* **Regression:** Uses MSE, MAE\n",
    "* **Classification:** Uses Logloss / Cross-Entropy\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **CatBoost Regressor** $\\rightarrow$ Numeric prediction, regression loss.\n",
    "* **CatBoost Classifier** $\\rightarrow$ Probability prediction, classification loss.\n",
    "* **Shared Core:** The boosting logic, symmetric tree structure, and categorical handling remain the **same**.\n",
    "* **Conversion:** To convert a regressor architecture to a classifier, you primarily change the **Loss Function**, **Gradient Calculation**, and **Output Mapping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2532ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class OrderedTargetEncoder:\n",
    "    def __init__(self):\n",
    "        self.category_means = {}\n",
    "\n",
    "    def fit_transform(self, X_cat, y):\n",
    "        X_new = np.zeros_like(X_cat, dtype=float)\n",
    "        self.category_means = {}\n",
    "\n",
    "        for i in range(len(X_cat)):\n",
    "            cat = X_cat[i]\n",
    "\n",
    "            if cat not in self.category_means:\n",
    "                self.category_means[cat] = []\n",
    "\n",
    "            # prefix mean of previous targets for this category\n",
    "            if len(self.category_means[cat]) == 0:\n",
    "                X_new[i] = np.mean(y[:i]) if i > 0 else np.mean(y)\n",
    "            else:\n",
    "                X_new[i] = np.mean(self.category_means[cat])\n",
    "\n",
    "            self.category_means[cat].append(y[i])\n",
    "\n",
    "        return X_new.reshape(-1, 1)\n",
    "\n",
    "    def transform(self, X_cat):\n",
    "        X_new = np.zeros_like(X_cat, dtype=float)\n",
    "\n",
    "        for i in range(len(X_cat)):\n",
    "            cat = X_cat[i]\n",
    "            if cat in self.category_means:\n",
    "                X_new[i] = np.mean(self.category_means[cat])\n",
    "            else:\n",
    "                # if new category, use global mean\n",
    "                X_new[i] = np.mean([v for lst in self.category_means.values() for v in lst])\n",
    "\n",
    "        return X_new.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "class CatBoostTree:\n",
    "    def __init__(self, depth=3):\n",
    "        self.depth = depth\n",
    "        self.splits = []\n",
    "        self.leaf_values = None\n",
    "\n",
    "    def fit(self, X, grad):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.splits = []\n",
    "\n",
    "        # symmetric tree: same feature splits at same depth\n",
    "        for level in range(self.depth):\n",
    "            best_gain = -1\n",
    "            best_feature = None\n",
    "            best_thresh = None\n",
    "\n",
    "            for f in range(n_features):\n",
    "                thresholds = np.unique(X[:, f])\n",
    "                for t in thresholds:\n",
    "                    left = grad[X[:, f] <= t].sum()\n",
    "                    right = grad[X[:, f] > t].sum()\n",
    "                    gain = left**2 + right**2\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_feature = f\n",
    "                        best_thresh = t\n",
    "\n",
    "            self.splits.append((best_feature, best_thresh))\n",
    "\n",
    "        # assign leaf values\n",
    "        num_leaves = 2 ** self.depth\n",
    "        self.leaf_values = np.zeros(num_leaves)\n",
    "        leaf_index = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "        for level, (f, t) in enumerate(self.splits):\n",
    "            decision = (X[:, f] > t).astype(int)\n",
    "            leaf_index += decision * (2 ** level)\n",
    "\n",
    "        for leaf in range(num_leaves):\n",
    "            mask = (leaf_index == leaf)\n",
    "            if mask.sum() > 0:\n",
    "                self.leaf_values[leaf] = -grad[mask].mean()\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        preds = np.zeros(n_samples)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            leaf = 0\n",
    "            for level, (f, t) in enumerate(self.splits):\n",
    "                decision = int(X[i, f] > t)\n",
    "                leaf += decision * (2 ** level)\n",
    "            preds[i] = self.leaf_values[leaf]\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "\n",
    "class CatBoostRegressorCore:\n",
    "    def __init__(self, n_estimators=20, learning_rate=0.1, depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = depth\n",
    "        self.trees = []\n",
    "        self.encoders = {}\n",
    "\n",
    "    def fit(self, X_numeric, X_categorical, y):\n",
    "        # Encode categorical features\n",
    "        encoded_cats = []\n",
    "        for j in range(X_categorical.shape[1]):\n",
    "            enc = OrderedTargetEncoder()\n",
    "            encoded_feature = enc.fit_transform(X_categorical[:, j], y)\n",
    "            encoded_cats.append(encoded_feature)\n",
    "            self.encoders[j] = enc\n",
    "\n",
    "        # Combine numeric + encoded categorical\n",
    "        X = np.hstack([X_numeric] + encoded_cats)\n",
    "        pred = np.zeros_like(y, dtype=float)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            grad = pred - y  # gradient of L2 loss\n",
    "            tree = CatBoostTree(depth=self.depth)\n",
    "            tree.fit(X, grad)\n",
    "            update = tree.predict(X)\n",
    "            pred -= self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X_numeric, X_categorical):\n",
    "        encoded_cats = []\n",
    "        for j in range(X_categorical.shape[1]):\n",
    "            encoded_feature = self.encoders[j].transform(X_categorical[:, j])\n",
    "            encoded_cats.append(encoded_feature)\n",
    "\n",
    "        X = np.hstack([X_numeric] + encoded_cats)\n",
    "        pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for tree in self.trees:\n",
    "            pred -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If  we can do this (Classifier):\n",
    "from catboost import CatBoostClassifier\n",
    "model_clf = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    cat_features=['category_col'],\n",
    "    eval_metric='Accuracy'\n",
    ")\n",
    "\n",
    "# Then you can automatically do this (Regressor):\n",
    "from catboost import CatBoostRegressor\n",
    "model_reg = CatBoostRegressor(\n",
    "    iterations=1000,           # Same\n",
    "    learning_rate=0.05,        # Same  \n",
    "    depth=6,                   # Same\n",
    "    cat_features=['category_col'],  # Same\n",
    "    eval_metric='RMSE'         # Only this changes!\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
